# autodata/agents/res.py
import sys
import re
import json
import logging
from typing import Callable, Union, Optional, List, Dict, Any
from autodata.tools.pdf import extract_text_from_pdf
from autodata.tools.legal import split_into_articles
from autodata.tools.keywords import extract_keywords
from pydantic import BaseModel, Field
from langchain.output_parsers import PydanticOutputParser
from easydict import EasyDict
from autodata.core.types import AgentState
from autodata.prompts.prompt_loader import load_prompt
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from autodata.agents.base import BaseAgent, BaseResponse
from autodata.agents.base import BaseAgent, BaseResponse
from autodata.tools.web import get_page_content, normalize_url
from autodata.tools.search import search_discussions
from autodata.tools.crawler import crawl_comments_from_url
from autodata.tools.csv_utils import save_results_csv
logger = logging.getLogger(__name__)

sys.dont_write_bytecode = True
logger = logging.getLogger(__name__)


class BrowserAction(BaseResponse):
    action: str = Field(description="Type of browser action", pattern="^(search|open|extract|click|scroll|get_pdf_links|download_pdf|extract_pdf_text|extract_keywords|search_discussions|crawl_comments|save_csv)$")
    query: Optional[str] = Field(None, description="Search query if action==search or search_discussions")
    url: Optional[str] = Field(None, description="URL for open/extract/download_pdf/crawl_comments")
    selector: Optional[str] = Field(None, description="CSS selector if needed")
    path: Optional[str] = Field(None, description="Local path (for downloads or save)")
    num_results: Optional[int] = Field(5, description="Number of results to fetch")
    notes: Optional[str] = Field(None, description="Optional notes")


class Step(BaseResponse):
    name: str = Field(description="The name of the step.")
    description: str = Field(description="The description of the step.")
    output: str = Field(description="The expected output of the step.")


class PlannerResponse(BaseResponse):
    thought: str = Field(description="A thinking step to complete the given user task.")
    steps: List[Step] = Field(description="A list of detailed steps to complete the task.")

class CrawlSource(BaseModel):
    source_name: str = Field(..., description="T√™n ngu·ªìn th√¥ng tin (v√≠ d·ª•: B√°o Ch√≠nh ph·ªß, D√¢n tr√≠, Facebook, Reddit)")
    base_url: str = Field(..., description="URL g·ªëc c·ªßa ngu·ªìn c·∫ßn crawl")
    search_query: str = Field(..., description="Truy v·∫•n c·ª• th·ªÉ d√πng ƒë·ªÉ t√¨m b√†i vi·∫øt, v√≠ d·ª• '√Ω ki·∫øn d·ª± th·∫£o lu·∫≠t khoa h·ªçc 2025 site:dantri.com.vn'")
    crawl_priority: int = Field(..., description="M·ª©c ƒë·ªô ∆∞u ti√™n crawl (1=quan tr·ªçng nh·∫•t, 5=ph·ª•)")
    expected_data_type: str = Field(..., description="Lo·∫°i d·ªØ li·ªáu d·ª± ki·∫øn: 'b√¨nh lu·∫≠n', 'b√†i vi·∫øt', 'ph√¢n t√≠ch', ...")

class BlueprintResponse(BaseResponse):
    status: str = Field(..., description="Tr·∫°ng th√°i x·ª≠ l√Ω: success ho·∫∑c error")
    topic_summary: Optional[str] = Field(None, description="T√≥m t·∫Øt n·ªôi dung d·ª± th·∫£o nh·∫≠n t·ª´ ToolAgent")
    keywords: List[str] = Field(default_factory=list, description="Danh s√°ch t·ª´ kh√≥a ch√≠nh v·ªÅ d·ª± th·∫£o")
    sources: List[CrawlSource] = Field(default_factory=list, description="Danh s√°ch ngu·ªìn v√† truy v·∫•n ƒë·ªÅ xu·∫•t ƒë·ªÉ crawl")
    total_sources: int = Field(0, description="T·ªïng s·ªë ngu·ªìn ƒë∆∞·ª£c sinh ra")

class ArticleSummary(BaseModel):
    article_no: Optional[int] = Field(None, description="S·ªë ƒëi·ªÅu trong vƒÉn b·∫£n")
    title: Optional[str] = Field(None, description="Ti√™u ƒë·ªÅ c·ªßa ƒëi·ªÅu lu·∫≠t")
    summary: str = Field(..., description="T√≥m t·∫Øt n·ªôi dung ƒëi·ªÅu lu·∫≠t b·∫±ng ti·∫øng Vi·ªát")
    keywords: List[str] = Field(default_factory=list, description="C√°c t·ª´ kh√≥a ch√≠nh c·ªßa ƒëi·ªÅu lu·∫≠t")
    content: str = Field(..., description="N·ªôi dung g·ªëc c·ªßa ƒëi·ªÅu lu·∫≠t")

class ToolResponse(BaseResponse):
    status: str = Field(..., description="Tr·∫°ng th√°i x·ª≠ l√Ω: success ho·∫∑c error")
    source_url: Optional[str] = Field(None, description="ƒê∆∞·ªùng d·∫´n PDF ngu·ªìn")
    article_count: int = Field(0, description="S·ªë l∆∞·ª£ng ƒëi·ªÅu ƒë∆∞·ª£c tr√≠ch xu·∫•t")
    summary: Optional[str] = Field(None, description="T√≥m t·∫Øt to√†n vƒÉn t√†i li·ªáu")
    articles: List[ArticleSummary] = Field(default_factory=list, description="Danh s√°ch ƒëi·ªÅu ƒë∆∞·ª£c tr√≠ch xu·∫•t")

class ResultItem(BaseModel):
    title: Optional[str] = Field(None, description="Ti√™u ƒë·ªÅ trang / b√†i vi·∫øt")
    link: str = Field(..., description="URL b√†i vi·∫øt")
    snippet: Optional[str] = Field(None, description="ƒêo·∫°n m√¥ t·∫£ / snippet")
    comments_count: int = Field(0, description="S·ªë comment thu th·∫≠p ƒë∆∞·ª£c (∆∞·ªõc t√≠nh)")
    sample_comments: List[str] = Field(default_factory=list, description="M·ªôt v√†i comment m·∫´u")


class SourceResult(BaseModel):
    source_name: Optional[str] = Field(None, description="T√™n ngu·ªìn (vd: D√¢n tr√≠, Vnexpress)")
    base_url: Optional[str] = Field(None, description="URL g·ªëc ngu·ªìn")
    search_query: Optional[str] = Field(None, description="Truy v·∫•n t√¨m ki·∫øm ƒë√£ d√πng")
    crawl_priority: Optional[int] = Field(3, description="ƒê·ªô ∆∞u ti√™n")
    expected_data_type: Optional[str] = Field(None, description="Lo·∫°i d·ªØ li·ªáu mong ƒë·ª£i")
    results: List[ResultItem] = Field(default_factory=list, description="Danh s√°ch k·∫øt qu·∫£ (b√†i vi·∫øt / thread)")


class WebAgentResponse(BaseResponse):
    status: str = Field(..., description="success ho·∫∑c error")
    processed_sources: int = Field(0, description="S·ªë ngu·ªìn ƒë√£ x·ª≠ l√Ω")
    total_links_found: int = Field(0, description="T·ªïng s·ªë link t√¨m ƒë∆∞·ª£c")
    total_comments_collected: int = Field(0, description="T·ªïng s·ªë comment ƒë√£ thu th·∫≠p")
    sources: List[SourceResult] = Field(default_factory=list, description="K·∫øt qu·∫£ chi ti·∫øt theo ngu·ªìn")
    saved_csv: Optional[str] = Field(None, description="ƒê∆∞·ªùng d·∫´n file CSV n·∫øu ƒë√£ l∆∞u")

# --------------------
# PlannerAgent
# --------------------
class PlannerAgent(BaseAgent):
    def __init__(self, agent_name: str = "PlannerAgent", description: str = "A planner agent to plan the data collection process.", model: Callable = None, tools=None, **kwargs):
        parser = PydanticOutputParser(pydantic_object=PlannerResponse)
        format_instr = parser.get_format_instructions()
        instruction = (
            "You are a Planning Agent.\n"
            "Your ONLY output must be valid JSON conforming to the schema below.\n\n"
            f"{load_prompt('planner')}\n\n{format_instr}\n\nReturn ONLY the JSON object."
        )

        super().__init__(
            agent_name=agent_name,
            instruction=instruction,
            description=description,
            model=model,
            tools=tools,
            output_parser=parser,
        )
        logger.info("PlannerAgent initialized successfully")


# --------------------
# ToolAgent (th·ª±c thi h√†nh ƒë·ªông)
# --------------------

class ToolAgent(BaseAgent):
    """
    ToolAgent ‚Äî t·ª± ƒë·ªông t·∫£i PDF t·ª´ URL, tr√≠ch xu·∫•t n·ªôi dung, ph√¢n t√≠ch v√† t√≥m t·∫Øt vƒÉn b·∫£n ph√°p lu·∫≠t ti·∫øng Vi·ªát.
    """

    def __init__(
        self,
        agent_name: str = "ToolAgent",
        description: str = "Agent ch·ªãu tr√°ch nhi·ªám s·ª≠ d·ª•ng c√°c c√¥ng c·ª• ƒë·ªÉ ho√†n th√†nh t√°c v·ª• c·ª• th·ªÉ.",
        model=None,
        tools=None,
        output_parser=None,
        **kwargs,
    ):
        # üîπ Load prompt tool.md
        instruction = load_prompt("tool")

        super().__init__(
            agent_name=agent_name,
            instruction=instruction,  # <--- quan tr·ªçng, thi·∫øu d√≤ng n√†y s·∫Ω l·ªói
            description=description,
            model=model,
            tools=tools,
            output_parser=output_parser,
            **kwargs,
        )
        logger.info("ToolAgent initialized successfully")
    # ======================================================
    # MAIN FUNCTION
    # ======================================================
    async def run(self, input_data: Dict[str, Any]) -> ToolResponse:
        """
        input_data example:
        {
            "pdf_url": "https://mst.gov.vn/du-thao/2256.pdf"
        }
        """
        try:
            logger.info("[ToolAgent] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω PDF...")

            pdf_url = input_data.get("pdf_url")
            if not pdf_url:
                raise ValueError("Thi·∫øu tr∆∞·ªùng 'pdf_url' trong input_data")

            pdf_bytes = self._download_pdf(pdf_url)
            if not pdf_bytes:
                raise ValueError("Kh√¥ng th·ªÉ t·∫£i file PDF t·ª´ URL.")

            # 1Ô∏è‚É£ Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ PDF
            text = extract_text_from_pdf(pdf_bytes)
            if not text or len(text) < 200:
                raise ValueError("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung h·ª£p l·ªá t·ª´ PDF.")

            # 2Ô∏è‚É£ Ph√¢n t√°ch vƒÉn b·∫£n th√†nh c√°c 'ƒêi·ªÅu'
            articles = split_into_articles(text)
            if not articles:
                raise ValueError("Kh√¥ng t√¨m th·∫•y c·∫•u tr√∫c 'ƒêi·ªÅu' trong vƒÉn b·∫£n.")

            # 3Ô∏è‚É£ Tr√≠ch xu·∫•t t·ª´ kh√≥a v√† t√≥m t·∫Øt t·ª´ng ƒëi·ªÅu
            parsed_articles = []
            for art in articles:
                art_kw = extract_keywords(art["content"], top_n=8)
                art_summary = await self._summarize_article(art["content"])
                parsed_articles.append(
                    ArticleSummary(
                        article_no=art.get("article_no"),
                        title=art.get("title"),
                        summary=art_summary,
                        keywords=art_kw,
                        content=art.get("content", "")
                    )
                )

            # 4Ô∏è‚É£ T√≥m t·∫Øt to√†n b·ªô vƒÉn b·∫£n
            global_summary = await self._summarize_whole_text(text)

            return ToolResponse(
                status="success",
                source_url=pdf_url,
                article_count=len(parsed_articles),
                summary=global_summary,
                articles=parsed_articles,
            )

        except Exception as e:
            logger.error(f"[ToolAgent] L·ªói khi x·ª≠ l√Ω PDF: {e}")
            return ToolResponse(status="error", summary=str(e))

    # ======================================================
    # SUPPORT FUNCTIONS
    # ======================================================
    def _download_pdf(self, url: str) -> Optional[bytes]:
        """T·∫£i file PDF t·ª´ URL."""
        try:
            logger.info(f"[ToolAgent] ƒêang t·∫£i file PDF t·ª´: {url}")
            headers = {"User-Agent": "Mozilla/5.0 (compatible; AutoDataBot/1.0)"}
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()

            content_type = response.headers.get("Content-Type", "")
            if "pdf" not in content_type.lower():
                logger.warning(f"[ToolAgent] C·∫£nh b√°o: N·ªôi dung kh√¥ng ph·∫£i PDF ({content_type})")

            return response.content
        except Exception as e:
            logger.error(f"[ToolAgent] L·ªói t·∫£i PDF t·ª´ {url}: {e}")
            return None

    async def _summarize_article(self, content: str) -> str:
        """T√≥m t·∫Øt n·ªôi dung t·ª´ng ƒêi·ªÅu b·∫±ng ti·∫øng Vi·ªát."""
        try:
            prompt = ChatPromptTemplate.from_template(
                "H√£y t√≥m t·∫Øt ng·∫Øn g·ªçn n·ªôi dung ƒëi·ªÅu lu·∫≠t sau b·∫±ng ti·∫øng Vi·ªát, ch·ªâ gi·ªØ th√¥ng tin c·ªët l√µi:\n\n{content}\n\n---\nT√≥m t·∫Øt:"
            )
            chain = prompt | self.model
            response = await chain.ainvoke({"content": content[:2500]})
            return response.content.strip()
        except Exception as e:
            logger.warning(f"[ToolAgent] L·ªói t√≥m t·∫Øt ƒêi·ªÅu: {e}")
            return content[:200] + "..."

    async def _summarize_whole_text(self, text: str) -> str:
        """T√≥m t·∫Øt to√†n vƒÉn b·∫£n lu·∫≠t (3‚Äì5 c√¢u)."""
        try:
            prompt = ChatPromptTemplate.from_template(
                "ƒê√¢y l√† m·ªôt d·ª± th·∫£o lu·∫≠t c·ªßa Vi·ªát Nam. H√£y t√≥m t·∫Øt to√†n b·ªô n·ªôi dung vƒÉn b·∫£n b·∫±ng 3‚Äì5 c√¢u ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát:\n\n{text}\n\n---\nT√≥m t·∫Øt:"
            )
            chain = prompt | self.model
            response = await chain.ainvoke({"text": text[:6000]})
            return response.content.strip()
        except Exception as e:
            logger.warning(f"[ToolAgent] L·ªói t√≥m t·∫Øt to√†n vƒÉn: {e}")
            return ""

# --------------------
# WebAgent (generates BrowserAction)
# --------------------
class WebAgent(BaseAgent):
    """
    WebAgent: t·ª´ blueprint (sources) ti·∫øn h√†nh t√¨m link / b√†i vi·∫øt v√† crawl comment.
    Input (state/messages) c√≥ th·ªÉ ch·ª©a:
      - JSON c√≥ key "sources": danh s√°ch CrawlSource (t·ª´ BlueprintAgent)
      - Ho·∫∑c key "keywords" v√† "topic_summary": khi sources kh√¥ng c√≥ s·∫µn, agent t·ª± sinh queries
    Tr·∫£ v·ªÅ: EasyDict v·ªõi 'results' (chi ti·∫øt) v√† 'next' (m·∫∑c ƒë·ªãnh v·ªÅ ToolAgent)
    """

    def __init__(
        self,
        agent_name: str = "WebAgent",
        description: str = "Agent thu th·∫≠p b√†i vi·∫øt v√† b√¨nh lu·∫≠n t·ª´ c√°c ngu·ªìn ƒë·ªÅ xu·∫•t.",
        model: Optional[object] = None,
        tools: Optional[List] = None,
        default_num_results: int = 5,
        max_comments_per_page: int = 50,
        **kwargs,
    ):
        # Kh√¥ng √©p output_parser ·ªü ƒë√¢y (WebAgent l√† agent th·ª±c thi), ƒë·ªÉ BaseAgent x·ª≠ l√Ω chu·ªói n·∫øu c·∫ßn
        instruction = load_prompt("web")
        super().__init__(
            agent_name=agent_name,
            instruction=instruction,
            description=description,
            model=model,
            tools=tools,
            output_parser=None,
            **kwargs,
        )
        self.default_num_results = default_num_results
        self.max_comments_per_page = max_comments_per_page
        logger.info("WebAgent initialized successfully")

    # helper: parse incoming state/messages to get sources or keywords
    def _parse_input(self, state: Any) -> Dict[str, Any]:
        """
        Tr·∫£ v·ªÅ dict c√≥ c√°c tr∆∞·ªùng:
          - sources: list[dict] (m·ªói dict ch·ª©a source_name, base_url, search_query, crawl_priority, expected_data_type)
          - keywords: list[str]
          - summary: str
          - save_csv: bool, out_path: str (tu·ª≥ ch·ªçn)
        """
        parsed = {"sources": [], "keywords": [], "summary": "", "save_csv": False, "out_path": None}
        # h·ªó tr·ª£ c·∫£ AgentState (TypedDict) v√† raw dict
        if hasattr(state, "messages"):
            messages = state.messages
        elif isinstance(state, dict):
            messages = state.get("messages", [])
        else:
            messages = []

        if not messages:
            return parsed

        # Last message may contain structured JSON (BlueprintResponse) or plain text.
        last = messages[-1]
        content = getattr(last, "content", str(last)).strip()

        # Try parse JSON
        try:
            data = json.loads(content)
            # If it's blueprint-like
            if isinstance(data, dict):
                if "sources" in data and isinstance(data["sources"], list):
                    parsed["sources"] = data["sources"]
                if "keywords" in data:
                    parsed["keywords"] = data.get("keywords") or parsed["keywords"]
                if "topic_summary" in data:
                    parsed["summary"] = data.get("topic_summary") or parsed["summary"]
                # optional flags
                parsed["save_csv"] = data.get("save_csv", False)
                parsed["out_path"] = data.get("out_path")
                return parsed
            # if it's list (maybe model returned list of sources)
            if isinstance(data, list):
                parsed["sources"] = data
                return parsed
        except Exception:
            # not JSON ‚Äî try to parse a fallback pattern
            pass

        # fallback: check if message contains keywords in a simple format (comma separated)
        # e.g., "keywords: khoa h·ªçc, ƒë·ªïi m·ªõi s√°ng t·∫°o"
        m = re.search(r"keywords?\s*[:\-]\s*(.+)$", content, flags=re.IGNORECASE)
        if m:
            kw_text = m.group(1)
            parsed["keywords"] = [k.strip() for k in re.split(r"[;,]", kw_text) if k.strip()]
            return parsed

        # if content is small text, set as summary
        if len(content) > 30:
            parsed["summary"] = content
            # attempt to extract keywords by simple split (if comma separated)
            if "," in content and len(content.split()) < 20:
                parsed["keywords"] = [k.strip() for k in content.split(",")][:10]

        return parsed

    # helper: find links by scanning base_url page for anchors that match keywords
    def _find_links_by_keywords(self, base_url: str, keywords: List[str], num: int) -> List[Dict[str, str]]:
        """
        Heuristic: t·∫£i base_url, t√¨m <a> c√≥ text/href ch·ª©a keywords. Return list of {title, link, snippet}.
        """
        html = get_page_content(base_url)
        if not html:
            return []

        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, "html.parser")
        anchors = soup.find_all("a", href=True)
        candidates = []
        for a in anchors:
            href = a["href"].strip()
            title = (a.get_text(" ", strip=True) or "").strip()
            link = normalize_url(base_url, href)
            score = 0
            text_lower = title.lower()
            for kw in keywords:
                if not kw:
                    continue
                k = kw.lower()
                if k in text_lower:
                    score += 3
                if k in href.lower():
                    score += 1
            # also boost if anchor appears near "√Ω ki·∫øn", "b√¨nh lu·∫≠n", "th·∫£o lu·∫≠n"
            if re.search(r"√Ω ki·∫øn|b√¨nh lu·∫≠n|th·∫£o lu·∫≠n|ph·∫£n h·ªìi|g√≥p √Ω", title, flags=re.IGNORECASE):
                score += 2
            if score > 0:
                snippet = None
                # try to find a short context paragraph sibling
                p = a.find_parent()
                if p:
                    snippet = p.get_text(" ", strip=True)[:300]
                candidates.append({"title": title or link, "link": link, "snippet": snippet or "" , "score": score})
        # sort by score desc, unique by link
        candidates.sort(key=lambda x: x["score"], reverse=True)
        seen = set()
        out = []
        for c in candidates:
            if c["link"] not in seen:
                seen.add(c["link"])
                out.append({"title": c["title"], "link": c["link"], "snippet": c["snippet"]})
            if len(out) >= num:
                break
        return out

    async def __call__(self, state: Any, model=None):
        """
        Entrypoint used by workflow. state contains 'messages' with blueprint or keywords.
        Returns an EasyDict with keys: messages (list), results (detailed), next (str).
        """
        try:
            parsed = self._parse_input(state)
            sources_input = parsed.get("sources", [])
            keywords = parsed.get("keywords", [])
            summary = parsed.get("summary", "")
            save_csv = parsed.get("save_csv", False)
            out_path = parsed.get("out_path", None)

            # if no sources provided by blueprint, create fallback sources from keywords
            if not sources_input and keywords:
                # create a generic source entry to search web with keywords
                q = " ".join(keywords)
                sources_input = [
                    {
                        "source_name": "web_search",
                        "base_url": "",
                        "search_query": q,
                        "crawl_priority": 3,
                        "expected_data_type": "b√¨nh lu·∫≠n / b√†i vi·∫øt"
                    }
                ]

            if not sources_input:
                return EasyDict({
                    "messages": ["WebAgent: Kh√¥ng t√¨m th·∫•y ngu·ªìn (sources) ho·∫∑c keywords ƒë·ªÉ crawl."],
                    "next": "ManagerAgent",
                })

            aggregated_sources: List[Dict[str, Any]] = []
            total_links = 0
            total_comments = 0
            all_comments_rows = []  # for optional CSV export

            for src in sources_input:
                name = src.get("source_name") or src.get("name") or "unknown"
                base_url = src.get("base_url") or ""
                search_query = src.get("search_query") or ""
                num_results = int(src.get("num_results", self.default_num_results))
                priority = int(src.get("crawl_priority", 3))
                expected_type = src.get("expected_data_type", "")

                logger.info("WebAgent: processing source '%s' (query=%s)", name, search_query or base_url)

                # 1) primary: try Google CSE (search_discussions)
                links = []
                if search_query:
                    try:
                        links = search_discussions(search_query, num=num_results)
                        # transform to expected minimal structure if necessary
                        tmp = []
                        for it in links:
                            tmp.append({"title": it.get("title"), "link": it.get("link"), "snippet": it.get("snippet")})
                        links = tmp
                    except Exception as e:
                        logger.warning("WebAgent: search_discussions error: %s", e)
                        links = []

                # 2) fallback: if search returned empty and base_url available => scan base_url for keyword matches
                if not links and base_url and keywords:
                    try:
                        found = self._find_links_by_keywords(base_url, keywords, num_results)
                        links = found
                    except Exception as e:
                        logger.debug("WebAgent: fallback scan error: %s", e)
                        links = []

                # 3) fallback: if still empty and search_query present but no CSE => try simple site search by constructing site search on base_url and scraping (best-effort)
                if not links and search_query and base_url:
                    # try to hit base_url and find anchors containing any keyword tokens
                    try:
                        found = self._find_links_by_keywords(base_url, keywords or search_query.split(), num_results)
                        links = found
                    except Exception:
                        links = []

                # Now for each link, fetch page and crawl comments
                result_items: List[Dict[str, Any]] = []
                for link_obj in (links or [])[:num_results]:
                    link = link_obj.get("link")
                    title = link_obj.get("title") or ""
                    snippet = link_obj.get("snippet") or ""
                    comments = []
                    try:
                        comments = crawl_comments_from_url(link, max_items=self.max_comments_per_page)
                    except Exception as e:
                        logger.warning("WebAgent: crawl_comments_from_url failed for %s: %s", link, e)
                        comments = []

                    sample_comments = [c.get("text") for c in comments[:5]] if comments else []
                    result_items.append({
                        "title": title,
                        "link": link,
                        "snippet": snippet,
                        "comments_count": len(comments),
                        "sample_comments": sample_comments,
                    })

                    # prepare CSV rows
                    for c in comments:
                        row = {
                            "source_name": name,
                            "source_base": base_url,
                            "search_query": search_query,
                            "article_link": link,
                            "article_title": title,
                            "comment_text": c.get("text"),
                        }
                        all_comments_rows.append(row)

                    total_links += 1
                    total_comments += len(comments)

                aggregated_sources.append({
                    "source_name": name,
                    "base_url": base_url,
                    "search_query": search_query,
                    "crawl_priority": priority,
                    "expected_data_type": expected_type,
                    "results": result_items,
                })

            saved_csv_path = None
            if save_csv and out_path and all_comments_rows:
                try:
                    saved_csv_path = save_results_csv(all_comments_rows, out_path)
                except Exception as e:
                    logger.warning("WebAgent: failed to save CSV to %s: %s", out_path, e)
                    saved_csv_path = None

            response = WebAgentResponse(
                status="success",
                processed_sources=len(aggregated_sources),
                total_links_found=total_links,
                total_comments_collected=total_comments,
                sources=[SourceResult(**s) for s in aggregated_sources],
                saved_csv=saved_csv_path,
            )

            # Return in same style as other ToolAgent returns (EasyDict)
            return EasyDict({
                "messages": [f"WebAgent: processed {len(aggregated_sources)} sources, collected {total_comments} comments."],
                "results": response,
                "next": "ToolAgent",
            })

        except Exception as e:
            logger.error("WebAgent encountered error: %s", e)
            return EasyDict({
                "messages": [f"WebAgent error: {e}"],
                "next": "ManagerAgent",
            })
# ======================================================
# BLUEPRINT AGENT
# ======================================================
class BlueprintAgent(BaseAgent):
    """
    BlueprintAgent ‚Äî t·∫°o k·∫ø ho·∫°ch crawl d·ªØ li·ªáu d∆∞ lu·∫≠n v·ªÅ d·ª± th·∫£o lu·∫≠t,
    bao g·ªìm danh s√°ch ngu·ªìn, t·ª´ kh√≥a t√¨m ki·∫øm v√† ƒë·ªô ∆∞u ti√™n crawl.
    """

    def __init__(
        self,
        agent_name: str = "BlueprintAgent",
        description: str = "Agent c√≥ nhi·ªám v·ª• x√¢y d·ª±ng b·∫£n thi·∫øt k·∫ø (blueprint) d·ªØ li·ªáu, x√°c ƒë·ªãnh c·∫•u tr√∫c d·ªØ li·ªáu c·∫ßn tr√≠ch xu·∫•t v√† ƒë·ªãnh d·∫°ng l∆∞u tr·ªØ.",
        model=None,
        tools=None,
        output_parser=None,
        **kwargs,
    ):
        # üîπ Load prompt blueprint.md
        instruction = load_prompt("blueprint")

        super().__init__(
            agent_name=agent_name,
            instruction=instruction,  # ‚ö†Ô∏è d√≤ng b·∫Øt bu·ªôc
            description=description,
            model=model,
            tools=tools,
            output_parser=output_parser,
            **kwargs,
        )
        logger.info("BlueprintAgent initialized successfully")
    # ======================================================
    # MAIN FUNCTION
    # ======================================================
    async def run(self, input_data: Dict[str, Any]) -> BlueprintResponse:
        """
        input_data example:
        {
            "summary": "...",
            "keywords": ["ƒë·ªïi m·ªõi s√°ng t·∫°o", "nghi√™n c·ª©u khoa h·ªçc", "c√¥ng ngh·ªá", ...]
        }
        """
        try:
            logger.info("[BlueprintAgent] B·∫Øt ƒë·∫ßu sinh k·∫ø ho·∫°ch crawl...")

            summary = input_data.get("summary", "")
            keywords = input_data.get("keywords", [])

            if not summary and not keywords:
                raise ValueError("Thi·∫øu d·ªØ li·ªáu ƒë·∫ßu v√†o cho BlueprintAgent")

            # D√πng LLM ƒë·ªÉ sinh danh s√°ch ngu·ªìn crawl
            sources = await self._generate_crawl_blueprint(summary, keywords)

            return BlueprintResponse(
                status="success",
                topic_summary=summary,
                keywords=keywords,
                sources=sources,
                total_sources=len(sources),
            )

        except Exception as e:
            logger.error(f"[BlueprintAgent] L·ªói khi sinh k·∫ø ho·∫°ch crawl: {e}")
            return BlueprintResponse(status="error", topic_summary=str(e))

    # ======================================================
    # SUPPORT FUNCTIONS
    # ======================================================
    async def _generate_crawl_blueprint(
        self, summary: str, keywords: List[str]
    ) -> List[CrawlSource]:
        """D√πng m√¥ h√¨nh Ollama ƒë·ªÉ ƒë·ªÅ xu·∫•t danh s√°ch ngu·ªìn v√† truy v·∫•n crawl."""
        try:
            prompt = ChatPromptTemplate.from_template(
                """
                B·∫°n l√† chuy√™n gia thu th·∫≠p d·ªØ li·ªáu tr·ª±c tuy·∫øn. 
                Cho n·ªôi dung t√≥m t·∫Øt sau ƒë√¢y v·ªÅ m·ªôt **d·ª± th·∫£o lu·∫≠t c·ªßa Vi·ªát Nam**, 
                h√£y t·∫°o danh s√°ch 8‚Äì12 ngu·ªìn web ti·ªÅm nƒÉng ƒë·ªÉ crawl th√¥ng tin, bao g·ªìm:
                - C√°c trang b√°o ch√≠nh th·ªëng (vnexpress, dantri, baochinhphu, vov, cafef)
                - C√°c di·ªÖn ƒë√†n, m·∫°ng x√£ h·ªôi (facebook, reddit, tiktok, youtube)
                - C√°c c·ªïng g√≥p √Ω d·ª± th·∫£o (duthaoonline.quochoi.vn, mst.gov.vn)

                D·ªØ li·ªáu ƒë·∫ßu v√†o:
                - T√≥m t·∫Øt: {summary}
                - T·ª´ kh√≥a: {keywords}

                ƒê·∫ßu ra y√™u c·∫ßu:
                JSON g·ªìm danh s√°ch c√°c ƒë·ªëi t∆∞·ª£ng v·ªõi tr∆∞·ªùng:
                {{
                    "source_name": "...",
                    "base_url": "...",
                    "search_query": "...",
                    "crawl_priority": 1-5,
                    "expected_data_type": "..."
                }}
                """
            )

            chain = prompt | self.model
            response = await chain.ainvoke({
                "summary": summary,
                "keywords": ", ".join(keywords),
            })

            # Ph√¢n t√≠ch k·∫øt qu·∫£ JSON t·ª´ m√¥ h√¨nh
            import json
            try:
                sources_json = json.loads(response.content)
                sources = [CrawlSource(**src) for src in sources_json if isinstance(src, dict)]
                return sources
            except Exception:
                # Fallback: x·ª≠ l√Ω chu·ªói tr·∫£ v·ªÅ kh√¥ng chu·∫©n
                logger.warning("[BlueprintAgent] Ph·∫£n h·ªìi kh√¥ng ph·∫£i JSON h·ª£p l·ªá, fallback sang heuristic parse.")
                sources = []
                lines = response.content.split("\n")
                for line in lines:
                    if "http" in line:
                        parts = line.split("-")
                        name = parts[0].strip()
                        url = [w for w in parts if "http" in w]
                        sources.append(
                            CrawlSource(
                                source_name=name,
                                base_url=url[0] if url else "",
                                search_query=f"√Ω ki·∫øn v·ªÅ {', '.join(keywords)} site:{url[0] if url else 'vnexpress.net'}",
                                crawl_priority=3,
                                expected_data_type="b√†i vi·∫øt"
                            )
                        )
                return sources

        except Exception as e:
            logger.error(f"[BlueprintAgent] L·ªói sinh danh s√°ch ngu·ªìn crawl: {e}")
            return []




RESEARCH_AGENTS = [PlannerAgent.__name__, WebAgent.__name__, BlueprintAgent.__name__]
