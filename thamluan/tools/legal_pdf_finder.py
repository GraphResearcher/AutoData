"""
Legal PDF Finder Tool - Tá»± Ä‘á»™ng tÃ¬m kiáº¿m vÃ  táº£i PDF Luáº­t/Nghá»‹ Ä‘á»‹nh tá»« keywords.
"""

import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
import logging
from urllib.parse import urljoin, quote_plus
import re

from core.config import config
from core.types import ToolResult
from tools.search_engine import search_engine_tool
from tools.web_crawler import web_crawler_tool

logger = logging.getLogger(__name__)


class LegalPDFFinderTool:
    """Tool Ä‘á»ƒ tÃ¬m kiáº¿m vÃ  phÃ¡t hiá»‡n PDF Luáº­t/Nghá»‹ Ä‘á»‹nh tá»« keywords"""
    
    # Danh sÃ¡ch cÃ¡c trang web chÃ­nh thá»©c vá» Luáº­t/Nghá»‹ Ä‘á»‹nh Viá»‡t Nam (Æ¯U TIÃŠN)
    PRIORITY_LEGAL_DOMAINS = [
        'duthaoonline.quochoi.vn',  # Dá»± tháº£o online Quá»‘c há»™i - Æ¯U TIÃŠN HÃ€NG Äáº¦U
        'thuvienphapluat.vn',  # ThÆ° viá»‡n PhÃ¡p luáº­t
        'baochinhphu.vn',  # BÃ¡o ChÃ­nh phá»§
    ]
    
    # CÃ¡c trang web chÃ­nh thá»©c khÃ¡c
    LEGAL_DOMAINS = [
        'duthaoonline.quochoi.vn',
        'thuvienphapluat.vn',
        'baochinhphu.vn',
        'luatvietnam.vn',
        'moj.gov.vn',  # Bá»™ TÆ° PhÃ¡p
        'chinhphu.vn',  # Cá»•ng thÃ´ng tin chÃ­nh phá»§
        'mpi.gov.vn',  # Bá»™ Káº¿ hoáº¡ch vÃ  Äáº§u tÆ°
        'mof.gov.vn',  # Bá»™ TÃ i chÃ­nh
        'most.gov.vn',  # Bá»™ Khoa há»c vÃ  CÃ´ng nghá»‡
        'mic.gov.vn',  # Bá»™ ThÃ´ng tin vÃ  Truyá»n thÃ´ng
        'na.gov.vn',  # Quá»‘c há»™i
        'vietnamlawmagazine.vn',
        'mst.gov.vn',
    ]
    
    # URL trá»±c tiáº¿p Ä‘á»ƒ crawl
    DIRECT_CRAWL_URLS = [
        'https://duthaoonline.quochoi.vn/du-thao/du-thao-luat',
        'https://baochinhphu.vn/',
        'https://thuvienphapluat.vn/',
    ]
    
    # Tá»« khÃ³a chá»‰ thá»‹ tÃ i liá»‡u luáº­t
    LEGAL_INDICATORS = [
        'dá»± tháº£o',
        'luáº­t',
        'nghá»‹ Ä‘á»‹nh',
        'thÃ´ng tÆ°',
        'quyáº¿t Ä‘á»‹nh',
        'nghá»‹ quyáº¿t',
        'vÄƒn báº£n',
        'phÃ¡p luáº­t',
        'du thao',
        'nghi dinh',
    ]

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': config.USER_AGENT
        })
        self.crawl4ai_available = False
        try:
            from crawl4ai import WebCrawler
            self.crawl4ai_available = True
            logger.info("âœ… Crawl4AI available for advanced crawling")
        except ImportError:
            logger.info("â„¹ï¸ Crawl4AI not available, using standard crawling")

    def search_legal_pdfs_direct_crawl(self, keywords: str, max_results: int = 10) -> ToolResult:
        """
        TÃ¬m kiáº¿m PDF báº±ng cÃ¡ch crawl trá»±c tiáº¿p cÃ¡c trang web uy tÃ­n.
        PhÆ°Æ¡ng phÃ¡p nÃ y CHÃNH XÃC hÆ¡n Google search.
        """
        try:
            logger.info(f"ðŸ” Direct crawling trusted sites for keywords: {keywords}")
            all_pdfs = []
            
            # BÆ°á»›c 1: Crawl trá»±c tiáº¿p cÃ¡c trang Æ°u tiÃªn
            priority_sites = [
                {
                    'name': 'Dá»± tháº£o Online Quá»‘c há»™i',
                    'url': 'https://duthaoonline.quochoi.vn/du-thao/du-thao-luat',
                    'search_url': f'https://duthaoonline.quochoi.vn/DuThao/Lists/DT_DUTHAO_NGHIQUYET/?keyword={quote_plus(keywords)}'
                },
                {
                    'name': 'ThÆ° viá»‡n PhÃ¡p luáº­t',
                    'url': 'https://thuvienphapluat.vn/',
                    'search_url': f'https://thuvienphapluat.vn/van-ban/tim-van-ban.aspx?keyword={quote_plus(keywords)}&Page=1'
                },
                {
                    'name': 'BÃ¡o ChÃ­nh phá»§',
                    'url': 'https://baochinhphu.vn/',
                    'search_url': f'https://baochinhphu.vn/tim-kiem.htm?keywords={quote_plus(keywords)}'
                }
            ]
            
            for site in priority_sites:
                logger.info(f"ðŸ“° Crawling {site['name']}...")
                try:
                    pdfs = self._crawl_site_for_pdfs(site['search_url'], keywords, site['name'])
                    if pdfs:
                        all_pdfs.extend(pdfs)
                        logger.info(f"  âœ“ Found {len(pdfs)} PDF(s) from {site['name']}")
                except Exception as e:
                    logger.warning(f"  âœ— Failed to crawl {site['name']}: {str(e)}")
                    continue
            
            # Loáº¡i bá» trÃ¹ng láº·p
            unique_pdfs = self._deduplicate_pdfs(all_pdfs)
            logger.info(f"âœ… Total unique PDFs from direct crawling: {len(unique_pdfs)}")
            
            return ToolResult(
                success=True,
                data={
                    'pdf_links': unique_pdfs[:max_results],
                    'total_found': len(unique_pdfs),
                    'method': 'direct_crawl'
                }
            )
            
        except Exception as e:
            logger.error(f"Direct crawl error: {str(e)}", exc_info=True)
            return ToolResult(
                success=False,
                error=f"Failed to crawl sites directly: {str(e)}"
            )
    
    def _crawl_site_for_pdfs(self, url: str, keywords: str, site_name: str) -> List[Dict[str, str]]:
        """Crawl má»™t trang web Ä‘á»ƒ tÃ¬m PDF"""
        try:
            # Thá»­ dÃ¹ng crawl4ai náº¿u cÃ³
            if self.crawl4ai_available:
                return self._crawl_with_crawl4ai(url, keywords, site_name)
            else:
                # Fallback: dÃ¹ng web_crawler_tool
                return self._crawl_with_standard_tool(url, keywords, site_name)
        except Exception as e:
            logger.error(f"Error crawling {site_name}: {str(e)}")
            return []
    
    def _crawl_with_crawl4ai(self, url: str, keywords: str, site_name: str) -> List[Dict[str, str]]:
        """Sá»­ dá»¥ng crawl4ai Ä‘á»ƒ crawl (advanced)"""
        try:
            from crawl4ai import WebCrawler
            
            crawler = WebCrawler()
            result = crawler.run(url=url)
            
            if not result.success:
                logger.warning(f"Crawl4AI failed for {url}")
                return self._crawl_with_standard_tool(url, keywords, site_name)
            
            # Parse HTML vá»›i BeautifulSoup
            soup = BeautifulSoup(result.html, 'lxml')
            
            # TÃ¬m táº¥t cáº£ links
            pdf_links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                text = link.get_text(strip=True)
                
                # Check náº¿u lÃ  PDF
                if href.lower().endswith('.pdf') or 'pdf' in href.lower():
                    full_url = urljoin(url, href)
                    
                    # Check relevance vá»›i keywords
                    if self._is_relevant_to_keywords(text, href, keywords):
                        pdf_links.append({
                            'url': full_url,
                            'text': text or f'PDF from {site_name}',
                            'source': site_name
                        })
            
            return pdf_links
            
        except Exception as e:
            logger.error(f"Crawl4AI error: {str(e)}")
            return self._crawl_with_standard_tool(url, keywords, site_name)
    
    def _crawl_with_standard_tool(self, url: str, keywords: str, site_name: str) -> List[Dict[str, str]]:
        """Sá»­ dá»¥ng web_crawler_tool tiÃªu chuáº©n"""
        try:
            result = web_crawler_tool.find_pdf_links(url)
            
            if not result.success:
                return []
            
            pdf_links = result.data.get('pdf_links', [])
            
            # Filter relevant PDFs
            relevant_pdfs = []
            for pdf in pdf_links:
                if self._is_relevant_to_keywords(pdf.get('text', ''), pdf.get('url', ''), keywords):
                    pdf['source'] = site_name
                    relevant_pdfs.append(pdf)
            
            return relevant_pdfs
            
        except Exception as e:
            logger.error(f"Standard crawl error: {str(e)}")
            return []
    
    def _is_relevant_to_keywords(self, text: str, url: str, keywords: str) -> bool:
        """Kiá»ƒm tra xem text/url cÃ³ liÃªn quan Ä‘áº¿n keywords khÃ´ng"""
        text_lower = text.lower()
        url_lower = url.lower()
        keywords_lower = keywords.lower()
        
        # Check tá»«ng tá»« trong keywords
        keyword_words = keywords_lower.split()
        matches = sum(1 for word in keyword_words if word in text_lower or word in url_lower)
        
        # Cáº§n Ã­t nháº¥t 40% tá»« khÃ³a match hoáº·c cÃ³ legal indicators
        threshold = len(keyword_words) * 0.4
        has_match = matches >= max(1, threshold)
        
        # Hoáº·c chá»©a legal indicators
        has_legal = any(indicator in text_lower or indicator in url_lower 
                       for indicator in self.LEGAL_INDICATORS)
        
        return has_match or has_legal

    def search_legal_pdfs(self, keywords: str, max_results: int = 10) -> ToolResult:
        """
        TÃ¬m kiáº¿m PDF Luáº­t/Nghá»‹ Ä‘á»‹nh dá»±a trÃªn keywords.
        CHIáº¾N LÆ¯á»¢C: Æ¯u tiÃªn crawl trá»±c tiáº¿p cÃ¡c trang uy tÃ­n, sau Ä‘Ã³ má»›i dÃ¹ng Google search.
        
        Args:
            keywords: Tá»« khÃ³a tÃ¬m kiáº¿m (vÃ­ dá»¥: "Luáº­t Khoa há»c cÃ´ng nghá»‡ 2025")
            max_results: Sá»‘ lÆ°á»£ng káº¿t quáº£ tá»‘i Ä‘a
            
        Returns:
            ToolResult vá»›i danh sÃ¡ch PDF links tÃ¬m Ä‘Æ°á»£c
        """
        try:
            logger.info(f"ðŸ” Searching for legal PDFs with keywords: {keywords}")
            
            # CHIáº¾N LÆ¯á»¢C 1: Crawl trá»±c tiáº¿p cÃ¡c trang uy tÃ­n TRÆ¯á»šC (CHÃNH XÃC NHáº¤T)
            logger.info("ðŸ“ Strategy 1: Direct crawling trusted Vietnamese legal sites...")
            direct_result = self.search_legal_pdfs_direct_crawl(keywords, max_results)
            
            if direct_result.success and len(direct_result.data.get('pdf_links', [])) > 0:
                logger.info(f"âœ… Found {len(direct_result.data['pdf_links'])} PDFs from direct crawling")
                # Náº¿u tÃ¬m Ä‘Æ°á»£c Ä‘á»§ PDFs tá»« direct crawl, return luÃ´n
                if len(direct_result.data['pdf_links']) >= max_results:
                    return direct_result
                
                # Náº¿u chÆ°a Ä‘á»§, lÆ°u láº¡i vÃ  tiáº¿p tá»¥c vá»›i Google search
                direct_pdfs = direct_result.data['pdf_links']
            else:
                logger.info("âš ï¸ Direct crawling found no PDFs, trying Google search...")
                direct_pdfs = []
            
            # CHIáº¾N LÆ¯á»¢C 2: Google search (FALLBACK)
            logger.info("ðŸ“ Strategy 2: Google search on legal domains...")
            search_queries = self._generate_legal_search_queries(keywords)
            logger.info(f"ðŸ“‹ Generated {len(search_queries)} search queries")
            
            all_results = []
            for query in search_queries[:3]:  # Giá»›i háº¡n 3 queries
                logger.info(f"ðŸ”Ž Searching: {query}")
                
                search_result = search_engine_tool.search(
                    query=query,
                    num_results=max_results,
                    search_engine="google"
                )
                
                if search_result.success:
                    results = search_result.data.get('results', [])
                    all_results.extend(results)
                    logger.info(f"  âœ“ Found {len(results)} results")
                
                import time
                time.sleep(1)
            
            # Lá»c vÃ  xáº¿p háº¡ng káº¿t quáº£ tá»« Google
            filtered_results = self._filter_legal_results(all_results, keywords)
            logger.info(f"ðŸ“Š Filtered to {len(filtered_results)} relevant Google results")
            
            # TÃ¬m PDF links tá»« cÃ¡c trang web (Google results)
            google_pdfs = []
            for result in filtered_results[:max_results]:
                url = result.get('url', '')
                logger.info(f"ðŸ”— Checking URL: {url}")
                
                pdfs = self._extract_pdfs_from_url(url, keywords)
                if pdfs:
                    google_pdfs.extend(pdfs)
                    logger.info(f"  âœ“ Found {len(pdfs)} PDF(s)")
                
                if len(google_pdfs) >= max_results:
                    break
            
            # Káº¿t há»£p káº¿t quáº£: Direct crawl PDFs Æ¯U TIÃŠN trÆ°á»›c, sau Ä‘Ã³ Google PDFs
            all_pdfs = direct_pdfs + google_pdfs
            
            # Loáº¡i bá» trÃ¹ng láº·p
            unique_pdfs = self._deduplicate_pdfs(all_pdfs)
            logger.info(f"âœ… Total unique PDFs found: {len(unique_pdfs)} (Direct: {len(direct_pdfs)}, Google: {len(google_pdfs)})")
            
            return ToolResult(
                success=True,
                data={
                    'pdf_links': unique_pdfs[:max_results],
                    'total_found': len(unique_pdfs),
                    'search_queries': search_queries,
                    'direct_count': len(direct_pdfs),
                    'google_count': len(google_pdfs)
                }
            )
            
        except Exception as e:
            logger.error(f"Legal PDF search error: {str(e)}", exc_info=True)
            return ToolResult(
                success=False,
                error=f"Failed to search legal PDFs: {str(e)}"
            )

    def _generate_legal_search_queries(self, keywords: str) -> List[str]:
        """Táº¡o cÃ¡c search queries tá»‘i Æ°u cho tÃ¬m kiáº¿m luáº­t"""
        queries = []
        
        # Query 1: Æ¯u tiÃªn cÃ¡c trang VIP nháº¥t - Dá»± tháº£o Quá»‘c há»™i, ThÆ° viá»‡n PhÃ¡p luáº­t, BÃ¡o ChÃ­nh phá»§
        priority_sites = ' OR '.join([f'site:{domain}' for domain in self.PRIORITY_LEGAL_DOMAINS])
        queries.append(f'{keywords} filetype:pdf ({priority_sites})')
        
        # Query 2: TÃ¬m dá»± tháº£o luáº­t cá»¥ thá»ƒ trÃªn Quá»‘c há»™i
        queries.append(f'dá»± tháº£o {keywords} filetype:pdf site:duthaoonline.quochoi.vn')
        
        # Query 3: TÃ¬m trÃªn ThÆ° viá»‡n PhÃ¡p luáº­t
        queries.append(f'{keywords} filetype:pdf site:thuvienphapluat.vn')
        
        # Query 4: TÃ¬m trÃªn BÃ¡o ChÃ­nh phá»§
        queries.append(f'{keywords} filetype:pdf site:baochinhphu.vn')
        
        # Query 5: TÃ¬m trÃªn cÃ¡c site chÃ­nh thá»©c khÃ¡c
        other_sites = ' OR '.join([f'site:{domain}' for domain in self.LEGAL_DOMAINS[3:8]])
        queries.append(f'{keywords} filetype:pdf ({other_sites})')
        
        # Query 6: TÃ¬m vá»›i tá»« khÃ³a dá»± tháº£o + nghá»‹ Ä‘á»‹nh
        queries.append(f'dá»± tháº£o nghá»‹ Ä‘á»‹nh {keywords} filetype:pdf')
        
        return queries

    def _filter_legal_results(self, results: List[Dict], keywords: str) -> List[Dict]:
        """Lá»c vÃ  xáº¿p háº¡ng káº¿t quáº£ theo Ä‘á»™ liÃªn quan"""
        filtered = []
        keywords_lower = keywords.lower()
        
        for result in results:
            url = result.get('url', '').lower()
            title = result.get('title', '').lower()
            snippet = result.get('snippet', '').lower()
            
            # Kiá»ƒm tra cÃ³ chá»©a PRIORITY domain khÃ´ng (Æ¯U TIÃŠN CAO NHáº¤T)
            is_priority_domain = any(domain in url for domain in self.PRIORITY_LEGAL_DOMAINS)
            
            # Kiá»ƒm tra cÃ³ chá»©a domain legal khÃ´ng
            is_legal_domain = any(domain in url for domain in self.LEGAL_DOMAINS)
            
            # Kiá»ƒm tra cÃ³ chá»©a indicators khÃ´ng
            has_legal_indicator = any(indicator in title or indicator in snippet 
                                     for indicator in self.LEGAL_INDICATORS)
            
            # Kiá»ƒm tra cÃ³ chá»©a keywords khÃ´ng
            keyword_words = keywords_lower.split()
            keyword_matches = sum(1 for word in keyword_words if word in title or word in snippet)
            has_keywords = keyword_matches > 0
            
            # TÃ­nh Ä‘iá»ƒm
            score = 0
            if is_priority_domain:
                score += 20  # Äiá»ƒm cá»±c cao cho cÃ¡c trang Æ°u tiÃªn
            elif is_legal_domain:
                score += 10
            
            if has_legal_indicator:
                score += 5
            
            # Äiá»ƒm keywords theo tá»· lá»‡ match
            score += keyword_matches * 2
            
            if 'pdf' in url or 'pdf' in title:
                score += 7
            
            if 'dá»± tháº£o' in title or 'du thao' in title or 'du-thao' in url:
                score += 8  # TÄƒng Ä‘iá»ƒm cho dá»± tháº£o
            
            if 'quochoi.vn' in url or 'duthaoonline' in url:
                score += 15  # Bonus cho Quá»‘c há»™i
            
            # Chá»‰ giá»¯ káº¿t quáº£ cÃ³ Ä‘iá»ƒm >= 8
            if score >= 8:
                result['relevance_score'] = score
                filtered.append(result)
        
        # Sáº¯p xáº¿p theo Ä‘iá»ƒm giáº£m dáº§n
        filtered.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        
        return filtered

    def _extract_pdfs_from_url(self, url: str, keywords: str) -> List[Dict[str, str]]:
        """TrÃ­ch xuáº¥t PDF links tá»« má»™t URL"""
        try:
            # Sá»­ dá»¥ng web_crawler_tool Ä‘á»ƒ tÃ¬m PDF
            result = web_crawler_tool.find_pdf_links(url)
            
            if not result.success:
                return []
            
            pdf_links = result.data.get('pdf_links', [])
            
            # Lá»c PDF links liÃªn quan Ä‘áº¿n keywords
            keywords_lower = keywords.lower()
            relevant_pdfs = []
            
            for pdf in pdf_links:
                pdf_url = pdf.get('url', '')
                pdf_text = pdf.get('text', '').lower()
                
                # Kiá»ƒm tra liÃªn quan
                is_relevant = any(word in pdf_text or word in pdf_url.lower() 
                                 for word in keywords_lower.split())
                
                # Hoáº·c chá»©a legal indicators
                has_legal = any(indicator in pdf_text or indicator in pdf_url.lower()
                               for indicator in self.LEGAL_INDICATORS)
                
                if is_relevant or has_legal:
                    relevant_pdfs.append(pdf)
            
            return relevant_pdfs if relevant_pdfs else pdf_links[:3]  # Fallback: láº¥y 3 PDFs Ä‘áº§u
            
        except Exception as e:
            logger.error(f"Error extracting PDFs from {url}: {str(e)}")
            return []

    def _deduplicate_pdfs(self, pdf_links: List[Dict]) -> List[Dict]:
        """Loáº¡i bá» PDF trÃ¹ng láº·p"""
        seen_urls = set()
        unique_pdfs = []
        
        for pdf in pdf_links:
            url = pdf.get('url', '')
            # Chuáº©n hÃ³a URL (bá» fragment vÃ  query params Ä‘á»ƒ so sÃ¡nh)
            normalized_url = url.split('?')[0].split('#')[0]
            
            if normalized_url and normalized_url not in seen_urls:
                seen_urls.add(normalized_url)
                unique_pdfs.append(pdf)
        
        return unique_pdfs

    def find_best_pdf(self, keywords: str) -> ToolResult:
        """
        TÃ¬m PDF tá»‘t nháº¥t cho keywords.
        
        Args:
            keywords: Tá»« khÃ³a tÃ¬m kiáº¿m
            
        Returns:
            ToolResult vá»›i PDF link tá»‘t nháº¥t
        """
        result = self.search_legal_pdfs(keywords, max_results=5)
        
        if not result.success:
            return result
        
        pdf_links = result.data.get('pdf_links', [])
        
        if not pdf_links:
            return ToolResult(
                success=False,
                error="No PDF found for the given keywords"
            )
        
        # Tráº£ vá» PDF Ä‘áº§u tiÃªn (cÃ³ Ä‘iá»ƒm cao nháº¥t)
        best_pdf = pdf_links[0]
        
        return ToolResult(
            success=True,
            data={
                'pdf_link': best_pdf,
                'alternatives': pdf_links[1:3] if len(pdf_links) > 1 else []
            }
        )


# Singleton instance
legal_pdf_finder_tool = LegalPDFFinderTool()
