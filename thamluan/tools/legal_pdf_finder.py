"""
Legal PDF Finder Tool - Tá»± Ä‘á»™ng tÃ¬m kiáº¿m vÃ  táº£i PDF Luáº­t/Nghá»‹ Ä‘á»‹nh tá»« keywords.
"""

import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
import logging
from urllib.parse import urljoin, quote_plus
import re

from core.config import config
from core.types import ToolResult
from tools.search_engine import search_engine_tool
from tools.web_crawler import web_crawler_tool

logger = logging.getLogger(__name__)


class LegalPDFFinderTool:
    """Tool Ä‘á»ƒ tÃ¬m kiáº¿m vÃ  phÃ¡t hiá»‡n PDF Luáº­t/Nghá»‹ Ä‘á»‹nh tá»« keywords"""
    
    # Danh sÃ¡ch cÃ¡c trang web chÃ­nh thá»©c vá» Luáº­t/Nghá»‹ Ä‘á»‹nh Viá»‡t Nam
    LEGAL_DOMAINS = [
        'thuvienphapluat.vn',
        'luatvietnam.vn',
        'moj.gov.vn',  # Bá»™ TÆ° PhÃ¡p
        'chinhphu.vn',  # Cá»•ng thÃ´ng tin chÃ­nh phá»§
        'mpi.gov.vn',  # Bá»™ Káº¿ hoáº¡ch vÃ  Äáº§u tÆ°
        'mof.gov.vn',  # Bá»™ TÃ i chÃ­nh
        'most.gov.vn',  # Bá»™ Khoa há»c vÃ  CÃ´ng nghá»‡
        'mic.gov.vn',  # Bá»™ ThÃ´ng tin vÃ  Truyá»n thÃ´ng
        'na.gov.vn',  # Quá»‘c há»™i
        'vietnamlawmagazine.vn',
        'mst.gov.vn',  # Bá»™ TÃ i chÃ­nh
    ]
    
    # Tá»« khÃ³a chá»‰ thá»‹ tÃ i liá»‡u luáº­t
    LEGAL_INDICATORS = [
        'dá»± tháº£o',
        'luáº­t',
        'nghá»‹ Ä‘á»‹nh',
        'thÃ´ng tÆ°',
        'quyáº¿t Ä‘á»‹nh',
        'nghá»‹ quyáº¿t',
        'vÄƒn báº£n',
        'phÃ¡p luáº­t',
        'du thao',
        'nghi dinh',
    ]

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': config.USER_AGENT
        })

    def search_legal_pdfs(self, keywords: str, max_results: int = 10) -> ToolResult:
        """
        TÃ¬m kiáº¿m PDF Luáº­t/Nghá»‹ Ä‘á»‹nh dá»±a trÃªn keywords.
        
        Args:
            keywords: Tá»« khÃ³a tÃ¬m kiáº¿m (vÃ­ dá»¥: "Luáº­t Khoa há»c cÃ´ng nghá»‡ 2025")
            max_results: Sá»‘ lÆ°á»£ng káº¿t quáº£ tá»‘i Ä‘a
            
        Returns:
            ToolResult vá»›i danh sÃ¡ch PDF links tÃ¬m Ä‘Æ°á»£c
        """
        try:
            logger.info(f"ðŸ” Searching for legal PDFs with keywords: {keywords}")
            
            # BÆ°á»›c 1: Táº¡o search queries tá»‘i Æ°u
            search_queries = self._generate_legal_search_queries(keywords)
            logger.info(f"ðŸ“‹ Generated {len(search_queries)} search queries")
            
            # BÆ°á»›c 2: TÃ¬m kiáº¿m trÃªn Google vá»›i site: operators
            all_results = []
            for query in search_queries[:3]:  # Giá»›i háº¡n 3 queries Ä‘á»ƒ trÃ¡nh rate limit
                logger.info(f"ðŸ”Ž Searching: {query}")
                
                search_result = search_engine_tool.search(
                    query=query,
                    num_results=max_results,
                    search_engine="google"
                )
                
                if search_result.success:
                    results = search_result.data.get('results', [])
                    all_results.extend(results)
                    logger.info(f"  âœ“ Found {len(results)} results")
                
                # Chá» má»™t chÃºt Ä‘á»ƒ trÃ¡nh rate limit
                import time
                time.sleep(1)
            
            # BÆ°á»›c 3: Lá»c vÃ  xáº¿p háº¡ng káº¿t quáº£
            filtered_results = self._filter_legal_results(all_results, keywords)
            logger.info(f"ðŸ“Š Filtered to {len(filtered_results)} relevant results")
            
            # BÆ°á»›c 4: TÃ¬m PDF links tá»« cÃ¡c trang web
            pdf_links = []
            for result in filtered_results[:max_results]:
                url = result.get('url', '')
                logger.info(f"ðŸ”— Checking URL: {url}")
                
                pdfs = self._extract_pdfs_from_url(url, keywords)
                if pdfs:
                    pdf_links.extend(pdfs)
                    logger.info(f"  âœ“ Found {len(pdfs)} PDF(s)")
                
                # Dá»«ng náº¿u Ä‘Ã£ cÃ³ Ä‘á»§ káº¿t quáº£
                if len(pdf_links) >= max_results:
                    break
            
            # Loáº¡i bá» trÃ¹ng láº·p
            unique_pdfs = self._deduplicate_pdfs(pdf_links)
            logger.info(f"âœ… Total unique PDFs found: {len(unique_pdfs)}")
            
            return ToolResult(
                success=True,
                data={
                    'pdf_links': unique_pdfs[:max_results],
                    'total_found': len(unique_pdfs),
                    'search_queries': search_queries
                }
            )
            
        except Exception as e:
            logger.error(f"Legal PDF search error: {str(e)}", exc_info=True)
            return ToolResult(
                success=False,
                error=f"Failed to search legal PDFs: {str(e)}"
            )

    def _generate_legal_search_queries(self, keywords: str) -> List[str]:
        """Táº¡o cÃ¡c search queries tá»‘i Æ°u cho tÃ¬m kiáº¿m luáº­t"""
        queries = []
        
        # Query 1: TÃ¬m trÃªn cÃ¡c site chÃ­nh thá»©c vá»›i "filetype:pdf"
        site_operators = ' OR '.join([f'site:{domain}' for domain in self.LEGAL_DOMAINS[:5]])
        queries.append(f'{keywords} filetype:pdf ({site_operators})')
        
        # Query 2: TÃ¬m dá»± tháº£o + PDF
        queries.append(f'dá»± tháº£o {keywords} filetype:pdf')
        
        # Query 3: TÃ¬m vá»›i tá»« khÃ³a "táº£i vá»" hoáº·c "download"
        queries.append(f'{keywords} (táº£i vá» OR download) PDF')
        
        # Query 4: TÃ¬m vÄƒn báº£n phÃ¡p luáº­t
        queries.append(f'vÄƒn báº£n {keywords} PDF site:thuvienphapluat.vn OR site:luatvietnam.vn')
        
        # Query 5: TÃ¬m trÃªn trang ChÃ­nh phá»§/Bá»™
        queries.append(f'{keywords} PDF site:chinhphu.vn OR site:moj.gov.vn OR site:most.gov.vn')
        
        return queries

    def _filter_legal_results(self, results: List[Dict], keywords: str) -> List[Dict]:
        """Lá»c vÃ  xáº¿p háº¡ng káº¿t quáº£ theo Ä‘á»™ liÃªn quan"""
        filtered = []
        keywords_lower = keywords.lower()
        
        for result in results:
            url = result.get('url', '').lower()
            title = result.get('title', '').lower()
            snippet = result.get('snippet', '').lower()
            
            # Kiá»ƒm tra cÃ³ chá»©a domain legal khÃ´ng
            is_legal_domain = any(domain in url for domain in self.LEGAL_DOMAINS)
            
            # Kiá»ƒm tra cÃ³ chá»©a indicators khÃ´ng
            has_legal_indicator = any(indicator in title or indicator in snippet 
                                     for indicator in self.LEGAL_INDICATORS)
            
            # Kiá»ƒm tra cÃ³ chá»©a keywords khÃ´ng
            has_keywords = any(word in title or word in snippet 
                              for word in keywords_lower.split())
            
            # TÃ­nh Ä‘iá»ƒm
            score = 0
            if is_legal_domain:
                score += 10
            if has_legal_indicator:
                score += 5
            if has_keywords:
                score += 3
            if 'pdf' in url or 'pdf' in title:
                score += 7
            if 'dá»± tháº£o' in title or 'du thao' in title:
                score += 5
            
            # Chá»‰ giá»¯ káº¿t quáº£ cÃ³ Ä‘iá»ƒm >= 8
            if score >= 8:
                result['relevance_score'] = score
                filtered.append(result)
        
        # Sáº¯p xáº¿p theo Ä‘iá»ƒm giáº£m dáº§n
        filtered.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        
        return filtered

    def _extract_pdfs_from_url(self, url: str, keywords: str) -> List[Dict[str, str]]:
        """TrÃ­ch xuáº¥t PDF links tá»« má»™t URL"""
        try:
            # Sá»­ dá»¥ng web_crawler_tool Ä‘á»ƒ tÃ¬m PDF
            result = web_crawler_tool.find_pdf_links(url)
            
            if not result.success:
                return []
            
            pdf_links = result.data.get('pdf_links', [])
            
            # Lá»c PDF links liÃªn quan Ä‘áº¿n keywords
            keywords_lower = keywords.lower()
            relevant_pdfs = []
            
            for pdf in pdf_links:
                pdf_url = pdf.get('url', '')
                pdf_text = pdf.get('text', '').lower()
                
                # Kiá»ƒm tra liÃªn quan
                is_relevant = any(word in pdf_text or word in pdf_url.lower() 
                                 for word in keywords_lower.split())
                
                # Hoáº·c chá»©a legal indicators
                has_legal = any(indicator in pdf_text or indicator in pdf_url.lower()
                               for indicator in self.LEGAL_INDICATORS)
                
                if is_relevant or has_legal:
                    relevant_pdfs.append(pdf)
            
            return relevant_pdfs if relevant_pdfs else pdf_links[:3]  # Fallback: láº¥y 3 PDFs Ä‘áº§u
            
        except Exception as e:
            logger.error(f"Error extracting PDFs from {url}: {str(e)}")
            return []

    def _deduplicate_pdfs(self, pdf_links: List[Dict]) -> List[Dict]:
        """Loáº¡i bá» PDF trÃ¹ng láº·p"""
        seen_urls = set()
        unique_pdfs = []
        
        for pdf in pdf_links:
            url = pdf.get('url', '')
            # Chuáº©n hÃ³a URL (bá» fragment vÃ  query params Ä‘á»ƒ so sÃ¡nh)
            normalized_url = url.split('?')[0].split('#')[0]
            
            if normalized_url and normalized_url not in seen_urls:
                seen_urls.add(normalized_url)
                unique_pdfs.append(pdf)
        
        return unique_pdfs

    def find_best_pdf(self, keywords: str) -> ToolResult:
        """
        TÃ¬m PDF tá»‘t nháº¥t cho keywords.
        
        Args:
            keywords: Tá»« khÃ³a tÃ¬m kiáº¿m
            
        Returns:
            ToolResult vá»›i PDF link tá»‘t nháº¥t
        """
        result = self.search_legal_pdfs(keywords, max_results=5)
        
        if not result.success:
            return result
        
        pdf_links = result.data.get('pdf_links', [])
        
        if not pdf_links:
            return ToolResult(
                success=False,
                error="No PDF found for the given keywords"
            )
        
        # Tráº£ vá» PDF Ä‘áº§u tiÃªn (cÃ³ Ä‘iá»ƒm cao nháº¥t)
        best_pdf = pdf_links[0]
        
        return ToolResult(
            success=True,
            data={
                'pdf_link': best_pdf,
                'alternatives': pdf_links[1:3] if len(pdf_links) > 1 else []
            }
        )


# Singleton instance
legal_pdf_finder_tool = LegalPDFFinderTool()
