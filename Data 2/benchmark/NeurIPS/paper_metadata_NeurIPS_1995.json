{
    "0": {
        "TITLE": "Learning to Predict Visibility and Invisibility from Occlusion Events",
        "AUTHORS": "Jonathan A. Marshall, Richard K. Alley, Robert S. Hubbard",
        "ABSTRACT": "Visual  occlusion  events  constitute  a  major source  of depth  information.  This paper presents a self-organizing neural network that learns to detect,  represent,  and predict the visibility and invisibility relationships that arise  during  occlusion  events,  after  a  period  of exposure  to  motion sequences  containing occlusion  and  disocclusion events.  The  network develops  two  parallel opponent  channels  or  \"chains\"  of lateral  excitatory  connections  for  every  resolvable  motion trajectory.  One  channel,  the  \"On\"  chain or  \"visible\"  chain, is  activated when a moving stimulus is visible.  The other  channel, the  \"Off\"  chain or  \"invisible\"  chain,  carries  a persistent,  amodal  representation that predicts the motion of a formerly visible stimulus that  becomes  invisible  due  to occlusion.  The  learning  rule  uses  disinhibition  from  the  On  chain  to  trigger  learning  in  the  Off  chain.  The  On  and  Off chain  neurons  can  learn  separate  associations  with  object  depth  or(cid:173) dering.  The results  are closely  related  to the  recent  discovery  (Assad  &  Maunsell,  1995)  of neurons in macaque monkey posterior  parietal cortex  that respond selectively to inferred  motion of invisible stimuli. \n1 \nINTRODUCTION:  LEARNING ABOUT  OCCLUSION  EVENTS \nVisual  occlusion  events  constitute  a  major source  of depth  information.  Yet  lit(cid:173) tle  is  known  about  the  neural  mechanisms  by  which  visual  systems  use  occlusion  events  to infer  the  depth  relations  among visual objects.  What is  the structure  of  such  mechanisms?  Some possible  answers  to this question  are revealed  through  an  analysis of learning rules that can cause  such  mechanisms to self-organize. \nEvidence  from  psychophysics  (Kaplan,  1969;  Nakayama  &  Shimojo,  1992;  Nakayama,  Shimojo,  &  Silverman,  1989;  Shimojo,  Silverman,  &  Nakayama,  1988,  1989;  Yonas,  Craton,  &  Thompson,  1987)  and  neurophysiology  (Assad  &  Maunsell,  1995;  Frost,  1993)  suggests  that  the  process  of  determining  relative  depth  from  occlusion  events  operates  at  an early stage  of visual  processing.  Mar(cid:173) shall  (1991)  describes  evidence  that suggests  that the same early  processing  mech(cid:173) anisms maintain a representation of temporarily occluded objects for  some amount \nLearning to  Predict Visibility  and  Invisibility  from  Occlusion  Events \n817 \nof time after  they  have  disappeared  behind  an  occluder,  and  that  these  represen(cid:173) tations of invisible objects  interact  with other  object  representations,  in  much  the  same  manner  as  do  representations  of visible  objects.  The  evidence  includes  the  phenomena of kinetic subjective contours (Kellman & Cohen,  1984), motion viewed  through a slit (Parks' Camel)  (Parks,  1965) , illusory occlusion  (Ramachandran, In(cid:173) ada, & Kiama, 1986) , and interocular occlusion sequencing  (Shimojo, Silverman, &  Nakayama, 1988).  2  PERCEPTION OF OCCLUSION  AND \nDISOCCLUSION  EVENTS:  AN ANALYSIS \nThe neural network model exploits the visual changes that occur at occlusion bound(cid:173) aries to form a mechanism for detecting and representing object visibility/invisibility  information.  The set of learning  rules  used  in  this model is  an extended  version  of  one  that has  been  used  before  to describe  the  formation of neural  mechanisms for  a  variety  of other  visual  processing  functions  (Hubbard  &  Marshall,  1994;  Mar(cid:173) shall,  1989,  1990ac,  1991,  1992;  Martin &  Marshall,  1993). \nOur  analysis  is  derived  from  the  following  visual  predictivity principle,  which \nmay be postulated  as  a  fundamental principle  of neural organization in visual sys(cid:173) tems:  Visual  systems  represent the  world in  terms of predictions of its  appearance,  and they reorganize themselves  to generate  better predictions.  To maximize the cor(cid:173) rectness  and completeness of its predictions, a visual system  would  need  to predict  the motions and visibility/invisibility of all objects in a scene.  Among other things,  it would  need  to predict the disappearance of an object moving behind an occluder  and the reappearance  of an object emerging from  behind  an occluder. \nA  consequence  of this  postulate  is  that occluded  objects  must,  at  some  level, \ncontinue  to  be  represented  even  though  they  are  invisible.  Moreover,  the  repre(cid:173) sentation of an  object  must  distinguish  whether  the  object  is  visible  or  invisible;  otherwise, the visual system could not determine whether its representations predict  visibility  or  invisibility,  which  would  contravene  the  predictivity  principle.  Thus,  simple single-channel prediction schemes like the one  described  by  Marshall (1989,  1990a)  are inadequate to represent  occlusion  and disocclusion events.  3  A  MODEL FOR GROUNDED LEARNING TO \nPREDICT VISIBILITY AND  INVISIBILITY \nThe initial structure  of the  Visible/Invisible network  model is  given  in  Figure  1A.  The network self-organizes in  response  to a  training regime containing many input  sequences  representing motion with and without occlusion and disocclusion events.  After  a  period  of self-organization,  the  specific  connections  that  a  neuron  receives  (Figure  1B)  determine whether it responds  to visible or invisible objects.  A neuron  that  responds  to  visible  objects  would  have  strong  bottom-up input  connections,  and it would  also have strong time-delayed lateral excitatory input connections.  A  neuron that responds selectively to invisible objects would  not have strong bottom(cid:173) up connections, but it would have strong lateral excitatory input connections.  These  lateral inputs would transmit to the neuron evidence that a previously visible object  existed.  The neurons  that respond  to invisible objects  must operate in  a  way  that  allows lateral  input  excitation  alone  to activate  the  neurons  supraliminally, in  the  absence of bottom-up input excitation from  actual visible objects.  4  SIMULATION OF A  SIMPLIFIED  NETWORK  4.1 \nINITIAL  NETWORK STRUCTURE \nThe  simulated  network,  shown  in  Figure  2,  describes  a  simplified  one(cid:173) dimensional  subnetwork  (Marshall  &  Alley,  1993)  of  the  more  general  two(cid:173) dimensional  network.  Layer  1  is  restricted  to  a  set  of motion-sensitive  neurons  corresponding  to one rightward motion trajectory. \nThe  L+  connections  in  the  simulation  have  a  signal  transmission  latency  of  one  time unit.  Restricting  the lateral connections  to  a  single  time delay  and  to a  single direction limits the simulation to representing  a single speed  and direction of  motion; these results are therefore preliminary. This restriction reduced the number  of connections  and  made the simulation much faster. \n818 \nJ. A. MARSHALL, R.  K. ALLEY, R. S. HUBBARD",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/00e26af6ac3b1c1c49d7c3d79c60d000-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/00e26af6ac3b1c1c49d7c3d79c60d000-Bibtex.bib",
            "SUPP": ""
        }
    },
    "1": {
        "TITLE": "Onset-based Sound Segmentation",
        "AUTHORS": "Leslie S. Smith",
        "ABSTRACT": "A technique for  segmenting sounds using processing based on mam(cid:173) malian  early  auditory  processing  is  presented.  The  technique  is  based on features in sound which neuron spike recording suggests  are  detected in  the  cochlear  nucleus.  The  sound  signal  is  band(cid:173) passed  and  each  signal  processed  to  enhance  onsets  and  offsets.  The onset and offset signals are compressed, then clustered both in  time and  across  frequency  channels  using  a  network  of integrate(cid:173) and-fire  neurons.  Onsets  and  offsets  are  signalled  by  spikes,  and  the timing of these spikes used to segment the sound.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/021bbc7ee20b71134d53e20206bd6feb-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/021bbc7ee20b71134d53e20206bd6feb-Bibtex.bib",
            "SUPP": ""
        }
    },
    "2": {
        "TITLE": "Beating a Defender in Robotic Soccer: Memory-Based Learning of a Continuous Function",
        "AUTHORS": "Peter Stone, Manuela M. Veloso",
        "ABSTRACT": "Learning how to adjust to an opponent's position is critical to  the success of having intelligent agents collaborating towards the  achievement of specific tasks in unfriendly environments. This pa(cid:173) per describes our work on a Memory-based technique for to choose  an action based on a continuous-valued state attribute indicating  the position of an opponent. We investigate the question of how an  agent performs in nondeterministic variations of the training situ(cid:173) ations. Our experiments indicate that when the random variations  fall within some bound of the initial training, the agent performs  better with some initial training rather than from a tabula-rasa.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/03f544613917945245041ea1581df0c2-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/03f544613917945245041ea1581df0c2-Bibtex.bib",
            "SUPP": ""
        }
    },
    "3": {
        "TITLE": "A Neural Network Autoassociator for Induction Motor Failure Prediction",
        "AUTHORS": "Thomas Petsche, Angelo Marcantonio, Christian Darken, Stephen Jose Hanson, Gary M. Kuhn, N. Iwan Santoso",
        "ABSTRACT": "We  present results  on  the  use  of neural  network based  autoassociators  which  act  as  novelty  or anomaly  detectors  to  detect  imminent  motor  failures.  The autoassociator is  trained  to  reconstruct  spectra obtained  from the healthy motor.  In laboratory tests, we have demonstrated that the  trained autoassociator has a small reconstruction error on measurements  recorded from healthy motors but a larger error on those recorded from a  motor with a fault.  We have designed and built a motor monitoring system  using an  autoassociator for anomaly detection and are in the process of  testing the system at three industrial and commercial sites.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/062ddb6c727310e76b6200b7c71f63b5-Bibtex.bib",
            "SUPP": ""
        }
    },
    "4": {
        "TITLE": "The Gamma MLP for Speech Phoneme Recognition",
        "AUTHORS": "Steve Lawrence, Ah Chung Tsoi, Andrew D. Back",
        "ABSTRACT": "We  define  a  Gamma  multi-layer  perceptron  (MLP)  as  an  MLP  with the usual synaptic weights replaced by gamma filters  (as pro(cid:173) posed by de Vries and Principe  (de Vries and Principe,  1992)) and  associated  gain  terms  throughout  all  layers.  We  derive  gradient  descent  update equations and apply  the  model  to  the recognition  of speech  phonemes.  We  find  that  both  the  inclusion  of gamma  filters  in  all  layers,  and  the  inclusion  of synaptic  gains,  improves  the  performance  of the  Gamma  MLP.  We  compare  the  Gamma  MLP  with  TDNN,  Back-Tsoi FIR MLP,  and Back-Tsoi I1R  MLP  architectures, and a  local approximation scheme.  We find  that the  Gamma MLP results in an substantial reduction  in error rates.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/0768281a05da9f27df178b5c39a51263-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/0768281a05da9f27df178b5c39a51263-Bibtex.bib",
            "SUPP": ""
        }
    },
    "5": {
        "TITLE": "Laterally Interconnected Self-Organizing Maps in Hand-Written Digit Recognition",
        "AUTHORS": "Yoonsuck Choe, Joseph Sirosh, Risto Miikkulainen",
        "ABSTRACT": "An  application  of  laterally  interconnected  self-organizing  maps  (LISSOM)  to handwritten  digit recognition  is  presented.  The lat(cid:173) eral connections  learn the correlations of activity between units on  the  map.  The  resulting  excitatory  connections  focus  the  activity  into local patches and the inhibitory connections decorrelate redun(cid:173) dant activity on the map.  The map thus forms internal representa(cid:173) tions that are easy to recognize with e.g.  a perceptron network.  The  recognition rate on a subset of NIST database 3 is 4.0% higher with  LISSOM  than  with  a  regular  Self-Organizing  Map  (SOM)  as  the  front  end,  and 15.8% higher than recognition of raw input bitmaps  directly.  These results form a promising starting point for  building  pattern  recognition systems with a  LISSOM  map as  a  front  end.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/09c6c3783b4a70054da74f2538ed47c6-Bibtex.bib",
            "SUPP": ""
        }
    },
    "6": {
        "TITLE": "Improved Silicon Cochlea using Compatible Lateral Bipolar Transistors",
        "AUTHORS": "André van Schaik, Eric Fragnière, Eric A. Vittoz",
        "ABSTRACT": "Analog  electronic  cochlear  models  need  exponentially  scaled  filters.  CMOS  Compatible  Lateral  Bipolar  Transistors  (CLBTs)  can  create  exponentially scaled currents when biased using a resistive line with a  voltage  difference  between both  ends  of the  line.  Since these  CLBTs  are  independent  of  the  CMOS  threshold  voltage,  current  sources  implemented  with  CLBTs  are  much  better  matched  than  current  sources  created  with  MOS  transistors  operated  in  weak  inversion.  Measurements  from  integrated  test  chips  are  shown  to  verify  the  improved matching.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Bibtex.bib",
            "SUPP": ""
        }
    },
    "7": {
        "TITLE": "On the Computational Power of Noisy Spiking Neurons",
        "AUTHORS": "Wolfgang Maass",
        "ABSTRACT": "It  has  remained  unknown  whether  one  can  in  principle  carry out  reliable digital  computations with networks of biologically realistic  models  for  neurons.  This  article  presents  rigorous  constructions  for  simulating in  real-time arbitrary given  boolean circuits  and fi(cid:173) nite automata with arbitrarily high reliability by networks of noisy  spiking neurons.  In  addition  we  show  that  with  the  help  of  \"shunting  inhibition\"  even  networks  of very  unreliable  spiking  neurons  can  simulate  in  real-time  any  McCulloch-Pitts  neuron  (or  \"threshold  gate\"),  and  therefore  any  multilayer  perceptron  (or  \"threshold  circuit\")  in  a  reliable  manner.  These  constructions  provide  a  possible  explana(cid:173) tion for  the fact  that biological  neural systems can carry out quite  complex  computations within  100  msec.  It  turns  out that the assumption  that these  constructions require  about the shape of the  EPSP's and  the behaviour of the  noise  are  surprisingly weak.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/0c048b3a434e49e655c1247efb389cec-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/0c048b3a434e49e655c1247efb389cec-Bibtex.bib",
            "SUPP": ""
        }
    },
    "8": {
        "TITLE": "Parallel Optimization of Motion Controllers via Policy Iteration",
        "AUTHORS": "Jefferson A. Coelho Jr., R. Sitaraman, Roderic A. Grupen",
        "ABSTRACT": "This paper describes a  policy iteration algorithm for optimizing the  performance  of a  harmonic function-based  controller  with  respect  to a  user-defined  index.  Value functions  are represented  as poten(cid:173) tial  distributions  over  the  problem  domain,  being  control  policies  represented  as  gradient  fields  over  the  same domain.  All  interme(cid:173) diate  policies  are intrinsically  safe,  i.e.  collisions  are not  promoted  during  the adaptation process.  The algorithm  has  efficient  imple(cid:173) mentation in parallel  SIMD  architectures.  One  potential  applica(cid:173) tion - travel distance  minimization - illustrates  its usefulness.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/0eec27c419d0fe24e53c90338cdc8bc6-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/0eec27c419d0fe24e53c90338cdc8bc6-Bibtex.bib",
            "SUPP": ""
        }
    },
    "9": {
        "TITLE": "Model Matching and SFMD Computation",
        "AUTHORS": "Steven Rehfuss, Dan W. Hammerstrom",
        "ABSTRACT": "In  systems  that  process sensory  data there is  frequently  a  model  matching stage where class hypotheses are combined to recognize a  complex entity.  We introduce a new model of parallelism, the Single  Function  Multiple  Data  (SFMD) model, appropriate to this stage.  SFMD functionality can be added with small hardware expense to  certain existing SIMD architectures, and as an incremental addition  to the programming model.  Adding  SFMD  to an SIMD  machine  will  not  only  allow  faster  model  matching,  but  also  increase  its  flexibility as a  general purpose machine and its scope in performing  the initial stages of sensory processing.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/0f2c9a93eea6f38fabb3acb1c31488c6-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/0f2c9a93eea6f38fabb3acb1c31488c6-Bibtex.bib",
            "SUPP": ""
        }
    },
    "10": {
        "TITLE": "Learning with ensembles: How overfitting can be useful",
        "AUTHORS": "Peter Sollich, Anders Krogh",
        "ABSTRACT": "We  study  the  characteristics  of learning  with ensembles.  Solving  exactly  the  simple  model  of an  ensemble  of linear  students,  we  find  surprisingly  rich  behaviour.  For  learning  in  large  ensembles,  it  is  advantageous  to  use  under-regularized  students,  which  actu(cid:173) ally  over-fit  the  training data.  Globally optimal performance  can  be  obtained by  choosing  the training set  sizes  of the students ap(cid:173) propriately.  For  smaller ensembles,  optimization of the  ensemble  weights  can yield significant improvements in ensemble generaliza(cid:173) tion  performance,  in particular if the  individual students  are  sub(cid:173) ject to noise in the training process.  Choosing students with a wide  range of regularization parameters makes this improvement robust  against changes in the unknown level of noise  in the training data.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/1019c8091693ef5c5f55970346633f92-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/1019c8091693ef5c5f55970346633f92-Bibtex.bib",
            "SUPP": ""
        }
    },
    "11": {
        "TITLE": "Analog VLSI Processor Implementing the Continuous Wavelet Transform",
        "AUTHORS": "R. Timothy Edwards, Gert Cauwenberghs",
        "ABSTRACT": "We present an integrated analog processor for real-time wavelet decom(cid:173) position and reconstruction of continuous temporal signals covering the  audio frequency range. The processor performs complex harmonic modu(cid:173) lation and Gaussian lowpass filtering in 16 parallel channels, each clocked  at a different rate, producing a multiresolution mapping on a logarithmic  frequency scale. Our implementation uses mixed-mode analog and dig(cid:173) ital circuits, oversampling techniques, and switched-capacitor filters to  achieve a wide linear dynamic range while maintaining compact circuit  size and low power consumption. We include experimental results on the  processor and characterize its components separately from measurements  on a single-channel test chip.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/1579779b98ce9edb98dd85606f2c119d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/1579779b98ce9edb98dd85606f2c119d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "12": {
        "TITLE": "A Novel Channel Selection System in Cochlear Implants Using Artificial Neural Network",
        "AUTHORS": "Marwan A. Jabri, Raymond J. Wang",
        "ABSTRACT": "State-of-the-art  speech  processors  in  cochlear  implants  perform  channel selection  using  a spectral maxima strategy.  This strategy  can  lead  to  confusions  when  high  frequency  features  are  needed  to discriminate between  sounds.  We  present  in this paper  a  novel  channel selection strategy based upon pattern recognition which al(cid:173) lows  \"smart\" channel selections to be made.  The proposed strategy  is  implemented using  multi-layer  perceptrons  trained  on  a  multi(cid:173) speaker labelled speech database.  The input to the network are the  energy coefficients of N  energy channels.  The output of the system  are  the indices  of the M  selected  channels.  We  compare  the  performance  of our  proposed  system  to  that  of  spectral maxima strategy, and show that our strategy can produce  significantly better results.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/16e6a3326dd7d868cbc926602a61e4d0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/16e6a3326dd7d868cbc926602a61e4d0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "13": {
        "TITLE": "Rapid Quality Estimation of Neural Network Input Representations",
        "AUTHORS": "Kevin J. Cherkauer, Jude W. Shavlik",
        "ABSTRACT": "The choice of an input representation for a neural network can have  a  profound  impact  on  its  accuracy  in  classifying  novel  instances.  However,  neural networks are typically  computationally expensive  to  train,  making  it  difficult  to  test  large  numbers  of alternative  representations.  This  paper  introduces  fast  quality  measures  for  neural  network  representations,  allowing  one  to  quickly  and  ac(cid:173) curately estimate which  of a  collection  of possible  representations  for  a  problem is  the best.  We show  that our measures for  ranking  representations are more accurate than a previously published mea(cid:173) sure,  based on experiments with  three difficult,  real-world pattern  recognition  problems.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/184260348236f9554fe9375772ff966e-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/184260348236f9554fe9375772ff966e-Bibtex.bib",
            "SUPP": ""
        }
    },
    "14": {
        "TITLE": "Generating Accurate and Diverse Members of a Neural-Network Ensemble",
        "AUTHORS": "David W. Opitz, Jude W. Shavlik",
        "ABSTRACT": "Neural-network  ensembles  have  been  shown  to  be  very  accurate  classification  techniques.  Previous  work  has  shown  that  an effec(cid:173) tive ensemble  should  consist  of networks  that are not  only  highly  correct,  but  ones  that  make  their  errors on  different  parts  of the  input  space  as  well.  Most  existing  techniques,  however,  only  in(cid:173) directly  address  the  problem  of  creating  such  a  set  of  networks.  In  this  paper  we  present  a  technique  called  ADDEMUP  that  uses  genetic  algorithms  to  directly  search  for  an  accurate  and  diverse  set  of trained  networks.  ADDEMUP  works  by  first  creating an ini(cid:173) tial  population,  then  uses  genetic  operators  to  continually  create  new  networks,  keeping the set of networks that are as  accurate as  possible while disagreeing with each other as much as possible.  Ex(cid:173) periments on three DNA problems show that  ADDEMUP  is able to  generate a  set of trained  networks that is  more accurate than sev(cid:173) eral existing  approaches.  Experiments  also  show  that  ADDEMUP  is  able  to  effectively  incorporate  prior  knowledge,  if  available,  to  improve the quality of its ensemble.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/1896a3bf730516dd643ba67b4c447d36-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/1896a3bf730516dd643ba67b4c447d36-Bibtex.bib",
            "SUPP": ""
        }
    },
    "15": {
        "TITLE": "A Model of Spatial Representations in Parietal Cortex Explains Hemineglect",
        "AUTHORS": "Alexandre Pouget, Terrence J. Sejnowski",
        "ABSTRACT": "We have recently developed a theory of spatial representations in  which the position of an object is not encoded in a particular frame  of reference but, instead, involves neurons computing basis func(cid:173) tions of their sensory inputs. This type of representation is able  to perform nonlinear sensorimotor transformations and is consis(cid:173) tent with the response properties of parietal neurons. We now ask  whether the same theory could account for the behavior of human  patients with parietal lesions. These lesions induce a deficit known  as hemineglect that is characterized by a lack of reaction to stimuli  located in the hemispace contralateral to the lesion. A simulated  lesion in a basis function representation was found to replicate three  of the most important aspects of hemineglect: i) The models failed  to cross the leftmost lines in line cancellation experiments, ii) the  deficit affected multiple frames of reference and, iii) it could be  object centered. These results strongly support the basis function  hypothesis for spatial representations and provide a computational  theory of hemineglect at the single cell level.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/1b0114c51cc532ed34e1954b5b9e4b58-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/1b0114c51cc532ed34e1954b5b9e4b58-Bibtex.bib",
            "SUPP": ""
        }
    },
    "16": {
        "TITLE": "Temporal Difference Learning in Continuous Time and Space",
        "AUTHORS": "Kenji Doya",
        "ABSTRACT": "A continuous-time, continuous-state version of the temporal differ(cid:173) ence (TD) algorithm is derived in order to facilitate the application  of reinforcement  learning to real-world  control tasks and  neurobi(cid:173) ological modeling.  An optimal nonlinear feedback control law  was  also  derived  using the derivatives  of the value  function.  The  per(cid:173) formance  of the algorithms  was  tested in a  task of swinging  up  a  pendulum with limited torque.  Both the  \"critic\"  that specifies the  paths to the upright position and the \"actor\"  that works as a non(cid:173) linear feedback  controller  were  successfully implemented by radial  basis function  (RBF)  networks.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/1e1d184167ca7676cf665225e236a3d2-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/1e1d184167ca7676cf665225e236a3d2-Bibtex.bib",
            "SUPP": ""
        }
    },
    "17": {
        "TITLE": "Tempering Backpropagation Networks: Not All Weights are Created Equal",
        "AUTHORS": "Nicol N. Schraudolph, Terrence J. Sejnowski",
        "ABSTRACT": "Terrence J. Sejnowski \nComputational Neurobiology Lab  The Salk Institute for BioI. Studies  San Diego, CA 92186-5800, USA",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/1e6e0a04d20f50967c64dac2d639a577-Bibtex.bib",
            "SUPP": ""
        }
    },
    "18": {
        "TITLE": "Improving Policies without Measuring Merits",
        "AUTHORS": "Peter Dayan, Satinder P. Singh",
        "ABSTRACT": "Performing policy  iteration  in  dynamic  programming should  only  require knowledge of relative rather than absolute measures of the  utility of actions (Werbos,  1991) - what Baird  (1993)  calls the  ad(cid:173) vantages  of actions at states.  Nevertheless, most existing methods  in dynamic programming (including Baird's) compute some form of  absolute utility function .  For smooth problems,  advantages satisfy  two  differential  consistency  conditions  (including  the  requirement  that they be free of curl), and we show that enforcing these can lead  to appropriate policy improvement solely in terms of advantages. \n1",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/208e43f0e45c4c78cafadb83d2888cb6-Bibtex.bib",
            "SUPP": ""
        }
    },
    "19": {
        "TITLE": "Selective Attention for Handwritten Digit Recognition",
        "AUTHORS": "Ethem Alpaydin",
        "ABSTRACT": "Completely  parallel object  recognition  is  NP-complete.  Achieving  a  recognizer  with  feasible  complexity  requires  a  compromise  be(cid:173) tween parallel and sequential processing  where a  system selectively  focuses  on  parts  of a  given  image,  one  after  another.  Successive  fixations  are  generated  to sample the  image and these  samples are  processed  and abstracted  to  generate  a  temporal context  in which  results are integrated over time.  A computational model based on a  partially recurrent feedforward network is proposed and made cred(cid:173) ible  by  testing  on  the  real-world  problem  of recognition  of hand(cid:173) written  digits with encouraging results.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/20b5e1cf8694af7a3c1ba4a87f073021-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/20b5e1cf8694af7a3c1ba4a87f073021-Bibtex.bib",
            "SUPP": ""
        }
    },
    "20": {
        "TITLE": "Learning Model Bias",
        "AUTHORS": "Jonathan Baxter",
        "ABSTRACT": "In  this  paper  the  problem  of learning appropriate  domain-specific  bias is  addressed.  It is shown that this can be achieved by learning  many related  tasks from  the  same domain,  and a  theorem  is  given  bounding the number  tasks that must be learnt.  A  corollary of the  theorem  is  that if the  tasks  are  known to  possess  a  common  inter(cid:173) nal  representation  or  preprocessing  then  the  number  of examples  required  per  task for  good generalisation  when learning n  tasks  si(cid:173) multaneously  scales  like  O(a + ~),  where  O(a)  is  a  bound  on  the  minimum  number  of examples  requred  to  learn  a  single  task,  and  O( a + b)  is  a  bound  on the  number  of examples  required  to  learn  each  task independently.  An  experiment  providing  strong qualita(cid:173) tive  support for  the  theoretical  results  is  reported.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/20d135f0f28185b84a4cf7aa51f29500-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/20d135f0f28185b84a4cf7aa51f29500-Bibtex.bib",
            "SUPP": ""
        }
    },
    "21": {
        "TITLE": "Estimating the Bayes Risk from Sample Data",
        "AUTHORS": "Robert R. Snapp, Tong Xu",
        "ABSTRACT": "A new nearest-neighbor method is described for estimating the Bayes risk  of a multiclass pattern claSSification  problem  from  sample data  (e.g.,  a  classified training set). Although it is assumed that the classification prob(cid:173) lem can be accurately described by sufficiently smooth class-conditional  distributions, neither these distributions, nor the corresponding prior prob(cid:173) abilities of the classes are required.  Thus  this  method can  be applied to  practical problems where the underlying probabilities are not known.  This  method is illustrated using two different pattern recognition problems.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/2290a7385ed77cc5592dc2153229f082-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/2290a7385ed77cc5592dc2153229f082-Bibtex.bib",
            "SUPP": ""
        }
    },
    "22": {
        "TITLE": "A Model of Auditory Streaming",
        "AUTHORS": "Susan L. McCabe, Michael J. Denham",
        "ABSTRACT": "An essential feature of intelligent sensory processing is the ability to  focus on the part of the signal of interest against a background of  distracting signals, and to be able to direct this focus at will. In this  paper the problem of auditory scene segmentation is considered and a  model of the early stages of the process is proposed. The behaviour of  the model is shown to be in agreement with a number of well known  psychophysical results. The principal contribution of this model lies in  demonstrating how streaming might result from interactions between  the tonotopic patterns of activity of input signals and traces of previous  activity which feedback and influence the way in which subsequent  signals are processed.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/24146db4eb48c718b84cae0a0799dcfc-Bibtex.bib",
            "SUPP": ""
        }
    },
    "23": {
        "TITLE": "Context-Dependent Classes in a Hybrid Recurrent Network-HMM Speech Recognition System",
        "AUTHORS": "Dan J. Kershaw, Anthony J. Robinson, Mike Hochberg",
        "ABSTRACT": "A  method  for  incorporating  context-dependent  phone  classes  in  a  connectionist-HMM  hybrid  speech  recognition  system  is  intro(cid:173) duced.  A modular approach is adopted, where single-layer networks  discriminate between different context classes given the phone class  and  the  acoustic data.  The context networks  are combined with a  context-independent  (CI)  network  to  generate  context-dependent  (CD)  phone  probability estimates.  Experiments show  an  average  reduction  in  word  error  rate  of 16%  and  13%  from  the  CI  system  on  ARPA 5,000  word  and  SQALE  20,000  word  tasks respectively.  Due  to improved modelling, the decoding speed  of the  CD system  is  more than twice  as fast  as  the CI system.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/27ed0fb950b856b06e1273989422e7d3-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/27ed0fb950b856b06e1273989422e7d3-Bibtex.bib",
            "SUPP": ""
        }
    },
    "24": {
        "TITLE": "Exploiting Tractable Substructures in Intractable Networks",
        "AUTHORS": "Lawrence K. Saul, Michael I. Jordan",
        "ABSTRACT": "We  develop  a  refined  mean field  approximation for  inference  and  learning  in  probabilistic neural  networks.  Our  mean field  theory,  unlike most, does not assume that the units behave as independent  degrees  of freedom;  instead,  it  exploits  in  a  principled  way  the  existence of large substructures that are computationally tractable.  To  illustrate  the  advantages  of this  framework,  we  show  how  to  incorporate weak higher order interactions into a first-order  hidden  Markov  model,  treating  the  corrections  (but  not  the  first  order  structure)  within mean field  theory.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/285f89b802bcb2651801455c86d78f2a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/285f89b802bcb2651801455c86d78f2a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "25": {
        "TITLE": "Statistical Theory of Overtraining - Is Cross-Validation Asymptotically Effective?",
        "AUTHORS": "Shun-ichi Amari, Noboru Murata, Klaus-Robert Müller, Michael Finke, Howard Hua Yang",
        "ABSTRACT": "A  statistical  theory  for  overtraining  is  proposed.  The  analysis  treats realizable stochastic neural networks, trained with Kullback(cid:173) Leibler loss in the  asymptotic case.  It is shown that the asymptotic  gain  in  the  generalization error  is  small if we  perform early  stop(cid:173) ping, even if we have access to the optimal stopping time.  Consider(cid:173) ing cross-validation stopping we answer the question:  In what ratio  the examples should be divided into training and testing sets in or(cid:173) der  to  obtain  the  optimum  performance.  In  the  non-asymptotic  region  cross-validated early stopping always  decreases  the general(cid:173) ization  error.  Our  large  scale  simulations  done  on  a  CM5  are  in  nice  agreement with our analytical findings.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/299a23a2291e2126b91d54f3601ec162-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/299a23a2291e2126b91d54f3601ec162-Bibtex.bib",
            "SUPP": ""
        }
    },
    "26": {
        "TITLE": "Primitive Manipulation Learning with Connectionism",
        "AUTHORS": "Yoky Matsuoka",
        "ABSTRACT": "Infants' manipulative exploratory behavior within the environment  is a vehicle of cognitive stimulation[McCall 1974].  During this time,  infants practice and perfect sensorimotor patterns that become be(cid:173) havioral modules which will be seriated and imbedded in more com(cid:173) plex actions.  This paper explores the development of such primitive  learning systems  using  an  embodied  light-weight  hand  which  will  be  used for  a  humanoid being  developed  at the  MIT Artificial  In(cid:173) telligence  Laboratory[Brooks and Stein  1993].  Primitive grasping  procedures  are  learned  from  sensory  inputs  using  a  connectionist  reinforcement  algorithm while two submodules preprocess  sensory  data to  recognize  the  hardness  of objects  and  detect  shear  using  competitive  learning  and  back-propagation  algorithm  strategies,  respectively.  This  system  is  not  only  consistent  and  quick  dur(cid:173) ing the initial learning stage,  but also adaptable to new  situations  after  training is completed.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/2b38c2df6a49b97f706ec9148ce48d86-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/2b38c2df6a49b97f706ec9148ce48d86-Bibtex.bib",
            "SUPP": ""
        }
    },
    "27": {
        "TITLE": "Silicon Models for Auditory Scene Analysis",
        "AUTHORS": "John Lazzaro, John Wawrzynek",
        "ABSTRACT": "We  are  developing  special-purpose,  low-power  analog-to-digital  converters  for  speech  and  music  applications,  that  feature  analog  circuit  models  of biological  audition  to  process  the  audio  signal  before  conversion.  This  paper describes  our most recent  converter  design,  and a working system that uses several copies ofthe chip to  compute  multiple representations  of sound  from  an  analog input.  This  multi-representation  system  demonstrates  the  plausibility of  inexpensively implementing an auditory scene  analysis approach to  sound  processing.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/2cfd4560539f887a5e420412b370b361-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/2cfd4560539f887a5e420412b370b361-Bibtex.bib",
            "SUPP": ""
        }
    },
    "28": {
        "TITLE": "Human Face Detection in Visual Scenes",
        "AUTHORS": "Henry A. Rowley, Shumeet Baluja, Takeo Kanade",
        "ABSTRACT": "We  present  a  neural  network-based face  detection  system.  A retinally  connected  neural  network  examines  small  windows  of an  image,  and  decides  whether each  window  contains  a  face.  The  system  arbitrates  between multiple networks to improve performance over a single network.  We  use  a  bootstrap  algorithm for training, which  adds  false  detections  into the training set as  training progresses.  This eliminates the difficult  task  of manually  selecting  non-face  training examples,  which  must  be  chosen  to span the entire space of non-face images.  Comparisons with  another state-of-the-art face  detection system are presented;  our system  has better performance in terms of detection and false-positive rates.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/2f29b6e3abc6ebdefb55456ea6ca5dc8-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/2f29b6e3abc6ebdefb55456ea6ca5dc8-Bibtex.bib",
            "SUPP": ""
        }
    },
    "29": {
        "TITLE": "SPERT-II: A Vector Microprocessor System and its Application to Large Problems in Backpropagation Training",
        "AUTHORS": "John Wawrzynek, Krste Asanovic, Brian Kingsbury, James Beck, David Johnson, Nelson Morgan",
        "ABSTRACT": "We  report  on  our  development  of a  high-performance  system  for  neural  network  and other signal  processing  applications.  We  have  designed  and  implemented  a  vector  microprocessor  and  pack(cid:173) aged  it  as  an  attached  processor  for  a  conventional  workstation.  We  present  performance  comparisons  with  commercial  worksta(cid:173) tions on neural  network  backpropagation training.  The SPERT-II  system  demonstrates  significant  speedups  over  extensively  hand(cid:173) optimization code running on  the workstations.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/31857b449c407203749ae32dd0e7d64a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/31857b449c407203749ae32dd0e7d64a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "30": {
        "TITLE": "Pruning with generalization based weight saliencies: λOBD, λOBS",
        "AUTHORS": "Morten With Pedersen, Lars Kai Hansen, Jan Larsen",
        "ABSTRACT": "Abstract Unavailable",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/3473decccb0509fb264818a7512a8b9b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/3473decccb0509fb264818a7512a8b9b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "31": {
        "TITLE": "Stochastic Hillclimbing as a Baseline Method for Evaluating Genetic Algorithms",
        "AUTHORS": "Ari Juels, Martin Wattenberg",
        "ABSTRACT": "We investigate the effectiveness of stochastic hillclimbing as a baseline for  evaluating the  performance  of genetic  algorithms  (GAs)  as combinato(cid:173) rial function optimizers.  In particular, we address two problems to which  GAs have been applied in the literature:  Koza's ll-multiplexer problem  and the jobshop  problem.  We  demonstrate that simple  stochastic  hill(cid:173) climbing methods are  able  to achieve results  comparable or superior  to  those obtained by the GAs designed to address these two problems.  We  further  illustrate,  in the case  of the jobshop  problem,  how  insights  ob(cid:173) tained in the formulation of a stochastic hillclimbing algorithm can lead  to improvements in the encoding used by a  GA.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/36a1694bce9815b7e38a9dad05ad42e0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/36a1694bce9815b7e38a9dad05ad42e0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "32": {
        "TITLE": "Using the Future to \"Sort Out\" the Present: Rankprop and Multitask Learning for Medical Risk Evaluation",
        "AUTHORS": "Rich Caruana, Shumeet Baluja, Tom Mitchell",
        "ABSTRACT": "A patient visits the doctor; the doctor reviews  the patient's history,  asks questions, makes basic measurements (blood pressure,  .. . ), and  prescribes  tests  or  treatment .  The  prescribed  course  of action  is  based  on  an assessment of patient risk-patients at higher risk  are  given  more  and  faster  attention.  It is  also  sequential- it  is  too  expensive  to  immediately order  all  tests  which  might  later  be  of  value.  This  paper  presents  two  methods  that  together  improve  the  accuracy  of  backprop  nets  on  a  pneumonia  risk  assessment  problem  by  10-50%.  Rankprop improves on backpropagation  with  sum of squares error in ranking patients by risk.  Multitask learning  takes advantage of future lab tests  available in the training set,  but  not  available  in  practice  when  predictions  must  be  made.  Both  methods are broadly  applicable.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/36a16a2505369e0c922b6ea7a23a56d2-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/36a16a2505369e0c922b6ea7a23a56d2-Bibtex.bib",
            "SUPP": ""
        }
    },
    "33": {
        "TITLE": "Exponentially many local minima for single neurons",
        "AUTHORS": "Peter Auer, Mark Herbster, Manfred K. Warmuth",
        "ABSTRACT": "We show that for a single neuron with the logistic function as the transfer  function the number of local  minima of the error function based on the  square loss can grow exponentially in the dimension.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/3806734b256c27e41ec2c6bffa26d9e7-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/3806734b256c27e41ec2c6bffa26d9e7-Bibtex.bib",
            "SUPP": ""
        }
    },
    "34": {
        "TITLE": "An Information-theoretic Learning Algorithm for Neural Network Classification",
        "AUTHORS": "David J. Miller, Ajit V. Rao, Kenneth Rose, Allen Gersho",
        "ABSTRACT": "A  new  learning  algorithm is  developed for  the design  of statistical  classifiers  minimizing  the  rate  of misclassification.  The  method,  which  is  based on  ideas  from  information  theory  and  analogies  to  statistical  physics,  assigns  data to  classes  in  probability.  The  dis(cid:173) tributions  are  chosen  to minimize the  expected  classification  error  while  simultaneously enforcing  the classifier's structure and a level  of \"randomness\"  measured  by  Shannon's entropy.  Achievement of  the classifier structure is  quantified by an  associated cost.  The con(cid:173) strained optimization  problem is  equivalent to the minimization of  a  Helmholtz  free  energy,  and  the  resulting  optimization  method  is  a  basic  extension  of the  deterministic  annealing  algorithm  that  explicitly  enforces  structural  constraints on  assignments  while  re(cid:173) ducing  the  entropy  and  expected  cost  with  temperature.  In  the  limit of low temperature, the error rate is minimized directly and a  hard classifier with the requisite structure is  obtained.  This learn(cid:173) ing algorithm can be used to design a variety of classifier structures.  The approach is  compared with standard methods for  radial basis  function  design  and  is  demonstrated  to  substantially  outperform  other  design  methods  on  several  benchmark  examples,  while  of(cid:173) ten retaining design complexity comparable to,  or only moderately  greater than that of strict descent-based methods. \n592 \nD.  NnLLER.A.RAO.K. ROSE.A. GERSHO",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/38ca89564b2259401518960f7a06f94b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/38ca89564b2259401518960f7a06f94b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "35": {
        "TITLE": "Improving Elevator Performance Using Reinforcement Learning",
        "AUTHORS": "Robert H. Crites, Andrew G. Barto",
        "ABSTRACT": "This paper describes the application of reinforcement learning (RL)  to the difficult  real  world problem of elevator dispatching.  The el(cid:173) evator domain poses a  combination of challenges not seen in  most  RL  research to date.  Elevator systems operate in continuous state  spaces  and in continuous time  as discrete  event  dynamic systems.  Their  states  are  not  fully  observable  and  they  are  nonstationary  due to changing passenger arrival rates.  In addition, we  use a team  of RL  agents,  each  of which is  responsible  for  controlling one  ele(cid:173) vator car.  The team receives  a  global reinforcement  signal  which  appears noisy to each agent due to the effects of the actions of the  other agents, the random nature of the arrivals and the incomplete  observation of the state.  In  spite  of these complications,  we  show  results that in simulation surpass the best of the heuristic elevator  control  algorithms  of which  we  are  aware.  These  results  demon(cid:173) strate  the  power  of RL  on  a  very  large  scale  stochastic  dynamic  optimization problem of practical utility.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/390e982518a50e280d8e2b535462ec1f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/390e982518a50e280d8e2b535462ec1f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "36": {
        "TITLE": "Optimal Asset Allocation using Adaptive Dynamic Programming",
        "AUTHORS": "Ralph Neuneier",
        "ABSTRACT": "In  recent  years,  the  interest  of investors  has  shifted  to  computer(cid:173) ized asset allocation (portfolio management) to exploit the growing  dynamics of the capital markets.  In  this paper,  asset  allocation is  formalized  as  a  Markovian  Decision  Problem  which  can  be  opti(cid:173) mized  by applying dynamic programming or reinforcement learning  based  algorithms.  Using an artificial exchange rate,  the asset  allo(cid:173) cation strategy optimized with reinforcement learning (Q-Learning)  is  shown  to  be  equivalent  to  a  policy  computed  by  dynamic  pro(cid:173) gramming.  The approach is  then tested on the task to invest liquid  capital  in  the  German  stock  market.  Here,  neural  networks  are  used  as  value function  approximators.  The  resulting  asset  alloca(cid:173) tion  strategy  is  superior  to  a  heuristic  benchmark  policy.  This  is  a  further  example  which  demonstrates  the  applicability  of neural  network  based  reinforcement  learning  to  a  problem setting with  a  high  dimensional state space.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/3a15c7d0bbe60300a39f76f8a5ba6896-Bibtex.bib",
            "SUPP": ""
        }
    },
    "37": {
        "TITLE": "Recursive Estimation of Dynamic Modular RBF Networks",
        "AUTHORS": "Visakan Kadirkamanathan, Maha Kadirkamanathan",
        "ABSTRACT": "In this paper, recursive estimation algorithms for dynamic modular  networks  are  developed.  The  models  are  based  on Gaussian  RBF  networks  and  the  gating network  is  considered  in two  stages:  At  first,  it  is  simply  a  time-varying  scalar  and  in  the  second,  it  is  based  on the state, as in the mixture of local experts scheme.  The  resulting  algorithm  uses  Kalman  filter  estimation  for  the  model  estimation and the gating probability estimation.  Both, 'hard' and  'soft' competition based estimation schemes are developed where in  the former, the most probable network is  adapted and in the latter  all networks  are  adapted by  appropriate weighting of the data.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/3b712de48137572f3849aabd5666a4e3-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/3b712de48137572f3849aabd5666a4e3-Bibtex.bib",
            "SUPP": ""
        }
    },
    "38": {
        "TITLE": "A Neural Network Classifier for the I100 OCR Chip",
        "AUTHORS": "John C. Platt, Timothy P. Allen",
        "ABSTRACT": "This paper describes a neural network classifier for  the 11000  chip, which  optically  reads  the  E13B  font  characters  at  the  bottom of checks.  The  first  layer  of  the  neural  network  is  a  hardware  linear  classifier  which  recognizes  the  characters  in  this  font .  A  second  software  neural  layer  is  implemented  on  an  inexpensive  microprocessor  to  clean  up  the  re(cid:173) sults of the first  layer.  The hardware linear  classifier  is  mathematically  specified  using  constraints  and  an  optimization  principle.  The  weights  of the  classifier  are  found  using  the  active  set  method,  similar  to  Vap(cid:173) nik's separating hyperplane algorithm.  In 7.5 minutes ofSPARC 2 time,  the  method solves for  1523  Lagrange mUltipliers,  which  is  equivalent  to  training  on  a  data set  of approximately  128,000  examples.  The  result(cid:173) ing network  performs quite  well:  when  tested  on  a  test set  of 1500  real  checks,  it has  a  99.995%  character accuracy  rate. \n1  A  BRIEF OVERVIEW OF THE 11000  CHIP \nAt Synaptics, we have created the 11000,  an analog VLSI chip that, when combined  with associated software, optically  reads  the E13B font  from  the bottom of checks.  This  E13B  font  is  shown  in  figure  1.  The  overall  architecture  of the  11000  chip  is  shown  in  figure  2.  The 11000  recognizes  checks  hand-swiped  through  a  slot.  A  lens  focuses  the image  of the bottom of the  check  onto the  retina.  The  retina has  circuitry  which  locates  the  vertical  position  of the  characters  on  the  check .  The  retina then  sends  an  image  vertically  centered  around  a  possible  character  to  the  classifier.  The classifier  in  the nooo  has  a  tough job.  It  must  be  very accurate  and immune  to  noise  and ink scribbles  in  the input .  Therefore , we  decided  to use  an  integrated  segmentation  and  recognition  approach  (Martin  &  Pittman,  1992)(Platt,  et  al.,  1992).  When the classifier produces a  strong response,  we  know  that a  character is  horizontally  centered in  the retina. \nA Neural Network Classifier for the 11000 OCR Chip \n939 \nFigure  1: The  E13B font,  as  seen  by  the 11000  chip",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/3eb71f6293a2a31f3569e10af6552658-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/3eb71f6293a2a31f3569e10af6552658-Bibtex.bib",
            "SUPP": ""
        }
    },
    "39": {
        "TITLE": "Modeling Saccadic Targeting in Visual Search",
        "AUTHORS": "Rajesh P. N. Rao, Gregory J. Zelinsky, Mary M. Hayhoe, Dana H. Ballard",
        "ABSTRACT": "Visual cognition depends criticalIy on the ability to make rapid eye movements  known as saccades that orient the fovea over targets of interest in a visual  scene. Saccades are known to be ballistic: the pattern of muscle activation  for foveating a prespecified target location is computed prior to the movement  and visual feedback is precluded. Despite these distinctive properties, there  has been no general model of the saccadic targeting strategy employed by  the human visual system during visual search in natural scenes. This paper  proposes a model for saccadic targeting that uses iconic scene representations  derived from oriented spatial filters at multiple scales. Visual search proceeds  in a coarse-to-fine fashion with the largest scale filter responses being compared  first. The model was empirically tested by comparing its perfonnance with  actual eye movement data from human subjects in a natural visual search task;  preliminary results indicate substantial agreement between eye movements  predicted by the model and those recorded from human subjects.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/3fe78a8acf5fda99de95303940a2420c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/3fe78a8acf5fda99de95303940a2420c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "40": {
        "TITLE": "Plasticity of Center-Surround Opponent Receptive Fields in Real and Artificial Neural Systems of Vision",
        "AUTHORS": "S. Yasui, T. Furukawa, M. Yamada, T. Saito",
        "ABSTRACT": "Despite  the  phylogenic  and  structural  differences,  the  visual  sys(cid:173) tems of different species, whether vertebrate or invertebrate, share  certain functional properties.  The center-surround opponent recep(cid:173) tive field  (CSRF)  mechanism represents  one  such example.  Here,  analogous  CSRFs  are  shown  to  be  formed  in  an  artificial  neural  network which learns to localize contours (edges)  of the luminance  difference.  Furthermore,  when  the  input  pattern  is  corrupted  by  a  background noise,  the CSRFs of the hidden  units  becomes shal(cid:173) lower and broader with decrease of the signal-to-noise ratio (SNR).  The same kind of SNR-dependent plasticity is present in the CSRF  of real visual neurons; in bipolar cells of the carp retina as is shown  here  experimentally,  as  well  as  in  large  monopolar  cells  of the  fly  compound eye  as  was  described  by others.  Also,  analogous  SNR(cid:173) dependent  plasticity  is  shown  to  be  present  in  the  biphasic  flash  responses  (BPFR) of these  artificial  and  biological visual systems .  Thus,  the spatial  (CSRF)  and  temporal  (BPFR) filtering  proper(cid:173) ties with  which a  wide variety of creatures see the world appear to  be optimized for  detectability of changes in space  and time.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/41bfd20a38bb1b0bec75acf0845530a7-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/41bfd20a38bb1b0bec75acf0845530a7-Bibtex.bib",
            "SUPP": ""
        }
    },
    "41": {
        "TITLE": "Active Gesture Recognition using Learned Visual Attention",
        "AUTHORS": "Trevor Darrell, Alex Pentland",
        "ABSTRACT": "We have developed a foveated  gesture recognition system that runs  in an unconstrained office environment with an active camera.  Us(cid:173) ing vision routines previously implemented for  an interactive envi(cid:173) ronment,  we  determine  the  spatial  location  of salient  body  parts  of a  user  and guide  an active  camera to obtain images of gestures  or expressions.  A hidden-state reinforcement  learning paradigm is  used  to implement visual  attention.  The  attention  module selects  targets  to foveate  based on the goal of successful  recognition,  and  uses  a  new  multiple-model  Q-Iearning  formulation.  Given  a  set  of target  and  distractor  gestures,  our  system  can  learn  where  to  foveate  to maximally discriminate a  particular gesture.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/43baa6762fa81bb43b39c62553b2970d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/43baa6762fa81bb43b39c62553b2970d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "42": {
        "TITLE": "On Neural Networks with Minimal Weights",
        "AUTHORS": "Vasken Bohossian, Jehoshua Bruck",
        "ABSTRACT": "Linear threshold elements are the basic building blocks of artificial  neural  networks.  A  linear  threshold element computes  a  function  that is  a sign of a weighted sum of the input variables.  The weights  are  arbitrary  integers;  actually,  they  can  be  very  big  integers-(cid:173) exponential  in  the  number  of  the  input  variables.  However,  in  practice,  it  is  difficult  to  implement  big  weights.  In  the  present  literature  a  distinction  is  made  between  the  two  extreme  cases:  linear threshold functions  with polynomial-size weights as opposed  to those  with exponential-size weights.  The  main  contribution  of  this paper is  to fill  up  the gap by further  refining  that separation.  Namely,  we  prove that the class of linear threshold functions  with  polynomial-size  weights  can  be  divided  into  subclasses  according  to the degree of the polynomial.  In fact,  we  prove  a  more general  that there exists a minimal weight linear threshold function  result- for  any arbitrary number of inputs and any  weight  size.  To prove  those results we  have developed a  novel technique for  constructing  linear threshold functions  with minimal weights.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/43dd49b4fdb9bede653e94468ff8df1e-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/43dd49b4fdb9bede653e94468ff8df1e-Bibtex.bib",
            "SUPP": ""
        }
    },
    "43": {
        "TITLE": "Neural Networks with Quadratic VC Dimension",
        "AUTHORS": "Pascal Koiran, Eduardo D. Sontag",
        "ABSTRACT": "This paper shows that neural  networks  which  use  continuous acti(cid:173) vation functions have VC  dimension at least as large as the square  of  the  number  of weights  w.  This  result  settles  a  long-standing  open  question,  namely whether  the well-known  O( w log w)  bound,  known for hard-threshold nets, also held for  more general sigmoidal  nets.  Implications for  the number of samples needed for  valid gen(cid:173) eralization are  discussed.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/456ac9b0d15a8b7f1e71073221059886-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/456ac9b0d15a8b7f1e71073221059886-Bibtex.bib",
            "SUPP": ""
        }
    },
    "44": {
        "TITLE": "Factorial Hidden Markov Models",
        "AUTHORS": "Zoubin Ghahramani, Michael I. Jordan",
        "ABSTRACT": "We present a framework for learning in hidden Markov models with  distributed  state  representations.  Within  this framework ,  we  de(cid:173) rive  a  learning algorithm based  on  the  Expectation-Maximization  (EM)  procedure for  maximum likelihood estimation.  Analogous to  the  standard  Baum-Welch  update  rules,  the  M-step  of our  algo(cid:173) rithm is  exact and can be solved analytically.  However,  due  to the  combinatorial nature of the  hidden state representation,  the exact  E-step is intractable.  A simple and tractable mean field approxima(cid:173) tion is  derived.  Empirical results on a set of problems suggest that  both  the mean field  approximation and Gibbs sampling are viable  alternatives to  the computationally expensive exact algorithm.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/4588e674d3f0faf985047d4c3f13ed0d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/4588e674d3f0faf985047d4c3f13ed0d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "45": {
        "TITLE": "Extracting Tree-Structured Representations of Trained Networks",
        "AUTHORS": "Mark Craven, Jude W. Shavlik",
        "ABSTRACT": "A  significant  limitation  of  neural  networks  is  that  the  represen(cid:173) tations  they  learn  are  usually  incomprehensible  to  humans.  We  present a  novel algorithm , TREPAN,  for extracting comprehensible,  symbolic representations from  trained neural  networks.  Our algo(cid:173) rithm uses  queries to induce a  decision tree that approximates the  concept  represented by  a  given  network.  Our experiments demon(cid:173) strate that TREPAN is  able to produce decision trees that maintain  a high level of fidelity to their respective networks while being com(cid:173) prehensible  and  accurate.  Unlike  previous  work  in  this  area,  our  algorithm is  general in its applicability and scales well  to large net(cid:173) works  and problems with  high-dimensional input spaces.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/45f31d16b1058d586fc3be7207b58053-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/45f31d16b1058d586fc3be7207b58053-Bibtex.bib",
            "SUPP": ""
        }
    },
    "46": {
        "TITLE": "Improving Committee Diagnosis with Resampling Techniques",
        "AUTHORS": "Bambang Parmanto, Paul W. Munro, Howard R. Doyle",
        "ABSTRACT": "Central to the performance improvement of a committee relative to  individual networks is the error correlation between networks in the  committee.  We  investigated  methods  of achieving  error  indepen(cid:173) dence between the networks by training the networks with different  resampling sets  from  the  original training set.  The  methods were  tested  on the sinwave artificial task and the real-world problems of  hepatoma (liver cancer)  and breast  cancer  diagnoses.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/46072631582fc240dd2674a7d063b040-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/46072631582fc240dd2674a7d063b040-Bibtex.bib",
            "SUPP": ""
        }
    },
    "47": {
        "TITLE": "The Capacity of a Bump",
        "AUTHORS": "Gary William Flake",
        "ABSTRACT": "Recently, several researchers have reported encouraging experimental re(cid:173) sults when using Gaussian or bump-like activation functions in multilayer  perceptrons.  Networks of this type usually require fewer  hidden layers  and  units and often learn much  faster  than typical  sigmoidal networks.  To  explain these  results we consider a hyper-ridge network,  which is a  simple perceptron with no hidden units and a rid¥e activation function. If  we are interested in partitioningp points in d dimensions into two classes  then in the limit as d approaches infinity the capacity of a hyper-ridge and  a perceptron is identical.  However, we show that for p  ~ d, which is the  usual case in practice, the ratio of hyper-ridge to perceptron dichotomies  approaches pl2(d + 1).",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/47a658229eb2368a99f1d032c8848542-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/47a658229eb2368a99f1d032c8848542-Bibtex.bib",
            "SUPP": ""
        }
    },
    "48": {
        "TITLE": "Boosting Decision Trees",
        "AUTHORS": "Harris Drucker, Corinna Cortes",
        "ABSTRACT": "We introduce a constructive,  incremental learning system for regression  problems that models data by means of locally linear experts.  In contrast  to  other approaches,  the  experts  are  trained  independently  and  do  not  compete for data during learning.  Only when a prediction for  a query  is  required  do  the  experts  cooperate  by  blending  their  individual  predic(cid:173) tions.  Each expert is trained by  minimizing  a penalized local cross vali(cid:173) dation error using second order methods. In this way, an expert is able to  find a local distance metric by adjusting the size and  shape of the recep(cid:173) tive field in which its predictions are valid, and also to detect relevant in(cid:173) put features  by  adjusting  its bias  on  the  importance of individual input  dimensions. We derive asymptotic results for our method. In a variety of  simulations the properties of the algorithm are demonstrated with respect  to  interference,  learning  speed,  prediction  accuracy,  feature  detection,  and task oriented incremental learning.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/4a08142c38dbe374195d41c04562d9f8-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/4a08142c38dbe374195d41c04562d9f8-Bibtex.bib",
            "SUPP": ""
        }
    },
    "49": {
        "TITLE": "Some results on convergent unlearning algorithm",
        "AUTHORS": "Serguei A. Semenov, Irina B. Shuvalova",
        "ABSTRACT": "In  this  paper  we  consider  probabilities of different  asymptotics of  convergent  unlearning  algorithm for  the  Hopfield-type  neural  net(cid:173) work  (Plakhov  &  Semenov,  1994)  treating  the  case  of unbiased  random  patterns.  We  show  also  that  failed  unlearning  results  in  total memory breakdown.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/4a213d37242bdcad8e7300e202e7caa4-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/4a213d37242bdcad8e7300e202e7caa4-Bibtex.bib",
            "SUPP": ""
        }
    },
    "50": {
        "TITLE": "Forward-backward retraining of recurrent neural networks",
        "AUTHORS": "Andrew W. Senior, Anthony J. Robinson",
        "ABSTRACT": "This  paper  describes  the  training  of  a  recurrent  neural  network  as  the  letter  posterior  probability estimator for  a  hidden  Markov  model,  off-line  handwriting recognition  system.  The network  esti(cid:173) mates  posterior  distributions for  each  of a  series  of frames  repre(cid:173) senting  sections  of a  handwritten  word.  The  supervised  training  algorithm,  backpropagation through time,  requires  target outputs  to  be  provided for  each  frame.  Three  methods for  deriving  these  targets  are  presented.  A  novel  method  based  upon  the  forward(cid:173) backward  algorithm is  found  to  result  in  the  recognizer  with  the  lowest  error  rate.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/4ca82782c5372a547c104929f03fe7a9-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/4ca82782c5372a547c104929f03fe7a9-Bibtex.bib",
            "SUPP": ""
        }
    },
    "51": {
        "TITLE": "KODAK lMAGELINK™ OCR Alphanumeric Handprint Module",
        "AUTHORS": "Alexander Shustorovich, Christopher W. Thrasher",
        "ABSTRACT": "This  paper  describes  the  Kodak  Imageliok TM  OCR  alphanumeric  handprint  module.  There  are  two  neural  network  algorithms  at  its  cme:  the  first  network  is  trained  to  find  individual  characters  in  an  alphamuneric  field,  while  the  second one perfmns the  classification.  Both  networks  were  trained  on  Gabor  projections  of  the  ociginal  pixel  images,  which  resulted  in higher  recognition  rates  and  greater  noise  immunity.  Compared  its  purely  numeric  counterpart  (Shusurovich  and  Thrasher,  1995),  this  version  of the  system  has  a  significant  applicatim  specific  postprocessing  module.  The  system  has  been  implemented in specialized parallel hardware,  which allows  it to run at 80 char/sec/board.  It has been installed  at the Driver and  Vehicle  Licensing  Agency  (DVLA)  in  the  United  Kingdom.  and  its  overall  success  rate  exceeds  96%  (character  level  without  rejects).  which  translates  into  85%  field  rate.  If approximately  20%  of the  fields  are  rejected.  the  system  achieves  99.8%  character  and  99.5%  field success rate. \nto",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/4da04049a062f5adfe81b67dd755cecc-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/4da04049a062f5adfe81b67dd755cecc-Bibtex.bib",
            "SUPP": ""
        }
    },
    "52": {
        "TITLE": "Predictive Q-Routing: A Memory-based Reinforcement Learning Approach to Adaptive Traffic Control",
        "AUTHORS": "Samuel P. M. Choi, Dit-Yan Yeung",
        "ABSTRACT": "In  this  paper,  we  propose  a  memory-based  Q-Iearning  algorithm  called  predictive  Q-routing  (PQ-routing)  for  adaptive  traffic  con(cid:173) trol.  We attempt to address two problems encountered in Q-routing  (Boyan  &  Littman,  1994),  namely,  the inability to fine-tune  rout(cid:173) ing policies under  low  network  load and  the  inability to learn  new  optimal  policies  under  decreasing  load  conditions.  Unlike  other  memory-based  reinforcement  learning  algorithms  in  which  mem(cid:173) ory  is  used  to  keep  past  experiences  to  increase  learning  speed,  PQ-routing  keeps  the  best  experiences  learned  and  reuses  them  by  predicting  the  traffic  trend.  The  effectiveness  of  PQ-routing  has been  verified  under various network  topologies and traffic con(cid:173) ditions.  Simulation  results  show  that  PQ-routing  is  superior  to  Q-routing in terms of both learning speed and  adaptability.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/4e2545f819e67f0615003dd7e04a6087-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/4e2545f819e67f0615003dd7e04a6087-Bibtex.bib",
            "SUPP": ""
        }
    },
    "53": {
        "TITLE": "High-Performance Job-Shop Scheduling With A Time-Delay TD(λ) Network",
        "AUTHORS": "Wei Zhang, Thomas G. Dietterich",
        "ABSTRACT": "Abstract Unavailable",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/4f16c818875d9fcb6867c7bdc89be7eb-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/4f16c818875d9fcb6867c7bdc89be7eb-Bibtex.bib",
            "SUPP": ""
        }
    },
    "54": {
        "TITLE": "Family Discovery",
        "AUTHORS": "Stephen M. Omohundro",
        "ABSTRACT": "\"Family discovery\"  is the task of learning the dimension and struc(cid:173) ture  of a  parameterized  family  of  stochastic  models.  It is  espe(cid:173) cially appropriate when the training examples are partitioned into  \"episodes\"  of samples  drawn  from  a  single  parameter  value.  We  present  three  family  discovery  algorithms  based on  surface  learn(cid:173) ing and show that they significantly improve performance over two  alternatives on a  parameterized classification task.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/5055cbf43fac3f7e2336b27310f0b9ef-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/5055cbf43fac3f7e2336b27310f0b9ef-Bibtex.bib",
            "SUPP": ""
        }
    },
    "55": {
        "TITLE": "Modeling Interactions of the Rat's Place and Head Direction Systems",
        "AUTHORS": "A. David Redish, David S. Touretzky",
        "ABSTRACT": "We  have  developed  a  computational  theory  of rodent  navigation  that  includes analogs of the place cell system, the head direction system,  and  path integration.  In this paper we present simulation results showing how  interactions between the place and head direction systems can account for  recent observations about hippocampal  place cell  responses to doubling  and/or rotation of cue cards in a cylindrical arena (Sharp et at.,  1990). \nRodents have multiple internal representations of their relationship to their environment.  They  have,  for example,  a representation of their location (place cells in  the hippocampal  formation,  see  Muller  et  at.,  1991),  and  a  location-independent representation  of their  heading  (head  direction  cells  in  the  postsubiculum and  the  anterior thalamic  nuclei,  see  Taube et at.,  1990; Taube,  1995).  If these  representations  are  to  be  used  for  navigation, they  must  be  aligned  consistently  whenever the animal reenters a familiar environment.  This process was examined in a set  of experiments by Sharp et at.  (1990). \n1  The Sharp et al., 1990 experiment \nRats spent multiple sessions finding food scattered randomly on the floor of a black cylin(cid:173) drical arena with a white cue card along the wall subtending 90°  of arc.  The animals were  not disoriented before entering the arena, and they always entered at the same location:  the  northwest corner.  See  Figure 3a.  Hippocampal  place  fields  were  mapped  by  single-cell  recording.  A variety of probe trials were then  introduced.  When  an  identical second  cue \n62 \nA. D. REDISH, D. S. TOURETZKY",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/522a9ae9a99880d39e5daec35375e999-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/522a9ae9a99880d39e5daec35375e999-Bibtex.bib",
            "SUPP": ""
        }
    },
    "56": {
        "TITLE": "Empirical Entropy Manipulation for Real-World Problems",
        "AUTHORS": "Paul A. Viola, Nicol N. Schraudolph, Terrence J. Sejnowski",
        "ABSTRACT": "No finite sample is sufficient to determine the density, and therefore  the entropy, of a signal directly.  Some assumption about either the  functional form of the density or about its smoothness is necessary.  Both amount to a prior over the space of possible density functions.  By  far  the  most common approach  is  to  assume  that  the  density  has a  parametric form. \nBy  contrast  we  derive  a  differential  learning  rule  called  EMMA  that  optimizes  entropy  by  way  of kernel  density  estimation.  En(cid:173) tropy  and  its  derivative  can  then  be  calculated  by  sampling from  this density  estimate.  The resulting  parameter update rule  is  sur(cid:173) prisingly simple and efficient. \nWe  will  show  how  EMMA  can  be  used  to detect  and  correct  cor(cid:173) ruption  in  magnetic  resonance  images  (MRI).  This  application  is  beyond  the scope  of existing parametric entropy  models.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/537d9b6c927223c796cac288cced29df-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/537d9b6c927223c796cac288cced29df-Bibtex.bib",
            "SUPP": ""
        }
    },
    "57": {
        "TITLE": "A Neural Network Model of 3-D Lightness Perception",
        "AUTHORS": "Luiz Pessoa, William D. Ross",
        "ABSTRACT": "A  neural  network  model  of 3-D  lightness  perception  is  presented  which  builds  upon  the  FACADE  Theory  Boundary  Contour  Sys(cid:173) tem/Feature Contour  System  of Grossberg  and  colleagues.  Early  ratio encoding by  retinal ganglion neurons  as  well  as psychophysi(cid:173) cal  results on constancy  across  different  backgrounds (background  constancy)  are used to provide functional constraints to the theory  and  suggest  a  contrast  negation  hypothesis  which  states that ratio  measures  between  coplanar  regions  are  given  more  weight  in  the  determination  of lightness  of the  respective  regions.  Simulations  of the  model  address  data on  lightness  perception,  including  the  coplanar ratio hypothesis,  the  Benary cross,  and White's illusion.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/53adaf494dc89ef7196d73636eb2451b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "58": {
        "TITLE": "Adaptive Retina with Center-Surround Receptive Field",
        "AUTHORS": "Shih-Chii Liu, Kwabena Boahen",
        "ABSTRACT": "Both vertebrate and invertebrate retinas are highly efficient  in ex(cid:173) tracting contrast independent of the background intensity over five  or  more  decades.  This  efficiency  has  been  rendered  possible  by  the adaptation of the DC operating point to the background inten(cid:173) sity while maintaining high gain  transient responses.  The center(cid:173) surround properties of the retina allows the system  to extract in(cid:173) formation at the edges in the image.  This silicon retina models the  adaptation properties of the receptors and the antagonistic center(cid:173) surround properties of the laminar cells  of the invertebrate retina  and the outer-plexiform layer of the vertebrate retina.  We also illus(cid:173) trate the spatio-temporal responses of the silicon retina on moving  bars.  The  chip  has  59x64  pixels  on  a  6.9x6.8mm2  die  and  it  is  fabricated in 2 J-tm  n-well  technology.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/53c04118df112c13a8c34b38343b9c10-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/53c04118df112c13a8c34b38343b9c10-Bibtex.bib",
            "SUPP": ""
        }
    },
    "59": {
        "TITLE": "Does the Wake-sleep Algorithm Produce Good Density Estimators?",
        "AUTHORS": "Brendan J. Frey, Geoffrey E. Hinton, Peter Dayan",
        "ABSTRACT": "The wake-sleep algorithm (Hinton, Dayan, Frey and Neal 1995) is a rel(cid:173) atively efficient method of fitting  a multilayer stochastic generative  model to high-dimensional data. In addition to  the top-down connec(cid:173) tions in the generative model, it makes use of bottom-up connections for  approximating the probability distribution over the hidden units given  the data, and it trains these bottom-up connections using a simple delta  rule. We use a variety of synthetic and real data sets to compare the per(cid:173) formance of the wake-sleep algorithm with Monte Carlo and mean field  methods for fitting  the same generative model and also compare it with  other models that are less powerful but easier to fit.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/55b1927fdafef39c48e5b73b5d61ea60-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/55b1927fdafef39c48e5b73b5d61ea60-Bibtex.bib",
            "SUPP": ""
        }
    },
    "60": {
        "TITLE": "EM Optimization of Latent-Variable Density Models",
        "AUTHORS": "Christopher M. Bishop, Markus Svensén, Christopher K. I. Williams",
        "ABSTRACT": "There is  currently  considerable interest  in developing general non(cid:173) linear  density  models  based  on  latent,  or  hidden,  variables.  Such  models have the ability to discover the presence of a relatively small  number of underlying  'causes'  which,  acting  in  combination,  give  rise  to the apparent complexity of the observed  data set.  Unfortu(cid:173) nately, to train such models generally requires large computational  effort.  In this paper we  introduce a  novel  latent variable algorithm  which retains the general non-linear capabilities of previous models  but  which  uses  a  training procedure  based  on  the  EM  algorithm.  We  demonstrate  the  performance  of the  model  on  a  toy  problem  and on data from flow  diagnostics for  a  multi-phase oil  pipeline.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/571e0f7e2d992e738adff8b1bd43a521-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/571e0f7e2d992e738adff8b1bd43a521-Bibtex.bib",
            "SUPP": ""
        }
    },
    "61": {
        "TITLE": "Modern Analytic Techniques to Solve the Dynamics of Recurrent Neural Networks",
        "AUTHORS": "A.C.C. Coolen, S. N. Laughton, D. Sherrington",
        "ABSTRACT": "We  describe the use of modern analytical techniques in  solving the  dynamics  of  symmetric  and  nonsymmetric  recurrent  neural  net(cid:173) works  near saturation.  These explicitly take into account the cor(cid:173) relations between  the  post-synaptic  potentials,  and  thereby  allow  for  a  reliable prediction of transients.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/58c54802a9fb9526cd0923353a34a7ae-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/58c54802a9fb9526cd0923353a34a7ae-Bibtex.bib",
            "SUPP": ""
        }
    },
    "62": {
        "TITLE": "A Realizable Learning Task which Exhibits Overfitting",
        "AUTHORS": "Siegfried Bös",
        "ABSTRACT": "In  this  paper we  examine  a  perceptron  learning  task.  The  task  is  realizable  since  it  is  provided  by  another  perceptron  with  identi(cid:173) cal  architecture.  Both  perceptrons  have  nonlinear  sigmoid  output  functions.  The gain  of the output function  determines the level  of  nonlinearity  of the  learning  task.  It is  observed  that  a  high  level  of nonlinearity leads to overfitting. We  give an explanation for  this  rather  surprising  observation  and  develop  a  method  to  avoid  the  overfitting.  This  method  has  two  possible  interpretations,  one  is  learning with noise,  the other cross-validated early stopping. \n1  Learning  Rules  from  Examples \nThe  property  which  makes feedforward  neural  nets  interesting for  many  practical  applications  is  their  ability  to approximate functions,  which  are  given  only  by ex(cid:173) amples.  Feed-forward  networks  with  at  least  one  hidden  layer  of nonlinear  units  are  able  to  approximate  each  continuous function  on  a  N-dimensional  hypercube  arbitrarily  well.  While  the  existence  of neural  function  approximators  is  already  established, there is  still a lack of knowledge about their practical realizations. Also  major problems, which complicate a good realization, like overfitting, need  a better  understanding. \nIn  this  work  we  study  overfitting in  a  one-layer  percept ron  model.  The  model  allows a good theoretical description while it exhibits already a qualitatively similar  behavior as  the multilayer  perceptron. \nA  one-layer  perceptron  has  N  input  units  and  one  output  unit.  Between  input  and output it  has one layer of adjustable weights Wi, (i  =  1, ... ,N). The output  z  is  a possibly nonlinear function  of the weighted sum of inputs Xi,  i.e. \nz  =  g(h) ,  with",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/678a1491514b7f1006d605e9161946b1-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/678a1491514b7f1006d605e9161946b1-Bibtex.bib",
            "SUPP": ""
        }
    },
    "63": {
        "TITLE": "Simulation of a Thalamocortical Circuit for Computing Directional Heading in the Rat",
        "AUTHORS": "Hugh T. Blair",
        "ABSTRACT": "Several  regions  of the  rat  brain  contain  neurons  known  as  head-direc(cid:173) tion celis,  which encode the animal's directional heading during spatial  navigation.  This  paper presents  a  biophysical  model of head-direction  cell  acti vity,  which  suggests  that  a  thalamocortical  circuit  might com(cid:173) pute the  rat's  head  direction  by  integrating  the  angular  velocity  of the  head over time.  The model was implemented using the neural simulator  NEURON, and makes testable predictions about the structure and func(cid:173) tion of the rat head-direction circuit. \n1  HEAD-DIRECTION CELLS  As a rat navigates through space, neurons called head-direction celis encode the animal's  directional  heading in the horizontal plane (Ranck,  1984; Taube, Muller, &  Ranck,  1990).  Head-direction cells have been  recorded in  several  brain areas, including the postsubicu(cid:173) lum (Ranck,  1984) and anterior thalamus (Taube,  1995).  A  variety of theories  have pro(cid:173) posed  that  head-direction  cells  might  play  an  important  role  in  spatial  learning  and  navigation  (Brown  &  Sharp,  1995;  Burgess,  Recce,  &  O'Keefe,  1994;  McNaughton,  Knierim, &  Wilson,  1995; Wan, Touretzky, &  Redish,  1994; Zhang,  1995). \n1.1  BASIC FIRING PROPERTIES \nA head-direction cell fires action potentials only when the rat's head is facing in a particu(cid:173) lar direction with respect to the static surrounding environment, regardless of the animal's  location  within  that environment.  Head-direction cells are not influenced  by the position  of the rat's head with respect to its body, they are only influenced by the direction of the \n*Also at the Yale Neuroengineering and Neuroscience Center (NNC), 5 Science Park North, New",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/69a5b5995110b36a9a347898d97a610e-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/69a5b5995110b36a9a347898d97a610e-Bibtex.bib",
            "SUPP": ""
        }
    },
    "64": {
        "TITLE": "Clustering data through an analogy to the Potts model",
        "AUTHORS": "Marcelo Blatt, Shai Wiseman, Eytan Domany",
        "ABSTRACT": "A  new  approach for  clustering  is  proposed.  This  method is  based  on an analogy to a  physical model;  the  ferromagnetic  Potts model  at thermal equilibrium is  used  as an analog computer for  this hard  optimization problem .  We  do not  assume  any structure  of the  un(cid:173) derlying distribution of the data.  Phase space of the Potts model is  divided  into  three  regions; ferromagnetic,  super-paramagnetic and  paramagnetic phases.  The  region  of interest  is  that  corresponding  to the super-paramagnetic one, where  domains of aligned spins ap(cid:173) pear.  The range of temperatures  where  these structures  are  stable  is  indicated  by  a  non-vanishing magnetic susceptibility.  We  use  a  very  efficient  Monte  Carlo  algorithm  to  measure  the  susceptibil(cid:173) ity  and  the  spin  spin  correlation  function.  The  values  of the  spin  spin  correlation  function,  at  the  super-paramagnetic  phase,  serve  to identify the  partition of the  data points into clusters. \nMany natural phenomena can be viewed as optimization processes,  and the drive to  understand and analyze them yielded powerful  mathematical methods.  Thus when  wishing to solve a hard optimization problem, it may be advantageous to apply these  methods  through  a  physical  analogy.  Indeed,  recently  techniques  from  statistical  physics  have  been  adapted for  solving hard optimization problems  (see  e.g.  Yuille  and Kosowsky,  1994).  In  this work  we formulate the problem of clustering in  terms  of a  ferromagnetic  Potts spin  model.  Using  the  Monte  Carlo method  we  estimate  physical quantities such  as the spin spin correlation function  and the susceptibility,  and  deduce from them the  number of clusters  and cluster  sizes.  Cluster  analysis  is  an  important technique  in exploratory  data analysis  and  is  ap(cid:173) plied in a variety of engineering and scientific disciplines.  The problem of partitionaZ  clustering can  be formally stated  as  follows.  With everyone of i  =  1,2, ... N  pat(cid:173) terns  represented  as  a  point  Xi  in  a  d-dimensional  metric  space,  determine  the  partition  of these  N  points  into  M  groups,  called  clusters,  such  that  points  in  a  cluster are more similar to each other than to points in different  clusters.  The value  of M  also has  to be  determined. \nClustering Data  through  an  Analogy  to  the Potts  Model \n417 \nThe  two  main approaches  to partitional clustering  are  called  parametric and  non(cid:173) parametric.  In  parametric approaches  some knowledge of the  clusters'  structure  is  assumed  (e.g .  each  cluster  can  be  represented  by  a  center  and  a  spread  around  it) .  This assumption is  incorporated in a  global  criterion.  The goal is  to assign  the  data points so  that  the  criterion  is  minimized .  A  typical example is  variance  min(cid:173) imization (Rose, Gurewitz,  and  Fox,  1993) .  On  the other hand,  in  non-parametric  approaches  a  local  criterion is  used  to  build  clusters  by  utilizing local  structure  of  the  data.  For  example,  clusters  can  be formed  by  identifying high-density  regions  in  the  data space  or  by  assigning a  point  and its  K -nearest  neighbors  to the same  cluster.  In  recent  years  many  parametric  partitional  clustering  algorithms rooted  in statistical physics  were  presented  (see  e.g.  Buhmann and  Kiihnel  , 1993).  In the  present  work  we  use  methods of statistical physics  in non-parametric clustering. \nOur  aim is  to  use  a  physical  problem as  an analog to  the  clustering  problem.  The  notion of clusters  comes very  naturally in  Potts spin models  (Wang and Swendsen,  1990) where clusters are closely related to ordered regions of spins.  We place a Potts  spin variable Si  at each point Xi  (that represents one of the patterns), and introduce  a  short  range  ferromagnetic  interaction  Jij  between  pairs of spins,  whose  strength  decreases  as  the inter-spin distance Ilxi - Xj\"  increases.  The system is  governed by  the  Hamiltonian (energy function) \n1i =  - L  hj D8,,8j",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/6a2feef8ed6a9fe76d6b3f30f02150b4-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/6a2feef8ed6a9fe76d6b3f30f02150b4-Bibtex.bib",
            "SUPP": ""
        }
    },
    "65": {
        "TITLE": "Learning Fine Motion by Markov Mixtures of Experts",
        "AUTHORS": "Marina Meila, Michael I. Jordan",
        "ABSTRACT": "Compliant control is a standard method for performing fine  manip(cid:173) ulation tasks, like grasping and assembly, but it requires estimation  of the state of contact  (s.o.c.)  between  the robot  arm and  the  ob(cid:173) jects  involved.  Here  we  present  a  method to learn  a  model of the  movement from  measured  data.  The  method  requires  little or  no  prior  knowledge  and  the  resulting  model  explicitly  estimates  the  s.o.c.  The  current  s.o.c.  is  viewed  as  the  hidden  state  variable  of  a  discrete  HMM.  The  control  dependent  transition  probabilities  between  states  are  modeled  as  parametrized functions  of the mea(cid:173) surement.  We  show  that  their  parameters  can  be  estimated  from  measurements at the same time as the parameters of the movement  in  each  s.o.c.  The learning algorithm is  a  variant of the EM  proce(cid:173) dure.  The  E step  is  computed exactly ; solving  the  M step  exactly  is  not possible  in general.  Here,  gradient ascent  is  used  to produce  an increase  in  likelihood .",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/6c340f25839e6acdc73414517203f5f0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/6c340f25839e6acdc73414517203f5f0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "66": {
        "TITLE": "Stable LInear Approximations to Dynamic Programming for Stochastic Control Problems with Local Transitions",
        "AUTHORS": "Benjamin Van Roy, John N. Tsitsiklis",
        "ABSTRACT": "We  consider  the  solution  to  large  stochastic  control  problems  by  means of methods that rely on compact representations and a vari(cid:173) ant of the value iteration algorithm to compute approximate cost(cid:173) to-go functions.  While  such methods are known  to be unstable in  general, we identify a new class of problems for  which convergence,  as  well  as  graceful  error  bounds,  are  guaranteed.  This  class  in(cid:173) volves linear parameterizations of the cost-to- go function  together  with  an assumption  that the  dynamic  programming operator is  a  contraction  with  respect  to  the  Euclidean  norm  when  applied  to  functions  in  the  parameterized  class.  We  provide  a  special  case  where  this  assumption  is  satisfied,  which  relies  on  the  locality of  transitions in a  state space.  Other cases will  be discussed  in a  full  length version of this paper.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/6d70cb65d15211726dcce4c0e971e21c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/6d70cb65d15211726dcce4c0e971e21c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "67": {
        "TITLE": "Generalisation of A Class of Continuous Neural Networks",
        "AUTHORS": "John Shawe-Taylor, Jieyu Zhao",
        "ABSTRACT": "We  propose  a  way of using  boolean circuits  to perform real valued  computation  in  a  way  that  naturally  extends  their  boolean  func(cid:173) tionality.  The  functionality  of multiple  fan  in  threshold  gates  in  this  model  is  shown  to  mimic  that  of a  hardware implementation  of continuous Neural Networks.  A Vapnik-Chervonenkis dimension  and  sample  size  analysis  for  the  systems  is  performed  giving  best  known  sample sizes  for  a  real  valued  Neural Network.  Experimen(cid:173) tal results confirm the conclusion that the sample sizes required for  the  networks  are significantly  smaller than for  sigmoidal networks.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/6e7d2da6d3953058db75714ac400b584-Bibtex.bib",
            "SUPP": ""
        }
    },
    "68": {
        "TITLE": "Visual gesture-based robot guidance with a modular neural system",
        "AUTHORS": "Enno Littmann, Andrea Drees, Helge Ritter",
        "ABSTRACT": "We report on the development of the modular neural system  \"SEE(cid:173) EAGLE\"  for  the  visual  guidance  of robot  pick-and-place  actions.  Several  neural  networks  are  integrated  to  a  single system  that vi(cid:173) sually  recognizes  human hand  pointing gestures  from  stereo  pairs  of color  video  images.  The output of the  hand recognition stage is  processed  by  a  set  of color-sensitive  neural  networks  to  determine  the cartesian location of the target object that is  referenced  by the  pointing gesture.  Finally, this information is  used  to guide a robot  to grab  the  target  object  and  put  it  at  another  location  that  can  be specified  by a second pointing gesture.  The accuracy of the cur(cid:173) rent  system allows to identify the location of the referenced  target  object to an accuracy of 1 cm  in  a  workspace  area of 50x50 cm.  In  our  current  environment,  this  is  sufficient  to  pick  and  place  arbi(cid:173) trarily positioned target objects within the workspace.  The system  consists  of neural  networks  that  perform  the  tasks  of image seg(cid:173) mentation, estimation of hand  location, estimation of 3D-pointing  direction, object recognition, and necessary  coordinate transforms.  Drawing heavily on the use of learning algorithms, the functions of  all  network modules were  created from  data examples only.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/708f3cf8100d5e71834b1db77dfa15d6-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/708f3cf8100d5e71834b1db77dfa15d6-Bibtex.bib",
            "SUPP": ""
        }
    },
    "69": {
        "TITLE": "Symplectic Nonlinear Component Analysis",
        "AUTHORS": "Lucas C. Parra",
        "ABSTRACT": "Statistically independent features  can be extracted by finding a fac(cid:173) torial representation of a signal distribution.  Principal Component  Analysis  (PCA)  accomplishes  this  for  linear  correlated  and  Gaus(cid:173) sian  distributed signals.  Independent  Component Analysis  (ICA),  formalized  by  Comon (1994),  extracts  features  in  the  case  of lin(cid:173) ear  statistical  dependent  but  not  necessarily  Gaussian  distributed  signals.  Nonlinear  Component Analysis finally should find  a  facto(cid:173) rial  representation  for  nonlinear  statistical  dependent  distributed  signals.  This  paper  proposes  for  this  task  a  novel  feed-forward,  information  conserving,  nonlinear  map  - the  explicit  symplectic  transformations.  It also solves the problem of non-Gaussian output  distributions  by  considering  single  coordinate  higher  order  statis(cid:173) tics.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/731c83db8d2ff01bdc000083fd3c3740-Bibtex.bib",
            "SUPP": ""
        }
    },
    "70": {
        "TITLE": "Independent Component Analysis of Electroencephalographic Data",
        "AUTHORS": "Scott Makeig, Anthony J. Bell, Tzyy-Ping Jung, Terrence J. Sejnowski",
        "ABSTRACT": "Because of the distance between the skull and brain and their differ(cid:173) ent resistivities,  electroencephalographic (EEG) data collected from  any  point  on  the  human scalp  includes  activity  generated  within  a  large  brain area.  This spatial smearing of EEG  data by  volume  conduction  does  not involve significant time delays,  however,  sug(cid:173) gesting that the Independent  Component  Analysis (ICA) algorithm  of Bell and Sejnowski [1]  is suitable for  performing blind source sep(cid:173) aration on EEG data.  The ICA algorithm separates the problem of  source  identification from  that of source  localization.  First  results  of applying the ICA  algorithm to  EEG and  event-related potential  (ERP)  data  collected  during  a  sustained  auditory  detection  task  show:  (1)  ICA training is insensitive to different  random seeds.  (2)  ICA may be used to segregate obvious artifactual EEG components  (line and muscle noise, eye movements) from other sources.  (3) ICA  is  capable of isolating overlapping  EEG  phenomena,  including  al(cid:173) pha and theta bursts and spatially-separable ERP components, to  separate  ICA  channels.  (4)  N onstationarities  in  EEG  and  behav(cid:173) ioral state can be  tracked using ICA  via changes  in  the  amount of  residual  correlation  between  ICA-filtered  output channels. \n146 \nS. MAKEIG, A. l . BELL, T.-P. lUNG, T. l. SEJNOWSKI",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/754dda4b1ba34c6fa89716b85d68532b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/754dda4b1ba34c6fa89716b85d68532b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "71": {
        "TITLE": "Competence Acquisition in an Autonomous Mobile Robot using Hardware Neural Techniques",
        "AUTHORS": "Geoffrey B. Jackson, Alan F. Murray",
        "ABSTRACT": "In  this  paper  we  examine  the  practical  use  of  hardware  neural  networks  in  an  autonomous  mobile  robot.  We  have  developed  a  hardware  neural  system  based  around  a  custom  VLSI  chip,  EP(cid:173) SILON  III,  designed  specifically  for  embedded  hardware  neural  applications.  We  present  here  a  demonstration  application  of an  autonomous mobile robot that highlights the flexibility of this sys(cid:173) tem.  This robot gains basic mobility competence in very few  train(cid:173) ing epochs  using  an  \"instinct-rule\"  training methodology.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Bibtex.bib",
            "SUPP": ""
        }
    },
    "72": {
        "TITLE": "Gaussian Processes for Regression",
        "AUTHORS": "Christopher K. I. Williams, Carl Edward Rasmussen",
        "ABSTRACT": "The Bayesian analysis of neural networks is difficult because a sim(cid:173) ple  prior  over  weights  implies  a  complex  prior  distribution  over  functions .  In this paper we investigate the use of Gaussian process  priors  over  functions,  which  permit  the  predictive  Bayesian  anal(cid:173) ysis  for  fixed  values  of hyperparameters  to  be  carried  out exactly  using matrix operations.  Two methods, using optimization and av(cid:173) eraging (via Hybrid Monte Carlo) over  hyperparameters have been  tested  on  a  number  of  challenging  problems  and  have  produced  excellent  results.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/7cce53cf90577442771720a370c3c723-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/7cce53cf90577442771720a370c3c723-Bibtex.bib",
            "SUPP": ""
        }
    },
    "73": {
        "TITLE": "Reorganisation of Somatosensory Cortex after Tactile Training",
        "AUTHORS": "Rasmus S. Petersen, John G. Taylor",
        "ABSTRACT": "Topographic maps in  primary areas of mammalian cerebral cortex reor(cid:173) ganise as  a result of behavioural  training.  The nature  of this  reorgani(cid:173) sation  seems  consistent  with  the  behaviour  of competitive  neural  net(cid:173) works,  as  has  been  demonstrated  in  the  past  by  computer  simulation.  We model tactile training on the hand representation in primate somato(cid:173) sensory  cortex,  using  the  Neural  Field  Theory  of Amari  and  his  col(cid:173) leagues.  Expressions for  changes in  both receptive field  size  and  mag(cid:173) nification factor are derived,  which  are consistent with  owl  monkey ex(cid:173) periments and make a prediction which goes beyond them.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/7fec306d1e665bc9c748b5d2b99a6e97-Bibtex.bib",
            "SUPP": ""
        }
    },
    "74": {
        "TITLE": "Prediction of Beta Sheets in Proteins",
        "AUTHORS": "Anders Krogh, Soren Kamaric Riis",
        "ABSTRACT": "Most current methods for prediction of protein secondary structure  use a small window of the protein sequence to predict the structure  of the central amino acid.  We describe a new method for prediction  of the  non-local  structure  called  ,8-sheet,  which  consists  of two  or  more  ,8-strands  that  are  connected  by  hydrogen  bonds.  Since,8- strands  are  often  widely separated in the  protein  chain, a network  with two windows is introduced.  After training on a set of proteins  the network  predicts  the sheets  well,  but there are many false  pos(cid:173) itives.  By  using  a  global energy  function  the ,8-sheet  prediction is  combined with a  local prediction of the three secondary structures  a-helix, ,8-strand and coil.  The energy function is minimized using  simulated annealing to give  a  final  prediction.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/818f4654ed39a1c147d1e51a00ffb4cb-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/818f4654ed39a1c147d1e51a00ffb4cb-Bibtex.bib",
            "SUPP": ""
        }
    },
    "75": {
        "TITLE": "Active Learning in Multilayer Perceptrons",
        "AUTHORS": "Kenji Fukumizu",
        "ABSTRACT": "We propose an active learning method with  hidden-unit reduction.  which is  devised specially for multilayer perceptrons (MLP). First,  we  review  our active  learning  method,  and  point  out  that  many  Fisher-information-based  methods  applied  to  MLP  have a  critical  problem:  the  information  matrix  may  be  singular.  To  solve  this  problem, we  derive the singularity condition of an information ma(cid:173) trix, and  propose an active learning technique that is applicable to  MLP.  Its effectiveness is  verified  through experiments.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/8248a99e81e752cb9b41da3fc43fbe7f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/8248a99e81e752cb9b41da3fc43fbe7f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "76": {
        "TITLE": "A model of transparent motion and non-transparent motion aftereffects",
        "AUTHORS": "Alexander Grunewald",
        "ABSTRACT": "A  model  of  human  motion  perception  is  presented.  The  model  contains two stages of direction selective units.  The first stage con(cid:173) tains  broadly  tuned  units,  while  the  second  stage  contains  units  that are  narrowly  tuned.  The  model  accounts  for  the  motion  af(cid:173) tereffect  through  adapting  units  at  the  first  stage  and  inhibitory  interactions at the second stage.  The model explains how two pop(cid:173) ulations of dots moving in slightly different directions are perceived  as  a  single  population  moving  in  the  direction of the  vector  sum,  and how two populations moving in strongly different directions are  perceived as  transparent motion.  The model also explains why the  motion aftereffect in both cases appears as non-transparent motion.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/82b8a3434904411a9fdc43ca87cee70c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/82b8a3434904411a9fdc43ca87cee70c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "77": {
        "TITLE": "Improved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms and Network Averaging",
        "AUTHORS": "Dirk Ormoneit, Volker Tresp",
        "ABSTRACT": "Volker Tresp",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/83fa5a432ae55c253d0e60dbfa716723-Bibtex.bib",
            "SUPP": ""
        }
    },
    "78": {
        "TITLE": "A Practical Monte Carlo Implementation of Bayesian Learning",
        "AUTHORS": "Carl Edward Rasmussen",
        "ABSTRACT": "A  practical  method  for  Bayesian  training  of feed-forward  neural  networks  using  sophisticated  Monte  Carlo  methods  is  presented  and evaluated.  In reasonably small amounts of computer time this  approach  outperforms  other  state-of-the-art  methods  on  5  data(cid:173) limited tasks  from real world  domains.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/84d2004bf28a2095230e8e14993d398d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/84d2004bf28a2095230e8e14993d398d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "79": {
        "TITLE": "A Smoothing Regularizer for Recurrent Neural Networks",
        "AUTHORS": "Lizhong Wu, John E. Moody",
        "ABSTRACT": "We derive a smoothing regularizer for recurrent network models by  requiring robustness in prediction performance to perturbations of  the training data.  The regularizer can be viewed  as  a  generaliza(cid:173) tion of the first order Tikhonov stabilizer to dynamic models.  The  closed-form  expression  of the  regularizer  covers  both time-lagged  and  simultaneous recurrent  nets,  with  feedforward  nets  and  one(cid:173) layer linear nets as special cases.  We  have successfully tested this  regularizer in a  number of case studies and found that it performs  better than standard quadratic weight  decay. \n1",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/8597a6cfa74defcbde3047c891d78f90-Bibtex.bib",
            "SUPP": ""
        }
    },
    "80": {
        "TITLE": "REMAP: Recursive Estimation and Maximization of A Posteriori Probabilities - Application to Transition-Based Connectionist Speech Recognition",
        "AUTHORS": "Yochai Konig, Hervé Bourlard, Nelson Morgan",
        "ABSTRACT": "In  this  paper,  we  introduce  REMAP,  an  approach for  the training  and estimation of posterior probabilities using a recursive algorithm  that is  reminiscent of the EM-based  Forward-Backward  (Liporace  1982)  algorithm  for  the  estimation  of sequence  likelihoods.  Al(cid:173) though  very  general,  the  method  is  developed  in  the  context  of a  statistical  model for  transition-based speech  recognition  using  Ar(cid:173) tificial  Neural  Networks  (ANN)  to  generate  probabilities for  Hid(cid:173) den  Markov  Models  (HMMs).  In  the  new  approach,  we  use  local  conditional posterior probabilities of transitions to estimate global  posterior  probabilities of word  sequences.  Although  we  still  use  ANNs  to  estimate  posterior  probabilities,  the  network  is  trained  with targets that are themselves estimates of local posterior proba(cid:173) bilities.  An  initial experimental result shows a significant decrease  in error-rate in  comparison to a  baseline system.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/883e881bb4d22a7add958f2d6b052c9f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/883e881bb4d22a7add958f2d6b052c9f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "81": {
        "TITLE": "Harmony Networks Do Not Work",
        "AUTHORS": "René Gourley",
        "ABSTRACT": "Harmony  networks  have  been  proposed  as  a  means by  which  con(cid:173) nectionist  models can perform symbolic computation.  Indeed,  pro(cid:173) ponents claim that a harmony network can be built that constructs  parse trees for strings in  a context free  language.  This paper shows  that  harmony  networks  do  not  work  in  the  following  sense:  they  construct  many outputs that are  not valid  parse  trees. \nIn order to show  that the notion of systematicity is compatible with connectionism,  Paul  Smolensky,  Geraldine  Legendre  and  Yoshiro  Miyata  (Smolensky,  Legendre,  and  Miyata  1992;  Smolen sky  1993;  Smolen sky,  Legendre,  and  Miyata  1994)  pro(cid:173) posed a mechanism, \"Harmony Theory,\"  by which connectionist models purportedly  perform  structure  sensitive  operations  without  implementing classical  algorithms.  Harmony theory  describes  a  \"harmony network\"  which,  in the course of reaching a  stable equilibrium, apparently computes parse trees  that are valid according to the  rules  of a  particular context-free grammar. \nHarmony  networks  consist  of four  major  components  which  will  be  explained  in  detail in  Section  1.  The four  components are, \nTensor Representation:  A means to interpret the activation vector of a  connec(cid:173)\ntionist system as  a  parse  tree for  a string in a  context-free  language. \nHarmony:  A  function  that  maps all  possible  parse trees  to the  non-positive inte(cid:173)\ngers so that a  parse tree is  valid  if and only if its harmony is  zero. \nEnergy:  A  function  that  maps  the  set  of activation  vectors  to  the  real  numbers \nand which  is  minimized by  certain  connectionist  networks!. \nRecursive Construction:  A system for  determining  the weight  matrix of a  con(cid:173)\nnectionist  network  so  that if its  activation  vector  is  interpreted  as  a  parse \n1 Smolensky,  Legendre and Miyata use the term  \"harmony\"  to refer to both energy and  harmony.  To distinguish  between them, we  will  use the term that is often used  to describe  the Lyapunov  function  of dynamic  systems,  \"energy\"  (see for  example  Golden  1986).",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/884d79963bd8bc0ae9b13a1aa71add73-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/884d79963bd8bc0ae9b13a1aa71add73-Bibtex.bib",
            "SUPP": ""
        }
    },
    "82": {
        "TITLE": "Learning Sparse Perceptrons",
        "AUTHORS": "Jeffrey C. Jackson, Mark Craven",
        "ABSTRACT": "We  introduce  a  new  algorithm  designed  to  learn  sparse  percep(cid:173) trons over input representations which include high-order features.  Our  algorithm,  which  is  based  on a  hypothesis-boosting  method,  is  able to PAC-learn a  relatively  natural class  of target concepts.  Moreover, the algorithm appears to work well in practice:  on a set  of three  problem  domains,  the algorithm  produces  classifiers  that  utilize  small  numbers  of features  yet  exhibit  good  generalization  performance.  Perhaps most  importantly,  our algorithm generates  concept descriptions that are easy for  humans to understand. \n1",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/8a1e808b55fde9455cb3d8857ed88389-Bibtex.bib",
            "SUPP": ""
        }
    },
    "83": {
        "TITLE": "The Role of Activity in Synaptic Competition at the Neuromuscular Junction",
        "AUTHORS": "Samuel R. H. Joseph, David J. Willshaw",
        "ABSTRACT": "An  extended  version  of the  dual  constraint  model  of motor  end(cid:173) plate morphogenesis is  presented that includes activity dependent  and  independent  competition.  It is  supported by  a  wide  range  of  recent neurophysiological evidence that indicates a strong relation(cid:173) ship  between  synaptic  efficacy  and  survival.  The  computational  model is  justified  at the molecular level  and its predictions match  the developmental and regenerative behaviour of real synapses.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/8a3363abe792db2d8761d6403605aeb7-Bibtex.bib",
            "SUPP": ""
        }
    },
    "84": {
        "TITLE": "Absence of Cycles in Symmetric Neural Networks",
        "AUTHORS": "Xin Wang, Arun K. Jagota, Fernanda Botelho, Max H. Garzon",
        "ABSTRACT": "Abstract Unavailable",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/8b4066554730ddfaa0266346bdc1b202-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/8b4066554730ddfaa0266346bdc1b202-Bibtex.bib",
            "SUPP": ""
        }
    },
    "85": {
        "TITLE": "Explorations with the Dynamic Wave Model",
        "AUTHORS": "Thomas P. Rebotier, Jeffrey L. Elman",
        "ABSTRACT": "Following  Shrager  and  Johnson  (1995)  we  study  growth  of  logi(cid:173) cal  function  complexity  in  a  network  swept  by  two  overlapping  waves:  one  of pruning, and  the  other of Hebbian  reinforcement  of  connections.  Results  indicate  a  significant  spatial  gradient  in  the  appearance  of  both  linearly  separable  and  non  linearly  separable  functions of the two inputs of the network; the n.l.s.  cells are  much  sparser  and  their slope of appearance  is  sensitive  to  parameters  in  a  highly  non-linear  way.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/8ce6790cc6a94e65f17f908f462fae85-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/8ce6790cc6a94e65f17f908f462fae85-Bibtex.bib",
            "SUPP": ""
        }
    },
    "86": {
        "TITLE": "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding",
        "AUTHORS": "Richard S. Sutton",
        "ABSTRACT": "On large  problems,  reinforcement  learning  systems  must  use  parame(cid:173) terized function approximators such as neural networks in order to gen(cid:173) eralize  between similar  situations and actions.  In these cases  there are  no strong theoretical results on the accuracy  of convergence,  and com(cid:173) putational results  have  been  mixed.  In  particular,  Boyan and  Moore  reported at last year's meeting a series of negative results in attempting  to apply dynamic programming together with function approximation  to simple control problems with continuous state spaces.  In this paper,  we present positive results for all the control tasks they attempted, and  for  one that is significantly larger.  The most important differences  are  that  we  used  sparse-coarse-coded  function  approximators  (CMACs)  whereas  they used mostly global function  approximators, and that we  learned  online  whereas  they  learned  offline.  Boyan  and  Moore  and  others  have  suggested  that  the  problems  they  encountered  could  be  solved  by  using  actual  outcomes  (\"rollouts\"),  as  in  classical  Monte  Carlo  methods,  and as in the TD().)  algorithm when). =  1.  However,  in our experiments  this always resulted in substantially poorer perfor(cid:173) mance.  We  conclude  that  reinforcement  learning  can  work  robustly  in  conjunction  with  function  approximators,  and  that  there  is  little  justification at present for  avoiding the case  of general  ).. \n1  Reinforcement  Learning and  Function  Approximation \nReinforcement learning is a  broad class of optimal control methods based on estimating  value functions  from  experience,  simulation,  or  search  (Barto, Bradtke  &;  Singh,  1995;  Sutton,  1988;  Watkins,  1989).  Many  of these  methods,  e.g.,  dynamic  programming  and  temporal-difference  learning,  build  their  estimates  in  part  on  the  basis  of other \nGeneralization  in  Reinforcement Learning",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/8f1d43620bc6bb580df6e80b0dc05c48-Bibtex.bib",
            "SUPP": ""
        }
    },
    "87": {
        "TITLE": "A Multiscale Attentional Framework for Relaxation Neural Networks",
        "AUTHORS": "Dimitris I. Tsioutsias, Eric Mjolsness",
        "ABSTRACT": "We  investigate  the  optimization  of neural  networks  governed  by  general  objective functions.  Practical formulations of such  objec(cid:173) tives  are  notoriously  difficult  to solve;  a  common problem  is  the  poor  local extrema that  result  by  any  of the  applied  methods.  In  this paper, a novel framework is introduced for the solution oflarge(cid:173) scale  optimization problems.  It assumes  little about  the objective  function and can be applied to general nonlinear, non-convex func(cid:173) tions;  objectives  in thousand  of variables  are  thus efficiently  min(cid:173) imized  by  a  combination of techniques  - deterministic  annealing,  multiscale optimization, attention mechanisms and trust region op(cid:173) timization methods.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/93d65641ff3f1586614cf2c1ad240b6c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/93d65641ff3f1586614cf2c1ad240b6c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "88": {
        "TITLE": "VLSI Model of Primate Visual Smooth Pursuit",
        "AUTHORS": "Ralph Etienne-Cummings, Jan Van der Spiegel, Paul Mueller",
        "ABSTRACT": "A  one  dimensional  model  of primate  smooth  pursuit  mechanism  has  been  implemented  in  2  11m  CMOS  VLSI.  The  model  consolidates  Robinson's  negative  feedback  model  with  Wyatt  and  Pola's  positive  feedback scheme, to produce a  smooth  pursuit  system  which  zero's  the  velocity  of a  target on  the  retina.  Furthermore,  the  system  uses  the  current eye  motion  as  a  predictor for  future  target  motion.  Analysis,  stability and biological correspondence of the system are discussed.  For  implementation  at  the  focal  plane,  a  local  correlation  based  visual  motion  detection  technique  is  used.  Velocity  measurements,  ranging  over 4 orders of magnitude with < 15%  variation, provides  the  input  to  the smooth pursuit system.  The  system  performed  successful  velocity  tracking for high contrast scenes.  Circuit design and performance of the  complete smooth pursuit system is presented.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/995e1fda4a2b5f55ef0df50868bf2a8f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/995e1fda4a2b5f55ef0df50868bf2a8f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "89": {
        "TITLE": "Reinforcement Learning by Probability Matching",
        "AUTHORS": "Philip N. Sabes, Michael I. Jordan",
        "ABSTRACT": "We  present  a  new  algorithm  for  associative  reinforcement  learn(cid:173) ing.  The algorithm is based upon the idea of matching a network's  output probability with a probability distribution derived from the  environment's reward signal.  This Probability Matching algorithm  is shown  to perform faster  and  be  less  susceptible to local  minima  than  previously  existing  algorithms.  We  use  Probability  Match(cid:173) ing to train mixture of experts networks,  an architecture for  which  other reinforcement  learning rules fail  to converge reliably on even  simple  problems.  This  architecture  is  particularly  well  suited  for  our  algorithm as it can compute arbitrarily complex functions  yet  calculation of the output probability is  simple.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/9ac403da7947a183884c18a67d3aa8de-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/9ac403da7947a183884c18a67d3aa8de-Bibtex.bib",
            "SUPP": ""
        }
    },
    "90": {
        "TITLE": "Generalized Learning Vector Quantization",
        "AUTHORS": "Atsushi Sato, Keiji Yamada",
        "ABSTRACT": "We  propose  a  new  learning  method,  \"Generalized  Learning  Vec(cid:173) tor Quantization (GLVQ),\"  in which reference vectors are updated  based on the steepest descent method in order to minimize the cost  function .  The  cost  function  is  determined  so  that  the  obtained  learning  rule  satisfies  the  convergence  condition.  We  prove  that  Kohonen's  rule  as  used  in  LVQ  does  not  satisfy  the  convergence  condition  and  thus  degrades  recognition  ability.  Experimental re(cid:173) sults  for  printed  Chinese  character recognition  reveal  that  GLVQ  is  superior to  LVQ  in  recognition  ability.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/9c3b1830513cc3b8fc4b76635d32e692-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/9c3b1830513cc3b8fc4b76635d32e692-Bibtex.bib",
            "SUPP": ""
        }
    },
    "91": {
        "TITLE": "Bayesian Methods for Mixtures of Experts",
        "AUTHORS": "Steve R. Waterhouse, David MacKay, Anthony J. Robinson",
        "ABSTRACT": "We  present  a  Bayesian framework  for  inferring  the  parameters of  a  mixture of experts  model based  on  ensemble  learning  by  varia(cid:173) tional free energy minimisation.  The Bayesian approach avoids the  over-fitting and noise level under-estimation problems of traditional  maximum likelihood inference.  We  demonstrate these  methods on  artificial  problems and sunspot  time series  prediction.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/9da187a7a191431db943a9a5a6fec6f4-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/9da187a7a191431db943a9a5a6fec6f4-Bibtex.bib",
            "SUPP": ""
        }
    },
    "92": {
        "TITLE": "High-Speed Airborne Particle Monitoring Using Artificial Neural Networks",
        "AUTHORS": "Alistair Ferguson, Theo Sabisch, Paul Kaye, Laurence C. Dixon, Hamid Bolouri",
        "ABSTRACT": "Current environmental monitoring systems assume particles to be  spherical, and do not attempt to classify them. A laser-based sys(cid:173) tem developed at the University of Hertfordshire aims at classify(cid:173) ing airborne particles through the generation of two-dimensional  scattering profiles. The pedormances of template matching, and  two types of neural network (HyperNet and semi-linear units) are  compared for image classification. The neural network approach is  shown to be capable of comparable recognition pedormance, while  offering a number of advantages over template matching.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/9f36407ead0629fc166f14dde7970f68-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/9f36407ead0629fc166f14dde7970f68-Bibtex.bib",
            "SUPP": ""
        }
    },
    "93": {
        "TITLE": "Adaptive Mixture of Probabilistic Transducers",
        "AUTHORS": "Yoram Singer",
        "ABSTRACT": "We  introduce and  analyze  a  mixture  model  for  supervised  learning  of  probabilistic transducers.  We  devise an  online learning algorithm  that  efficiently infers the structure and estimates the parameters of each model  in the mixture. Theoretical analysis and comparative simulations indicate  that the learning algorithm tracks the best model from an arbitrarily large  (possibly infinite) pool of models.  We also present an application of the  model for inducing a noun phrase recognizer.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/a0160709701140704575d499c997b6ca-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/a0160709701140704575d499c997b6ca-Bibtex.bib",
            "SUPP": ""
        }
    },
    "94": {
        "TITLE": "SEEMORE: A View-Based Approach to 3-D Object Recognition Using Multiple Visual Cues",
        "AUTHORS": "Bartlett W. Mel",
        "ABSTRACT": "A  neurally-inspired  visual  object  recognition  system  is  described  called  SEEMORE,  whose  goal  is  to  identify  common objects  from  a  large  known  set-independent  of  3-D  viewiag  angle,  distance,  and  non-rigid  distortion.  SEEMORE's  database  consists  of 100  ob(cid:173) jects  that  are  rigid  (shovel),  non-rigid  (telephone  cord),  articu(cid:173) lated  (book), statistical (shrubbery),  and complex (photographs of  scenes).  Recognition results  were  obtained using a  set of 102 color  and shape feature channels within a simple feedforward network ar(cid:173) chitecture.  In  response  to  a  test  set  of 600  novel  test  views  (6  of  each object)  presented individually in color video images, SEEMORE  identified  the  object correctly  97% of the time (chance is 1%)  using  a  nearest  neighbor  classifier.  Similar levels  of performance  were  obtained for  the  subset  of 15  non-rigid objects.  Generalization be(cid:173) havior reveals emergence  of striking natural category structure  not  explicit in  the  input feature  dimensions.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/a0e2a2c563d57df27213ede1ac4ac780-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/a0e2a2c563d57df27213ede1ac4ac780-Bibtex.bib",
            "SUPP": ""
        }
    },
    "95": {
        "TITLE": "Using Pairs of Data-Points to Define Splits for Decision Trees",
        "AUTHORS": "Geoffrey E. Hinton, Michael Revow",
        "ABSTRACT": "Conventional binary classification trees  such  as  CART either split  the data using axis-aligned hyperplanes or  they perform a  compu(cid:173) tationally expensive search  in the continuous space of hyperplanes  with unrestricted orientations.  We show that the limitations of the  former  can  be overcome  without resorting to the latter.  For every  pair of training data-points, there is one hyperplane that is orthog(cid:173) onal  to the line joining the data-points and  bisects  this line.  Such  hyperplanes  are  plausible  candidates  for  splits.  In  a  comparison  on  a suite of 12  datasets  we  found  that this  method of generating  candidate splits outperformed  the standard  methods,  particularly  when  the training sets were  small.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/a113c1ecd3cace2237256f4c712f61b5-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/a113c1ecd3cace2237256f4c712f61b5-Bibtex.bib",
            "SUPP": ""
        }
    },
    "96": {
        "TITLE": "Dynamics of On-Line Gradient Descent Learning for Multilayer Neural Networks",
        "AUTHORS": "David Saad, Sara A. Solla",
        "ABSTRACT": "We  consider  the  problem  of on-line  gradient  descent  learning for  general  two-layer  neural  networks.  An  analytic  solution  is  pre(cid:173) sented  and used  to investigate the  role of the learning  rate in con(cid:173) trolling the evolution and  convergence  of the learning process. \nLearning in layered neural networks refers  to the modification of internal parameters  {J} which specify the strength of the interneuron couplings, so  as  to bring the map  fJ  implemented  by  the  network  as  close  as  possible  to  a  desired  map  1.  The  degree  of success  is  monitored  through  the  generalization  error,  a  measure  of the  dissimilarity between  fJ  and 1.  Consider  maps from  an  N-dimensional  input  space e onto  a  scalar  (,  as  arise  in  the formulation of classification  and  regression  tasks.  Two-layer  networks  with  an  arbitrary  number of hidden  units  have  been  shown  to  be  universal  approximators  [1]  for  such  N-to-one  dimensional  maps.  Information about  the  desired  map i  is  provided  through  independent  examples (e, (1'),  with  (I'  =  i(e) for  all  p .  The  examples are used  to train  a student  network with  N  input units,  K  hidden  units,  and  a  single  linear  output  unit;  the  target  map  i  is  defined  through  a  teacher  network  of  similar  architecture  except  for  the  number  M  of hidden  units.  We  investigate  the  emergence  of generalization  ability  in  an  on-line  learning scenario  [2],  in  which  the  couplings  are  modified  after  the  presentation  of each  example so  as  to minimize the corresponding error.  The resulting changes in {J} are described  as  a  dynamical evolution; the  number of examples plays the role  of time.  In  this  paper  we  limit our  discussion  to  the  case  of the  soft-committee  machine  [2],  in  which  all  the  hidden  units  are  connected  to  the  output  unit  with  positive  couplings  of unit  strength,  and  only  the  input-to-hidden  couplings  are  adaptive. \n*D.Saad@aston.ac.uk  tOn leave  from  AT&T  Bell  Laboratories,  Holmdel,  NJ  07733,  USA \nDynamics of On-line Gradient Descent Learning for Multilayer Neural Networks \n303 \nConsider  the  student  network:  hidden  unit  i  receives  information from  input  unit  r  through  the weight  hr, and  its activation under  presentation of an input pattern  ~ =  (6,· .. ,~N) is  Xi  =  J i  .~,  with  J i  =  (hl, ... ,JiN)  defined  as  the  vector  of  incoming weights  onto the  i-th  hidden  unit.  The output of the student  network  is  a(J,~) =  L:~l  9  (Ji . ~), where  9  is  the  activation function  of the  hidden  units,  taken here  to be the error function g(x) ==  erf(x/V2), and J  ==  {Jdl",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/a1519de5b5d44b31a01de013b9b51a80-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/a1519de5b5d44b31a01de013b9b51a80-Bibtex.bib",
            "SUPP": ""
        }
    },
    "97": {
        "TITLE": "Geometry of Early Stopping in Linear Networks",
        "AUTHORS": "Robert H. Dodier",
        "ABSTRACT": "A theory of early stopping as applied to linear models is presented.  The  backpropagation  learning  algorithm  is  modeled  as  gradient  descent  in  continuous time.  Given  a  training set and  a  validation  set,  all  weight  vectors  found  by  early  stopping must  lie  on  a  cer(cid:173) tain quadric surface, usually an ellipsoid.  Given a training set and  a  candidate early stopping weight  vector,  all  validation  sets  have  least-squares weights lying on a certain plane.  This latter fact  can  be exploited  to estimate  the  probability  of stopping  at any  given  point along the trajectory from the initial weight vector to the least(cid:173) squares weights  derived from  the training set, and to estimate the  probability  that  training  goes  on  indefinitely.  The  prospects  for  extending this theory to nonlinear models are discussed.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/a1d50185e7426cbb0acad1e6ca74b9aa-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/a1d50185e7426cbb0acad1e6ca74b9aa-Bibtex.bib",
            "SUPP": ""
        }
    },
    "98": {
        "TITLE": "A Unified Learning Scheme: Bayesian-Kullback Ying-Yang Machine",
        "AUTHORS": "Lei Xu",
        "ABSTRACT": "A  Bayesian-Kullback  learning scheme,  called Ying-Yang  Machine,  is  proposed  based on  the  two  complement but equivalent Bayesian  representations  for  joint  density  and  their  Kullback  divergence.  Not  only  the  scheme  unifies  existing  major supervised  and  unsu(cid:173) pervised  learnings,  including  the  classical  maximum likelihood  or  least  square learning,  the  maximum information preservation,  the  EM  & em algorithm and information geometry, the recent  popular  Helmholtz  machine,  as  well  as  other  learning  methods  with  new  variants  and  new  results;  but  also  the  scheme  provides  a  number  of new  learning models.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/a2137a2ae8e39b5002a3f8909ecb88fe-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/a2137a2ae8e39b5002a3f8909ecb88fe-Bibtex.bib",
            "SUPP": ""
        }
    },
    "99": {
        "TITLE": "Using Feedforward Neural Networks to Monitor Alertness from Changes in EEG Correlation and Coherence",
        "AUTHORS": "Scott Makeig, Tzyy-Ping Jung, Terrence J. Sejnowski",
        "ABSTRACT": "We  report  here  that  changes  in  the  normalized  electroencephalo(cid:173) graphic  (EEG)  cross-spectrum  can  be  used  in  conjunction  with  feedforward  neural networks to monitor changes in alertness of op(cid:173) erators  continuously  and  in  near-real  time.  Previously,  we  have  shown  that EEG spectral  amplitudes covary with changes in alert(cid:173) ness  as indexed by changes in behavioral error rate on an auditory  detection task [6,4].  Here, we report for the first time that increases  in the frequency  of detection errors  in  this task are  also  accompa(cid:173) nied  by  patterns of increased  and  decreased  spectral  coherence  in  several  frequency  bands  and  EEG  channel  pairs.  Relationships  between  EEG  coherence  and  performance vary  between  subjects,  but within subjects,  their topographic and spectral profiles appear  stable  from  session  to  session.  Changes  in  alertness  also  covary  with  changes  in  correlations  among  EEG  waveforms  recorded  at  different  scalp  sites,  and  neural  networks  can  also  estimate  alert(cid:173) ness  from  correlation  changes  in  spontaneous  and  unobtrusively(cid:173) recorded  EEG signals.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/a26398dca6f47b49876cbaffbc9954f9-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/a26398dca6f47b49876cbaffbc9954f9-Bibtex.bib",
            "SUPP": ""
        }
    },
    "100": {
        "TITLE": "A Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex",
        "AUTHORS": "Olivier J. M. D. Coenen, Terrence J. Sejnowski",
        "ABSTRACT": "The vestibulo-ocular reflex  (VOR)  stabilizes  images  on  the  retina during  rapid  head motions.  The gain of the VOR (the ratio of eye to head rotation velocity)  is  typically around -1  when the eyes are focused on a distant target.  However, to  stabilize images accurately, the VOR gain must vary  with context (eye position,  eye vergence and head  translation).  We  first  describe  a  kinematic  model of the  VOR which relies solely on sensory information available from the semicircular  canals (head rotation), the otoliths (head translation), and neural correlates of eye  position and vergence angle.  We then propose a dynamical model and compare it  to the eye velocity responses measured in monkeys. The dynamical model repro(cid:173) duces the observed amplitude and time course of the modulation of the VOR and  suggests one way to combine the required neural signals within the cerebellum and  the brain stem.  It also makes predictions for the responses of neurons to multiple  inputs (head rotation and translation, eye position, etc.) in the oculomotor system.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/a34bacf839b923770b2c360eefa26748-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/a34bacf839b923770b2c360eefa26748-Bibtex.bib",
            "SUPP": ""
        }
    },
    "101": {
        "TITLE": "Constructive Algorithms for Hierarchical Mixtures of Experts",
        "AUTHORS": "Steve R. Waterhouse, Anthony J. Robinson",
        "ABSTRACT": "We  present  two  additions  to  the  hierarchical  mixture  of experts  (HME)  architecture.  By  applying a  likelihood splitting criteria to  each expert in the HME we  \"grow\" the tree adaptively during train(cid:173) ing.  Secondly,  by considering only the most probable path through  the tree we  may \"prune\"  branches away, either temporarily, or per(cid:173) manently  if they  become  redundant.  We  demonstrate  results  for  the  growing  and  path  pruning  algorithms  which  show  significant  speed  ups  and  more efficient  use  of parameters over  the standard  fixed  structure  in  discriminating  between  two  interlocking spirals  and classifying 8-bit parity patterns.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/a3fb4fbf9a6f9cf09166aa9c20cbc1ad-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/a3fb4fbf9a6f9cf09166aa9c20cbc1ad-Bibtex.bib",
            "SUPP": ""
        }
    },
    "102": {
        "TITLE": "Discovering Structure in Continuous Variables Using Bayesian Networks",
        "AUTHORS": "Reimar Hofmann, Volker Tresp",
        "ABSTRACT": "We  study  Bayesian  networks  for  continuous  variables  using  non(cid:173) linear  conditional  density  estimators.  We  demonstrate  that  use(cid:173) ful  structures  can  be extracted  from  a  data set  in  a  self-organized  way and we  present sampling techniques for  belief update based on  Markov  blanket conditional density models.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/a5e0ff62be0b08456fc7f1e88812af3d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/a5e0ff62be0b08456fc7f1e88812af3d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "103": {
        "TITLE": "Temporal coding in the sub-millisecond range: Model of barn owl auditory pathway",
        "AUTHORS": "Richard Kempter, Wulfram Gerstner, J. Leo van Hemmen, Hermann Wagner",
        "ABSTRACT": "Binaural  coincidence  detection  is  essential  for  the  localization  of  external  sounds  and  requires  auditory signal  processing  with  high  temporal precision.  We present an integrate-and-fire model of spike  processing in the auditory pathway of the barn owl.  It is shown that  a temporal precision in the microsecond range can be achieved with  neuronal  time constants  which  are  at least  one  magnitude longer.  An  important  feature  of our  model  is  an  unsupervised  Hebbian  learning rule which leads to a  temporal fine  tuning of the neuronal  connections. \n·email:  kempter.wgerst.lvh@physik.tu-muenchen.de \nTemporal Coding in  the Submillisecond Range:  Model of Bam Owl Auditory Pathway",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/a8240cb8235e9c493a0c30607586166c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/a8240cb8235e9c493a0c30607586166c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "104": {
        "TITLE": "Stable Dynamic Parameter Adaption",
        "AUTHORS": "Stefan M. Rüger",
        "ABSTRACT": "A stability criterion for dynamic parameter adaptation is  given.  In  the case  of the learning rate of backpropagation,  a  class of stable  algorithms is presented and studied, including a convergence proof.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/a89cf525e1d9f04d16ce31165e139a4b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/a89cf525e1d9f04d16ce31165e139a4b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "105": {
        "TITLE": "Optimizing Cortical Mappings",
        "AUTHORS": "Geoffrey J. Goodhill, Steven Finch, Terrence J. Sejnowski",
        "ABSTRACT": "\"Topographic\"  mappings occur frequently  in the brain.  A  pop(cid:173) ular approach to understanding the structure of such mappings  is  to map points representing input features  in a  space of a  few  dimensions  to points in a  2 dimensional space using some self(cid:173) organizing  algorithm.  We  argue  that a  more general  approach  may be useful,  where similarities between features  are  not con(cid:173) strained to be geometric distances, and the objective function for  topographic matching is chosen explicitly rather than being spec(cid:173) ified implicitly by the self-organizing algorithm.  We  investigate  analytically an example of this more general approach applied to  the  structure of interdigitated mappings,  such as the pattern of  ocular dominance columns in primary visual cortex.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/aace49c7d80767cffec0e513ae886df0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/aace49c7d80767cffec0e513ae886df0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "106": {
        "TITLE": "Experiments with Neural Networks for Real Time Implementation of Control",
        "AUTHORS": "Peter K. Campbell, Michael Dale, Herman L. Ferrá, Adam Kowalczyk",
        "ABSTRACT": "This  paper  describes  a  neural  network  based  controller  for  allocating  capacity in a telecommunications network.  This system was proposed in  order  to  overcome  a  \"real  time\"  response  constraint.  Two  basic  architectures are evaluated:  1) a feedforward network-heuristic and; 2) a  feedforward  network-recurrent  network.  These  architectures  are  compared against a linear programming (LP) optimiser as  a benchmark.  This LP optimiser was  also used as  a teacher to label  the  data samples  for  the feedforward  neural  network training  algorithm.  It is  found  that  the systems  are  able to provide a traffic  throughput  of 99%  and  95%,  respectively,  of  the  throughput  obtained  by  the  linear  programming  solution. Once trained, the neural network based solutions are found in a  fraction of the time required by the LP optimiser.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/abea47ba24142ed16b7d8fbf2c740e0d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "107": {
        "TITLE": "Cholinergic suppression of transmission may allow combined associative memory function and self-organization in the neocortex",
        "AUTHORS": "Michael E. Hasselmo, Milos Cekic",
        "ABSTRACT": "Selective  suppression  of  transmission  at  feedback  synapses  during  learning  is  proposed  as  a  mechanism  for  combining  associative feed(cid:173) back  with  self-organization  of feed forward  synapses.  Experimental  data  demonstrates  cholinergic  suppression  of synaptic  transmission  in  layer I (feedback synapses), and a lack of suppression in layer IV (feed(cid:173) forward synapses).  A network with this feature uses local rules to learn  mappings  which  are not  linearly  separable.  During  learning,  sensory  stimuli  and  desired  response  are  simultaneously  presented  as  input.  Feedforward connections  form  self-organized representations  of input,  while suppressed feedback connections learn  the  transpose of feedfor(cid:173) ward connectivity. During recall, suppression is removed, sensory input  activates  the  self-organized  representation,  and  activity  generates  the  learned response.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/af21d0c97db2e27e13572cbf59eb343d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "108": {
        "TITLE": "Dynamics of Attention as Near Saddle-Node Bifurcation Behavior",
        "AUTHORS": "Hiroyuki Nakahara, Kenji Doya",
        "ABSTRACT": "In  consideration  of attention  as  a  means  for  goal-directed  behav(cid:173) ior in non-stationary environments,  we  argue that the dynamics of  attention  should  satisfy  two  opposing  demands:  long-term  main(cid:173) tenance  and  quick  transition.  These  two  characteristics  are  con(cid:173) tradictory  within  the linear domain.  We  propose  the near saddle(cid:173) node  bifurcation behavior of a  sigmoidal unit with self-connection  as  a  candidate of dynamical mechanism that satisfies both of these  demands.  We  further  show  in  simulations  of  the  'bug-eat-food'  tasks  that  the  near  saddle-node  bifurcation  behavior  of recurrent  networks  can  emerge  as  a functional  property  for  survival in  non(cid:173) stationary environments.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/afdec7005cc9f14302cd0474fd0f3c96-Bibtex.bib",
            "SUPP": ""
        }
    },
    "109": {
        "TITLE": "Softassign versus Softmax: Benchmarks in Combinatorial Optimization",
        "AUTHORS": "Steven Gold, Anand Rangarajan",
        "ABSTRACT": "A  new  technique,  termed  soft assign,  is  applied  for  the  first  time  to  two  classic  combinatorial  optimization  problems,  the  travel(cid:173) ing  salesman  problem  and  graph  partitioning.  Soft assign ,  which  has emerged from  the recurrent  neural  network/statistical physics  framework, enforces  two-way  (assignment) constraints without the  use  of penalty  terms  in  the  energy  functions.  The  soft assign  can  also  be  generalized  from  two-way  winner-take-all  constraints  to  multiple membership constraints which are required for graph par(cid:173) titioning.  The  soft assign  technique  is  compared  to  the  softmax  (Potts  glass).  Within  the  statistical  physics  framework,  softmax  and a penalty term has been a widely used method for enforcing the  two-way constraints common within many combinatorial optimiza(cid:173) tion  problems.  The  benchmarks  present  evidence  that  soft assign  has clear advantages in accuracy, speed,  parallelizabilityand algo(cid:173) rithmic simplicity over softmax and a penalty term in optimization  problems with two-way constraints.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/b1563a78ec59337587f6ab6397699afc-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/b1563a78ec59337587f6ab6397699afc-Bibtex.bib",
            "SUPP": ""
        }
    },
    "110": {
        "TITLE": "From Isolation to Cooperation: An Alternative View of a System of Experts",
        "AUTHORS": "Stefan Schaal, Christopher G. Atkeson",
        "ABSTRACT": "We introduce a constructive,  incremental learning system for regression  problems that models data by means of locally linear experts.  In contrast  to  other approaches,  the  experts  are  trained  independently  and  do  not  compete for data during learning.  Only when a prediction for  a query  is  required  do  the  experts  cooperate  by  blending  their  individual  predic(cid:173) tions.  Each expert is trained by  minimizing  a penalized local cross vali(cid:173) dation error using second order methods. In this way, an expert is able to  find a local distance metric by adjusting the size and  shape of the recep(cid:173) tive field in which its predictions are valid, and also to detect relevant in(cid:173) put features  by  adjusting  its bias  on  the  importance of individual input  dimensions. We derive asymptotic results for our method. In a variety of  simulations the properties of the algorithm are demonstrated with respect  to  interference,  learning  speed,  prediction  accuracy,  feature  detection,  and task oriented incremental learning.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/b4d168b48157c623fbd095b4a565b5bb-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/b4d168b48157c623fbd095b4a565b5bb-Bibtex.bib",
            "SUPP": ""
        }
    },
    "111": {
        "TITLE": "Fast Learning by Bounding Likelihoods in Sigmoid Type Belief Networks",
        "AUTHORS": "Tommi Jaakkola, Lawrence K. Saul, Michael I. Jordan",
        "ABSTRACT": "Sigmoid  type  belief networks,  a  class  of probabilistic  neural  net(cid:173) works,  provide  a  natural  framework  for  compactly  representing  probabilistic  information  in  a  variety  of unsupervised  and super(cid:173) vised  learning  problems.  Often  the  parameters used  in  these  net(cid:173) works  need  to  be  learned  from examples.  Unfortunately,  estimat(cid:173) ing  the  parameters  via  exact  probabilistic  calculations  (i.e,  the  EM-algorithm)  is  intractable  even  for  networks  with  fairly  small  numbers of hidden  units.  We  propose  to  avoid the  infeasibility of  the  E step  by  bounding likelihoods instead of computing them ex(cid:173) actly.  We  introduce extended  and  complementary representations  for  these  networks  and  show  that  the  estimation  of the  network  parameters  can  be  made fast  (reduced  to  quadratic optimization)  by  performing the estimation in either of the  alternative domains.  The  complementary networks  can  be  used  for  continuous  density  estimation as  well.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/b59c67bf196a4758191e42f76670ceba-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/b59c67bf196a4758191e42f76670ceba-Bibtex.bib",
            "SUPP": ""
        }
    },
    "112": {
        "TITLE": "Neural Control for Nonlinear Dynamic Systems",
        "AUTHORS": "Ssu-Hsin Yu, Anuradha M. Annaswamy",
        "ABSTRACT": "A neural network based approach is presented for controlling two distinct  types of nonlinear systems.  The first  corresponds to  nonlinear systems  with  parametric  uncertainties  where  the  parameters occur  nonlinearly.  The second corresponds to  systems  for  which  stabilizing control  struc(cid:173) tures cannot be determined.  The proposed neural controllers are shown  to result in closed-loop system stability under certain conditions.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/b9141aff1412dc76340b3822d9ea6c72-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/b9141aff1412dc76340b3822d9ea6c72-Bibtex.bib",
            "SUPP": ""
        }
    },
    "113": {
        "TITLE": "Sample Complexity for Learning Recurrent Perceptron Mappings",
        "AUTHORS": "Bhaskar DasGupta, Eduardo D. Sontag",
        "ABSTRACT": "Recurrent  perceptron  classifiers  generalize the classical  perceptron  model.  They take into account  those correlations and dependences  among input  coordinates  which  arise  from  linear  digital  filtering.  This paper provides tight bounds on sample complexity associated  to the fitting of such  models to experimental data.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/b9d487a30398d42ecff55c228ed5652b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/b9d487a30398d42ecff55c228ed5652b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "114": {
        "TITLE": "Is Learning The n-th Thing Any Easier Than Learning The First?",
        "AUTHORS": "Sebastian Thrun",
        "ABSTRACT": "This paper investigates learning in  a lifelong context.  Lifelong learning  addresses  situations  in  which  a  learner  faces  a  whole  stream  of learn(cid:173) ing tasks.  Such scenarios provide the opportunity to transfer knowledge  across multiple learning tasks, in order to generalize more accurately from  less  training data.  In  this paper,  several  different approaches  to lifelong  learning  are  described,  and  applied in  an  object recognition domain.  It  is  shown  that  across  the  board,  lifelong learning  approaches  generalize  consistently  more  accurately  from  less  training data,  by  their  ability  to  transfer knowledge across learning tasks.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/bdb106a0560c4e46ccc488ef010af787-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/bdb106a0560c4e46ccc488ef010af787-Bibtex.bib",
            "SUPP": ""
        }
    },
    "115": {
        "TITLE": "Parallel analog VLSI architectures for computation of heading direction and time-to-contact",
        "AUTHORS": "Giacomo Indiveri, Jörg Kramer, Christof Koch",
        "ABSTRACT": "We  describe  two  parallel analog VLSI  architectures  that integrate  optical flow  data obtained from  arrays of elementary velocity  sen(cid:173) sors to estimate heading direction and time-to-contact.  For heading  direction  computation,  we  performed  simulations to  evaluate  the  most important qualitative properties of the optical flow  field  and  determine the best  functional operators for  the  implementation of  the  architecture.  For  time-to-contact we  exploited  the  divergence  theorem  to integrate  data from  all  velocity  sensors  present  in the  architecture  and average out  possible errors.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/c21002f464c5fc5bee3b98ced83963b8-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/c21002f464c5fc5bee3b98ced83963b8-Bibtex.bib",
            "SUPP": ""
        }
    },
    "116": {
        "TITLE": "A Dynamical Systems Approach for a Learnable Autonomous Robot",
        "AUTHORS": "Jun Tani, Naohiro Fukumura",
        "ABSTRACT": "This  paper discusses  how  a  robot  can  learn  goal-directed  naviga(cid:173) tion  tasks  using  local  sensory  inputs.  The  emphasis  is  that  such  learning  tasks  could  be  formulated  as  an  embedding  problem  of  dynamical  systems:  desired  trajectories  in  a  task space  should  be  embedded  into an  adequate  sensory-based internal  state  space  so  that an unique mapping from the internal state space to the motor  command could  be established.  The  paper shows that a  recurrent  neural network suffices in self-organizing such an adequate internal  state space  from  the temporal  sensory input.  In  our experiments,  using  a  real  robot  with  a  laser  range  sensor,  the  robot  navigated  robustly  by achieving dynamical coherence with  the environment.  It was  also  shown  that  such  coherence  becomes  structurally  sta(cid:173) ble  as  the  global  attractor is  self-organized  in the coupling of the  internal and the environmental dynamics. \n1",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/c3e0c62ee91db8dc7382bde7419bb573-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/c3e0c62ee91db8dc7382bde7419bb573-Bibtex.bib",
            "SUPP": ""
        }
    },
    "117": {
        "TITLE": "Optimization Principles for the Neural Code",
        "AUTHORS": "Michael DeWeese",
        "ABSTRACT": "Recent  experiments  show  that  the  neural  codes  at  work  in  a  wide  range of creatures share some common features.  At first sight, these  observations seem unrelated.  However,  we show  that these  features  arise naturally in  a linear filtered  threshold crossing  (LFTC) model  when we set the threshold to maximize the transmitted information.  This maximization process  requires  neural  adaptation  to  not only  the  DC  signal  level,  as  in  conventional  light  and  dark  adaptation,  but also to the statistical structure of the signal and noise distribu(cid:173) tions.  We  also  present  a  new  approach for  calculating the  mutual  information between  a  neuron's  output  spike  train and any aspect  of its  input signal  which  does not require  reconstruction  of the  in(cid:173) put  signal.  This formulation  is  valid  provided  the  correlations  in  the  spike  train are  small, and we  provide  a  procedure  for  checking  this  assumption.  This paper is  based  on joint work  (DeWeese  [1],  1995).  Preliminary  results  from  the  LFTC  model  appeared  in  a  previous  proceedings  (DeWeese  [2],  1995),  and the  conclusions  we  reached  at that time have been reaffirmed by further  analysis of the  model.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/c6036a69be21cb660499b75718a3ef24-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/c6036a69be21cb660499b75718a3ef24-Bibtex.bib",
            "SUPP": ""
        }
    },
    "118": {
        "TITLE": "A Framework for Non-rigid Matching and Correspondence",
        "AUTHORS": "Suguna Pappu, Steven Gold, Anand Rangarajan",
        "ABSTRACT": "Matching feature point sets lies at the core of many approaches to  object recognition. We present a framework for non-rigid match(cid:173) ing that begins with a skeleton module, affine point matching,  and then integrates multiple features to improve correspondence  and develops an object representation based on spatial regions to  model local transformations. The algorithm for feature matching  iteratively updates the transformation parameters and the corre(cid:173) spondence solution, each in turn. The affine mapping is solved in  closed form, which permits its use for data of any dimension. The  correspondence is set via a method for two-way constraint satisfac(cid:173) tion, called softassign, which has recently emerged from the neural  network/statistical physics realm. The complexity of the non-rigid  matching algorithm with multiple features is the same as that of  the affine point matching algorithm. Results for synthetic and real  world data are provided for point sets in 2D and 3D, and for 2D  data with multiple types of features and parts.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/c60d060b946d6dd6145dcbad5c4ccf6f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/c60d060b946d6dd6145dcbad5c4ccf6f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "119": {
        "TITLE": "Hierarchical Recurrent Neural Networks for Long-Term Dependencies",
        "AUTHORS": "Salah El Hihi, Yoshua Bengio",
        "ABSTRACT": "We  have already shown  that extracting long-term dependencies  from  se(cid:173) quential  data is  difficult,  both for  determimstic dynamical systems  such  as  recurrent  networks,  and  probabilistic  models  such  as  hidden  Markov  models  (HMMs)  or input/output hidden  Markov  models  (IOHMMs).  In  practice,  to  avoid  this  problem,  researchers  have  used  domain  specific  a-priori  knowledge  to give  meaning  to  the hidden or  state variables rep(cid:173) resenting  past context.  In  this  paper,  we  propose  to use  a  more general  type  of a-priori  knowledge,  namely  that  the  temporal  dependencIes  are  structured  hierarchically.  This implies that  long-term  dependencies  are  represented  by  variables with  a long time scale.  This principle is  applied  to a recurrent network which includes delays and multiple time scales.  Ex(cid:173) periments confirm the advantages of such  structures.  A similar approach  is  proposed for  HMMs  and IOHMMs.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/c667d53acd899a97a85de0c201ba99be-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/c667d53acd899a97a85de0c201ba99be-Bibtex.bib",
            "SUPP": ""
        }
    },
    "120": {
        "TITLE": "Finite State Automata that Recurrent Cascade-Correlation Cannot Represent",
        "AUTHORS": "Stefan C. Kremer",
        "ABSTRACT": "This  paper  relates  the  computational  power  of Fahlman' s  Recurrent  Cascade Correlation (RCC) architecture to that of fInite  state automata  (FSA).  While some recurrent networks are FSA equivalent, RCC is  not.  The paper presents a theoretical analysis of the  RCC architecture in the  form of a proof describing a large class of FSA which cannot be realized  by  RCC.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/c6bff625bdb0393992c9d4db0c6bbe45-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/c6bff625bdb0393992c9d4db0c6bbe45-Bibtex.bib",
            "SUPP": ""
        }
    },
    "121": {
        "TITLE": "Memory-based Stochastic Optimization",
        "AUTHORS": "Andrew W. Moore, Jeff G. Schneider",
        "ABSTRACT": "In  this  paper  we  introduce  new  algorithms for  optimizing  noisy  plants in which each experiment is  very expensive.  The algorithms  build a global non-linear model of the expected output at the same  time as using Bayesian linear regression analysis of locally weighted  polynomial models.  The local  model answers  queries  about  confi(cid:173) dence,  noise,  gradient  and  Hessians,  and  use  them  to  make auto(cid:173) mated decisions similar to those made by a practitioner of Response  Surface  Methodology.  The global  and  local  models are  combined  naturally  as  a  locally  weighted  regression.  We  examine the  ques(cid:173) tion of whether the global model can really  help optimization, and  we  extend  it  to  the  case  of time-varying functions.  We  compare  the new algorithms with a  highly tuned higher-order stochastic op(cid:173) timization algorithm on  randomly-generated functions  and  a  sim(cid:173) ulated  manufacturing  task.  We  note  significant  improvements in  total regret , time to converge, and final  solution quality.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/c7635bfd99248a2cdef8249ef7bfbef4-Bibtex.bib",
            "SUPP": ""
        }
    },
    "122": {
        "TITLE": "Handwritten Word Recognition using Contextual Hybrid Radial Basis Function Network/Hidden Markov Models",
        "AUTHORS": "Bernard Lemarié, Michel Gilloux, Manuel Leroux",
        "ABSTRACT": "A hybrid and contextual radial basis function networklhidden Markov  model  off-line  handwritten  word  recognition  system  is  presented.  The  task  assigned  to  the radial  basis  function  networks  is  the  estimation of  emission probabilities associated to Markov states. The model is contex(cid:173) tual because the estimation of emission probabilities takes  into account  the left context of the current image segment as represented by its pred(cid:173) ecessor in the sequence. The new system does not outperform the previ(cid:173) ous system without context but acts differently.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/c9f95a0a5af052bffce5c89917335f67-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/c9f95a0a5af052bffce5c89917335f67-Bibtex.bib",
            "SUPP": ""
        }
    },
    "123": {
        "TITLE": "Universal Approximation and Learning of Trajectories Using Oscillators",
        "AUTHORS": "Pierre Baldi, Kurt Hornik",
        "ABSTRACT": "Natural  and  artificial  neural  circuits  must  be  capable  of travers(cid:173) ing  specific  state  space  trajectories.  A  natural  approach  to  this  problem  is  to  learn  the  relevant  trajectories  from  examples.  Un(cid:173) fortunately,  gradient  descent  learning  of complex  trajectories  in  amorphous  networks  is  unsuccessful.  We  suggest  a  possible  ap(cid:173) proach  where  trajectories  are  realized  by  combining simple oscil(cid:173) lators,  in  various  modular ways.  We  contrast  two  regimes  of fast  and slow oscillations.  In all cases,  we show that banks of oscillators  with bounded frequencies have universal approximation properties.  Open  questions  are also discussed  briefly. \n1 \nINTRODUCTION:  TRAJECTORY LEARNING \nThe  design  of artificial  neural  systems,  in  robotics  applications  and  others,  often  leads to the problem of constructing a recurrent neural network capable of producing  a particular trajectory, in the state space of its visible units.  Throughout evolution,  biological neural systems,  such  as  central  pattern generators,  have  also  been faced  with  similar  challenges.  A  natural  approach  to  tackle  this  problem  is  to  try  to  \"learn\"  the  desired  trajectory,  for  instance  through  a  process  of trial  and  error  and  subsequent  optimization.  Unfortunately,  gradient descent  learning of complex  trajectories  in  amorphous  networks  is  unsuccessful.  Here,  we  suggest  a  possible  approach where  trajectories are realized,  in a  modular and hierarchical fashion,  by  combining simple oscillators.  In particular,  we  show  that  banks of oscillators have  universal approximation properties. \n\nAlso  with the Jet Propulsion  Laboratory,  California  Institute of Technology. \n\n452 \nP.  BALDI,  K.  HORNIK \nTo  begin  with,  we  can restrict  ourselves  to the simple case  of a  network  with one!  visible  linear  unit  and consider  the  problem of adjusting  the  network  parameters  in  a  way  that  the output unit  activity  u(t)  is  equal  to a  target function  I(t),  over  an interval of time [0, T].  The hidden units of the network  may be non-linear and  satisfy,  for  instance, one of the usual  neural  network charging equations such  as \ndUi \ndt  = - Ti  + L..JjWij/jUj(t - Tij),",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/cd89fef7ffdd490db800357f47722b20-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/cd89fef7ffdd490db800357f47722b20-Bibtex.bib",
            "SUPP": ""
        }
    },
    "124": {
        "TITLE": "Correlated Neuronal Response: Time Scales and Mechanisms",
        "AUTHORS": "Wyeth Bair, Ehud Zohary, Christof Koch",
        "ABSTRACT": "We  have  analyzed the relationship between  correlated spike  count  and the peak in the cross-correlation of spike trains for  pairs of si(cid:173) multaneously recorded neurons  from  a  previous  study of area MT  in  the  macaque  monkey  (Zohary  et  al.,  1994).  We  conclude  that  common  input,  responsible  for  creating peaks  on  the order of ten  milliseconds  wide  in  the  spike  train  cross-correlograms  (CCGs),  is  also  responsible  for  creating  the  correlation  in  spike  count  ob(cid:173) served  at  the  two  second  time  scale  of  the  trial.  We  argue  that  both common excitation and  inhibition  may  play significant  roles  in establishing this  correlation.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/ce5140df15d046a66883807d18d0264b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/ce5140df15d046a66883807d18d0264b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "125": {
        "TITLE": "Worst-case Loss Bounds for Single Neurons",
        "AUTHORS": "David P. Helmbold, Jyrki Kivinen, Manfred K. Warmuth",
        "ABSTRACT": "We  analyze  and  compare  the  well-known  Gradient  Descent  algo(cid:173) rithm  and  a  new  algorithm,  called  the  Exponentiated  Gradient  algorithm, for  training  a  single neuron  with  an  arbitrary  transfer  function .  Both  algorithms are  easily  generalized  to  larger  neural  networks,  and the  generalization  of Gradient  Descent  is  the stan(cid:173) dard  back-propagation  algorithm.  In  this  paper  we  prove  worst(cid:173) case  loss  bounds  for  both  algorithms  in  the  single  neuron  case.  Since  local  minima make  it  difficult  to  prove  worst-case  bounds  for  gradient-based  algorithms,  we  must  use  a  loss  function  that  prevents  the  formation of spurious  local  minima.  We  define  such  a  matching  loss  function  for  any  strictly  increasing  differentiable  transfer  function  and  prove  worst-case  loss  bound  for  any  such  transfer  function  and  its  corresponding  matching loss.  For  exam(cid:173) ple,  the  matching loss  for  the  identity function  is  the  square  loss  and  the matching loss  for  the logistic sigmoid is  the entropic loss.  The different  structure  of the bounds for  the  two  algorithms indi(cid:173) cates that the new  algorithm out-performs Gradient Descent  when  the inputs contain a  large number of irrelevant components. \n310 \nD. P.  HELMBOLD, J.  KIVINEN, M.  K.  WARMUTH",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/d6723e7cd6735df68d1ce4c704c29a04-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/d6723e7cd6735df68d1ce4c704c29a04-Bibtex.bib",
            "SUPP": ""
        }
    },
    "126": {
        "TITLE": "Stock Selection via Nonlinear Multi-Factor Models",
        "AUTHORS": "Asriel E. Levin",
        "ABSTRACT": "This  paper discusses  the use  of multilayer feed forward  neural  net(cid:173) works  for  predicting  a stock's  excess  return  based  on  its  exposure  to various technical  and fundamental factors.  To demonstrate the  effectiveness  of the approach  a  hedged  portfolio which  consists  of  equally  capitalized long and short  positions is  constructed  and  its  historical  returns  are  benchmarked  against T-bill  returns  and  the  S&P500  index.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/d6ef5f7fa914c19931a55bb262ec879c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/d6ef5f7fa914c19931a55bb262ec879c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "127": {
        "TITLE": "The Geometry of Eye Rotations and Listing's Law",
        "AUTHORS": "Amir A. Handzel, Tamar Flash",
        "ABSTRACT": "We  analyse  the  geometry  of  eye  rotations,  and  in  particular  saccades,  using  basic  Lie  group  theory  and  differential  geome(cid:173) try.  Various  parameterizations  of  rotations  are  related  through  a  unifying  mathematical treatment,  and  transformations  between  co-ordinate  systems  are  computed  using  the  Campbell-Baker(cid:173) Hausdorff  formula.  Next,  we  describe  Listing's  law  by  means  of  the  Lie  algebra  so(3).  This  enables  us  to  demonstrate  a  direct  connection  to  Donders'  law, by  showing that  eye  orientations  are  restricted  to the quotient space  80(3)/80(2).  The latter is  equiv(cid:173) alent to the sphere S2, which is exactly the space of gaze directions.  Our analysis provides  a  mathematical framework  for  studying the  oculomotor system  and  could  also  be  extended  to  investigate  the  geometry of mUlti-joint arm movements.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/d736bb10d83a904aefc1d6ce93dc54b8-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/d736bb10d83a904aefc1d6ce93dc54b8-Bibtex.bib",
            "SUPP": ""
        }
    },
    "128": {
        "TITLE": "When is an Integrate-and-fire Neuron like a Poisson Neuron?",
        "AUTHORS": "Charles F. Stevens, Anthony M. Zador",
        "ABSTRACT": "In the  Poisson neuron  model, the output is  a  rate-modulated Pois(cid:173) son  process  (Snyder  and  Miller,  1991);  the  time  varying  rate  pa(cid:173) rameter  ret)  is  an  instantaneous  function  G[.]  of  the  stimulus,  ret)  =  G[s(t)].  In  a  Poisson  neuron,  then,  ret)  gives  the  instan(cid:173) taneous firing  rate-the instantaneous probability of firing  at  any  instant t-and the output is  a  stochastic function  of the input.  In  part because of its great simplicity, this model is  widely used  (usu(cid:173) ally with  the addition of a  refractory  period), especially  in  in  vivo  single unit electrophysiological studies,  where  set)  is usually taken  to be  the value of some sensory  stimulus.  In the  integrate-and-fire  neuron model, by  contrast, the output is a filtered and thresholded  function  of the input:  the input is  passed  through a  low-pass filter  (determined by the membrane time constant T)  and integrated un(cid:173) til the membrane potential vet)  reaches threshold 8,  at which  point  vet)  is reset  to its initial value.  By contrast with the Poisson model,  in the integrate-and-fire model the ouput is a deterministic function  of the input.  Although the integrate-and-fire model is  a caricature  of real  neural  dynamics,  it  captures  many of the  qualitative  fea(cid:173) tures,  and  is  often  used  as  a  starting point for  conceptualizing the  biophysical behavior of single neurons.  Here we show how a slightly  modified  Poisson model can  be  derived  from  the integrate-and-fire  model with noisy inputs  yet)  =  set) + net).  In  the modified model,  the  transfer  function  G[.] is  a  sigmoid  (erf)  whose  shape  is  deter(cid:173) mined  by  the  noise  variance  /T~.  Understanding  the  equivalence  between  the  dominant  in  vivo  and  in  vitro  simple neuron  models  may help  forge  links between  the  two levels.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/d8700cbd38cc9f30cecb34f0c195b137-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/d8700cbd38cc9f30cecb34f0c195b137-Bibtex.bib",
            "SUPP": ""
        }
    },
    "129": {
        "TITLE": "Examples of learning curves from a modified VC-formalism",
        "AUTHORS": "Adam Kowalczyk, Jacek Szymanski, Peter L. Bartlett, Robert C. Williamson",
        "ABSTRACT": "We  examine  the  issue  of evaluation of model  specific  parameters  in  a  modified VC-formalism.  Two examples are analyzed:  the 2-dimensional  homogeneous  perceptron  and  the  I-dimensional  higher  order  neuron.  Both models are solved theoretically, and their learning curves are com(cid:173) pared against  true learning  curves.  It is  shown  that the  formalism  has  the  potential  to  generate  a  variety  of learning  curves,  including  ones  displaying ''phase transitions.\"",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/d91d1b4d82419de8a614abce9cc0e6d4-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/d91d1b4d82419de8a614abce9cc0e6d4-Bibtex.bib",
            "SUPP": ""
        }
    },
    "130": {
        "TITLE": "Using Unlabeled Data for Supervised Learning",
        "AUTHORS": "Geoffrey G. Towell",
        "ABSTRACT": "Many classification problems have the property that the only costly  part of obtaining examples  is  the class  label.  This  paper suggests  a  simple  method  for  using  distribution  information  contained  in  unlabeled  examples  to augment  labeled  examples  in  a  supervised  training framework.  Empirical  tests  show  that  the  technique  de(cid:173) scribed  in  this  paper  can  significantly  improve  the  accuracy  of  a  supervised  learner  when  the  learner  is  well  below  its  asymptotic  accuracy level.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/db2b4182156b2f1f817860ac9f409ad7-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/db2b4182156b2f1f817860ac9f409ad7-Bibtex.bib",
            "SUPP": ""
        }
    },
    "131": {
        "TITLE": "Implementation Issues in the Fourier Transform Algorithm",
        "AUTHORS": "Yishay Mansour, Sigal Sahar",
        "ABSTRACT": "The  Fourier  transform of boolean  functions  has  come  to  play  an  important role in proving many important learnability results.  We  aim to demonstrate that the  Fourier transform techniques  are also  a  useful  and  practical  algorithm in  addition  to  being  a  powerful  theoretical  tool.  We  describe  the more prominent changes  we  have  introduced  to  the  algorithm,  ones  that  were  crucial  and  without  which  the  performance  of  the  algorithm  would  severely  deterio(cid:173) rate.  One of the benefits we  present  is  the confidence  level for each  prediction  which  measures  the  likelihood the prediction is  correct.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/db576a7d2453575f29eab4bac787b919-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/db576a7d2453575f29eab4bac787b919-Bibtex.bib",
            "SUPP": ""
        }
    },
    "132": {
        "TITLE": "A Bound on the Error of Cross Validation Using the Approximation and Estimation Rates, with Consequences for the Training-Test Split",
        "AUTHORS": "Michael J. Kearns",
        "ABSTRACT": "Abstract Unavailable",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/dc58e3a306451c9d670adcd37004f48f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "133": {
        "TITLE": "Classifying Facial Action",
        "AUTHORS": "Marian Stewart Bartlett, Paul A. Viola, Terrence J. Sejnowski, Beatrice A. Golomb, Jan Larsen, Joseph C. Hager, Paul Ekman",
        "ABSTRACT": "The Facial Action Coding System, (FACS),  devised by Ekman and  Friesen (1978), provides an objective meanS for measuring the facial  muscle  contractions  involved in  a  facial  expression.  In  this  paper,  we  approach automated facial expression  analysis by detecting and  classifying  facial  actions.  We  generated  a  database  of over  1100  image sequences  of 24  subjects  performing over  150 distinct facial  actions  or  action  combinations.  We  compare  three  different  ap(cid:173) proaches  to classifying the facial  actions  in  these  images:  Holistic  spatial analysis based on principal components of graylevel images;  explicit measurement of local image features  such  as wrinkles;  and  template matching with  motion flow  fields.  On  a  dataset contain(cid:173) ing six individual actions and 20 subjects, these methods had 89%,  57%, and 85% performances respectively for generalization to novel  subjects.  When combined,  performance improved to 92%.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/dd77279f7d325eec933f05b1672f6a1f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/dd77279f7d325eec933f05b1672f6a1f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "134": {
        "TITLE": "Strong Unimodality and Exact Learning of Constant Depth µ-Perceptron Networks",
        "AUTHORS": "Mario Marchand, Saeed Hadjifaradji",
        "ABSTRACT": "Abstract Unavailable",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/df0aab058ce179e4f7ab135ed4e641a9-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/df0aab058ce179e4f7ab135ed4e641a9-Bibtex.bib",
            "SUPP": ""
        }
    },
    "135": {
        "TITLE": "Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks",
        "AUTHORS": "James W. Howse, Chaouki T. Abdallah, Gregory L. Heileman",
        "ABSTRACT": "The  process  of machine  learning  can  be  considered  in  two  stages:  model  selection  and parameter estimation.  In this  paper a  technique  is  presented  for constructing dynamical systems with desired qualitative properties.  The  approach  is  based  on the fact  that  an n-dimensional  nonlinear  dynamical  system can be decomposed into one gradient and  (n - 1)  Hamiltonian sys(cid:173) tems.  Thus, the model  selection stage consists of choosing the gradient and  Hamiltonian portions appropriately so that a certain behavior is obtainable.  To estimate the parameters, a stably convergent learning rule  is  presented.  This algorithm has been proven to converge to the desired system trajectory  for  all  initial  conditions  and system inputs.  This technique can be used to  design  neural network models  which  are guaranteed to solve the trajectory  learning problem.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/e17184bcb70dcf3942c54e0b537ffc6d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/e17184bcb70dcf3942c54e0b537ffc6d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "136": {
        "TITLE": "A New Learning Algorithm for Blind Signal Separation",
        "AUTHORS": "Shun-ichi Amari, Andrzej Cichocki, Howard Hua Yang",
        "ABSTRACT": "A new on-line learning algorithm which minimizes a  statistical de(cid:173) pendency among outputs is  derived for  blind separation  of mixed  signals.  The  dependency  is  measured  by  the  average  mutual  in(cid:173) formation  (MI)  of the outputs.  The source signals and the mixing  matrix  are  unknown  except  for  the  number  of  the  sources.  The  Gram-Charlier  expansion  instead  of the  Edgeworth  expansion  is  used  in  evaluating  the  MI.  The  natural gradient  approach is  used  to minimize the MI. A novel activation function is proposed for the  on-line learning algorithm  which  has  an equivariant property and  is easily implemented on a  neural network like model.  The validity  of the new learning algorithm are verified by computer simulations.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/e19347e1c3ca0c0b97de5fb3b690855a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/e19347e1c3ca0c0b97de5fb3b690855a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "137": {
        "TITLE": "Adaptive Back-Propagation in On-Line Learning of Multilayer Networks",
        "AUTHORS": "Ansgar H. L. West, David Saad",
        "ABSTRACT": "An  adaptive back-propagation algorithm is  studied  and compared  with  gradient  descent  (standard  back-propagation)  for  on-line  learning  in  two-layer  neural  networks  with  an  arbitrary  number  of hidden  units.  Within  a  statistical  mechanics  framework ,  both  numerical  studies  and  a  rigorous  analysis show  that  the  adaptive  back-propagation method results in faster  training by breaking the  symmetry between  hidden  units  more efficiently  and by  providing  faster  convergence to optimal generalization than gradient descent.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/e22312179bf43e61576081a2f250f845-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/e22312179bf43e61576081a2f250f845-Bibtex.bib",
            "SUPP": ""
        }
    },
    "138": {
        "TITLE": "Neuron-MOS Temporal Winner Search Hardware for Fully-Parallel Data Processing",
        "AUTHORS": "Tadashi Shibata, Tsutomu Nakai, Tatsuo Morimoto, Ryu Kaihara, Takeo Yamashita, Tadahiro Ohmi",
        "ABSTRACT": "A  unique  architecture  of  winner  search  hardware  has  been  de(cid:173) veloped  using  a  novel  neuron-like  high  functionality  device  called  Neuron  MOS  transistor  (or  vMOS  in  short)  [1,2]  as  a  key  circuit  element.  The circuits developed  in  this work  can find  the location  of the  maximum  (or  minimum)  signal  among  a  number  of input  data on  the  continuous-time  basis,  thus enabling real-time  winner  tracking as well as fully-parallel sorting of multiple input data.  We  have  developed  two  circuit  schemes.  One  is  an  ensemble  of self(cid:173) loop-selecting v M OS ring oscillators finding the winner as an oscil(cid:173) lating node.  The other is  an ensemble of vMOS variable threshold  inverters receiving a common ramp-voltage for  competitive excita(cid:173) tion  where  data sorting  is  conducted  through  consecutive  winner  search actions.  Test circuits were fabricated by a double-polysilicon  CMOS  process  and  their  operation  has  been  experimentally veri(cid:173) fied.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/e515df0d202ae52fcebb14295743063b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/e515df0d202ae52fcebb14295743063b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "139": {
        "TITLE": "Human Reading and the Curse of Dimensionality",
        "AUTHORS": "Gale Martin",
        "ABSTRACT": "Whereas optical character recognition (OCR) systems learn to clas(cid:173) sify single characters;  people learn to classify long character strings  in  parallel,  within  a  single  fixation .  This  difference  is  surprising  because  high  dimensionality is  associated  with  poor  classification  learning.  This  paper  suggests  that  the  human  reading  system  avoids  these  problems  because  the  number  of to-be-classified  im(cid:173) ages  is  reduced  by  consistent  and  optimal eye  fixation  positions,  and  by  character sequence  regularities. \nAn interesting difference exists between human reading and optical character recog(cid:173) nition (OCR) systems.  The input/output dimensionality of character  classification  in  human reading is much greater than that for  OCR systems (see  Figure  1) .  OCR  systems  classify  one  character  at  time;  while  the  human  reading  system  classifies  as  many  as  8-13  characters  per  eye  fixation  (Rayner,  1979)  and  within  a  fixation,  character  category  and  sequence  information is  extracted  in  parallel  (Blanchard,  McConkie,  Zola,  and  Wolverton,  1984;  Reicher,  1969). \nOCR  (Low  Dbnensionality)  I Dorothy  lived In the  .... I  [Q]  ... _ ....................................  \"D\"  ~ ................................. ..  \"0\"  o",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/e58cc5ca94270acaceed13bc82dfedf7-Bibtex.bib",
            "SUPP": ""
        }
    },
    "140": {
        "TITLE": "Unsupervised Pixel-prediction",
        "AUTHORS": "William R. Softky",
        "ABSTRACT": "When a sensory system constructs a model of the environment  from its input, it might need to verify the model's accuracy. One  method of verification is multivariate time-series prediction: a good  model could predict the near-future activity of its inputs, much  as a good scientific theory predicts future data. Such a predict(cid:173) ing model would require copious top-down connections to compare  the predictions with the input. That feedback could improve the  model's performance in two ways: by biasing internal activity to(cid:173) ward expected patterns, and by generating specific error signals if  the predictions fail. A proof-of-concept model-an event-driven,  computationally efficient layered network, incorporating \"cortical\"  features like all-excitatory synapses and local inhibition- was con(cid:173) structed to make near-future predictions of a simple, moving stim(cid:173) ulus. After unsupervised learning, the network contained units not  only tuned to obvious features of the stimulus like contour orienta(cid:173) tion and motion, but also to contour discontinuity (\"end-stopping\")  and illusory contours.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Bibtex.bib",
            "SUPP": ""
        }
    },
    "141": {
        "TITLE": "Control of Selective Visual Attention: Modeling the \"Where\" Pathway",
        "AUTHORS": "Ernst Niebur, Christof Koch",
        "ABSTRACT": "Intermediate and higher vision processes  require selection of a sub(cid:173) set  of the  available sensory  information  before  further  processing.  Usually,  this  selection  is  implemented  in  the  form  of a  spatially  circumscribed  region of the  visual field,  the  so-called  \"focus  of at(cid:173) tention\"  which  scans  the  visual scene  dependent  on the  input  and  on the attentional state of the subject.  We here  present a  model for  the control of the focus of attention in primates, based on a saliency  map.  This mechanism is not only expected  to model the functional(cid:173) ity of biological vision but also to be essential for the understanding  of complex scenes  in machine vision. \n1 \nIntroduction:  \"What\"  and  \"Where\"  In Vision \nIt is  a  generally  accepted  fact  that  the  computations  of early  vision  are  massively  parallel operations,  i.e.,  applied in parallel to  all parts of the visual field.  This high  degree  of parallelism cannot be sustained in in~ermediate and higher  vision because  of the astronomical number of different possible combination of features.  Therefore,  it  becomes  necessary  to  select  only  a  part  of the  instantaneous  sensory  input  for  more  detailed  processing  and  to  discard  the  rest.  This is  the  mechanism of visual  selective  attention. \n•  Present  address:  Zanvyl  Krieger  Mind/Brain  Institute  and Department  of Neuros(cid:173)\ncience,  3400  N.  Charles  Street, The Johns  Hopkins  University,  Baltimore,  MD  21218 _ \nControl of Selective Visual  Attention:  Modeling the \"Where\" Pathway \n803 \nIt is  clear  that similar selection  mechanisms are  also  required  in  machine vision for  the  analysis of all but the simplest visual scenes.  Attentional mechanisms are slowly  introduced  in  this  field;  e.g. , Yamada and  Cottrell  (1995)  used  sequential scanning  by  a  \"focus  of attention\"  in  the  context  of face  recognition.  Another  model  for  eye  scan  path  generation,  which  is  characterized  by  a  strong  top-down  influence,  is  presented  by  Rao and Ballard (this  volume).  Sequential scanning can be applied to  more abstract spaces, like the dynamics of complex systems in optimization problems  with  large numbers  of minima (Tsioutsias and  Mjolsness,  this  volume). \nPrimate vision is  organized  along  two  major anatomical pathways.  One  of them  is  concerned  mainly  with  object  recognition.  For  this  reason ,  it  has  been  called  the  What- pathway;  for  anatomical  reasons ,  it  is  also  known  as  the  ventral  pathway.  The  principal  task  of the  other  major pathway  is  the  determination of the  location  of objects  and  therefore  it  is  called  the  Where  pathway  or,  again  for  anatomical  reasons,  the  dorsal  pathway. \nIn  previous work  (Niebur &  Koch ,  1994), we  presented  a  model for  the  implement(cid:173) ation  of the  What  pathway.  The  underlying  mechanism  is  \"temporal  tagging:\"  it  is  assumed  that  the  attended  region  of  the  visual  field  is  distinguished  from  the  unattended  parts  by  the  temporal  fine-structure  of the  neuronal  spike  trains.  We  have  shown  that  temporal tagging can  be  achieved  by  introducing  moderate  levels  of correlation 1  between  those  neurons  which  respond  to  attended stimuli. \nHow  can  such  synchronicity  be  obtained?  We  have  suggested  a  simple,  neurally  plausible mechanism , namely common input to  all  cells  which  respond  to  attended  stimuli.  Such  (excitatory)  input will  increase  the  propensity  of postsynaptic cells  to  fire  for  a  short  time  after  receiving  this  input,  and thereby  increase  the  correlation  between  spike  trains  without  necessarily  increasing  the  average  firing  rate . \nThe subject  of the  present  study  is  to  provide  a  model of the  control system which  generates  such  modulating  input.  We  will  show  that  it  is  possible  to  construct  an  integrated  system  of  attentional  control  which  is  based  on  neurally  plausible  elements  and  which  is  compatible with  the  anatomy and  physiology of the  primate  visual system .  The system scans  a  visual Scene  and identifies its most salient parts.  A  possible task would  be  \"Find all  faces  in this image.\"  We  are confident  that this  model  will  not  only  further  our  understanding  of the  function  of biological  vision  but  that it will  also  be  relevant for  technical  applications. \n2  A  Simple Model of The Dorsal  Pathway \n2.1  Overall Structure \nFigure  1 shows  an  overview  of the  model  Where  pathway.  Input  is  provided  in  the  form  of digitized  images from  an  NTSC  camera which  is  then  analyzed  in  various  feature  maps.  These maps are organized around the known operations in early visual  cortices.  They  are  implemented at different  spatial scales  and in  a  center-surround  structure  akin to  visual receptive  fields .  Different  spatial scales  are  implemented as  Gaussian pyramids (Adelson, Anderson , Bergen,  Burt,  &  Ogden,  1984).  The center \n1 In  (Niebur,  Koch,  &  Rosin,  1993),  a  similar  model  was  developed  using  periodic  \"40Hz\"  modulation.  The  present  model  can  be  adapted  mutatis  mutandis  to  this  type  of modulation. \n804 \nE. NIEBUR, C. KOCH \nof the  receptive  field  corresponds  to  the  value  of a  pixel  at  level  n  in  the  pyramid  and  the  surround to  the  corresponding  pixels  at level  n + 2,  level  0 being  the  image  in normal size.  The features  implemented so far are  the  thr~e principal components  of  primate  color  vision  (intensity,  red-green,  blue-yellow),  four  orientations,  and  temporal change.  Short  descriptions  of the  different  feature  maps are presented  in  the  next  section  (2.2). \nWe  then  (section  2.3)  address  the  question  of the  integration  of  the  input  in  the  \"saliency map,\"  a  topographically organized map which codes for  the instantaneous  conspicuity  of the  different  parts of the visual field .",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/e8b1cbd05f6e6a358a81dee52493dd06-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/e8b1cbd05f6e6a358a81dee52493dd06-Bibtex.bib",
            "SUPP": ""
        }
    },
    "142": {
        "TITLE": "Quadratic-Type Lyapunov Functions for Competitive Neural Networks with Different Time-Scales",
        "AUTHORS": "Anke Meyer-Bäse",
        "ABSTRACT": "The dynamics of complex neural networks modelling the self(cid:173) organization process in cortical maps must include the aspects of  long and short-term memory. The behaviour of the network is such  characterized by an equation of neural activity as a fast phenom(cid:173) enon and an equation of synaptic modification as a slow part of the  neural system. We present a quadratic-type Lyapunov function for  the flow of a competitive neural system with fast and slow dynamic  variables. We also show the consequences of the stability analysis  on the neural net parameters.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/eddb904a6db773755d2857aacadb1cb0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/eddb904a6db773755d2857aacadb1cb0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "143": {
        "TITLE": "Learning long-term dependencies is not as difficult with NARX networks",
        "AUTHORS": "Tsungnan Lin, Bill G. Horne, Peter Tiño, C. Lee Giles",
        "ABSTRACT": "It  has  recently  been  shown  that  gradient  descent  learning  algo(cid:173) rithms for  recurrent  neural  networks can  perform poorly  on  tasks  that  involve  long-term  dependencies.  In  this  paper  we  explore  this  problem  for  a  class  of architectures  called  NARX  networks,  which  have  powerful  representational capabilities.  Previous  work  reported that gradient descent learning is  more effective in NARX  networks  than  in  recurrent  networks  with  \"hidden  states\".  We  show  that although  NARX  networks  do  not  circumvent the  prob(cid:173) lem  of long-term dependencies,  they  can  greatly  improve  perfor(cid:173) mance  on  such  problems.  We  present  some  experimental 'results  that  show  that  NARX  networks  can  often  retain  information  for  two to three times as long as conventional recurrent  networks.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/f197002b9a0853eca5e046d9ca4663d5-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/f197002b9a0853eca5e046d9ca4663d5-Bibtex.bib",
            "SUPP": ""
        }
    },
    "144": {
        "TITLE": "Learning the Structure of Similarity",
        "AUTHORS": "Joshua B. Tenenbaum",
        "ABSTRACT": "The  additive  clustering (ADCL US)  model (Shepard &  Arabie,  1979)  treats  the  similarity of two  stimuli  as  a  weighted  additive  measure  of their  common features.  Inspired  by  recent  work  in  unsupervised  learning with  multiple cause  models,  we  propose  anew, statistically  well-motivated  algorithm  for  discovering  the  structure  of  natural  stimulus classes  using the ADCLUS model, which promises substan(cid:173) tial gains  in  conceptual  simplicity,  practical  efficiency,  and  solution  quality over earlier efforts.  We  also present  preliminary results  with  artificial data and two classic similarity data sets.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/f4dd765c12f2ef67f98f3558c282a9cd-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/f4dd765c12f2ef67f98f3558c282a9cd-Bibtex.bib",
            "SUPP": ""
        }
    },
    "145": {
        "TITLE": "Investment Learning with Hierarchical PSOMs",
        "AUTHORS": "Jörg A. Walter, Helge Ritter",
        "ABSTRACT": "We propose a hierarchical scheme for rapid learning of context dependent  \"skills\"  that  is  based  on  the  recently  introduced  \"Parameterized  Self(cid:173) Organizing Map\" (\"PSOM\"). The underlying idea is to first invest some  learning effort to  specialize the  system  into  a  rapid learner for  a  more  restricted range of contexts. \nThe specialization is carried out by a prior \"investment learning stage\",  during which the system acquires a set of basis mappings or \"skills\" for  a  set of prototypical contexts.  Adaptation  of a \"skill\" to a new context  can then be achieved by interpolating in the space of the basis mappings  and thus can be extremely rapid. \nWe demonstrate the potential of this approach for the task of a 3D visuo(cid:173) motor map for  a  Puma robot and  two  cameras.  This  includes  the for(cid:173) ward and backward robot kinematics in  3D end effector coordinates, the  2D+2D retina coordinates and also the 6D joint angles.  After the invest(cid:173) ment phase  the transformation can  be learned for  a new camera set-up  with a single observation.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/f7f580e11d00a75814d2ded41fe8e8fe-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/f7f580e11d00a75814d2ded41fe8e8fe-Bibtex.bib",
            "SUPP": ""
        }
    },
    "146": {
        "TITLE": "How Perception Guides Production in Birdsong Learning",
        "AUTHORS": "Christopher L. Fry",
        "ABSTRACT": "A  c.:omputational  model  of  song  learning  in  the  song  sparrow  (M elospiza  melodia)  learns  to  categorize  the  different  syllables  of  a  song  sparrow  song  and  uses  this  categorization to  train  itself to  reproduce  song.  The model fills  a crucial gap in the computational  explanation  of birdsong  learning  by  exploring  the  organization of  perception  in  songbirds.  It shows  how  competitive  learning  may  lead  to  the  organization  of  a  specific  nucleus  in  the  bird  brain,  replicates  the  song  production  results  of a  previous  model  (Doya  and  Sejnowski,  1995),  and  demonstrates  how  perceptual  learning  can  guide production through  reinforcement  learning.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/fc2c7c47b918d0c2d792a719dfb602ef-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/fc2c7c47b918d0c2d792a719dfb602ef-Bibtex.bib",
            "SUPP": ""
        }
    },
    "147": {
        "TITLE": "Stable Fitted Reinforcement Learning",
        "AUTHORS": "Geoffrey J. Gordon",
        "ABSTRACT": "We  describe  the  reinforcement  learning  problem,  motivate  algo(cid:173) rithms which seek an approximation to the Q function, and present  new  convergence results for  two such algorithms. \n1 \nINTRODUCTION AND BACKGROUND \nImagine an agent acting in some environment.  At time t, the environment is in some  state Xt  chosen from  a finite set of states.  The agent perceives Xt,  and is  allowed to  choose an action at from  some finite  set of actions.  The environment then changes  state,  so  that  at  time  (t + 1)  it  is  in  a  new  state  Xt+1  chosen  from  a  probability  distribution which depends only on  Xt  and at.  Meanwhile,  the agent experiences a  real-valued cost  Ct,  chosen  from  a  distribution  which  also  depends  only  on  Xt  and  at  and which  has finite  mean and variance. \nSuch  an environment is  called  a  Markov  decision  process,  or MDP.  The reinforce(cid:173) ment learning problem is  to control an MDP  to minimize the expected discounted  cost  Lt ,tCt  for  some  discount  factor,  E  [0,1].  Define  the  function  Q  so  that  Q(x, a)  is  the cost for  being in state x  at time  0,  choosing action a,  and behaving  optimally from  then on.  If we  can discover Q,  we  have solved the problem:  at each  step,  we  may simply choose at  to minimize Q(xt, at).  For more information about  MDPs, see  (Watkins,  1989, Bertsekas and Tsitsiklis,  1989).  We  may distinguish two classes of problems, online and offline.  In  the offline  prob(cid:173) lem,  we have a full  model of the MDP: given a state and an action, we  can describe  the  distributions  of the  cost  and  the  next  state.  We  will  be  concerned  with  the  online problem, in which our knowledge of the MDP is  limited to what we  can dis(cid:173) cover by  interacting with it.  To solve an online problem,  we  may approximate the  transition  and  cost functions,  then  proceed  as for  an offline  problem  (the indirect  approach);  or  we  may  try  to  learn  the Q  function  without  the  intermediate  step  (the direct approach).  Either approach may work better for  any given problem:  the \nStable Fitted Reinforcement  Learning",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "148": {
        "TITLE": "Information through a Spiking Neuron",
        "AUTHORS": "Charles F. Stevens, Anthony M. Zador",
        "ABSTRACT": "While  it  is  generally  agreed  that  neurons  transmit  information  about their synaptic inputs through spike trains, the code by which  this  information is  transmitted  is  not  well  understood.  An  upper  bound  on  the  information  encoded  is  obtained  by  hypothesizing  that the precise  timing of each spike conveys information.  Here  we  develop  a  general  approach  to quantifying the information carried  by  spike  trains  under  this  hypothesis,  and  apply  it  to  the  leaky  integrate-and-fire  (IF)  model  of  neuronal  dynamics.  We  formu(cid:173) late  the  problem  in  terms  of the  probability  distribution  peT)  of  interspike  intervals  (ISIs),  assuming that  spikes  are  detected  with  arbitrary  but finite  temporal resolution .  In  the  absence  of added  noise,  all  the  variability in  the  ISIs  could encode  information, and  the  information rate  is  simply the entropy of the lSI  distribution,  H (T)  =  (-p(T) log2 p(T)},  times  the  spike  rate.  H (T)  thus  pro(cid:173) vides  an  exact  expression  for  the  information rate.  The  methods  developed  here  can be  used  to determine experimentally the infor(cid:173) mation carried  by  spike  trains,  even  when  the  lower  bound of the  information rate  provided  by  the  stimulus reconstruction  method  is  not  tight.  In  a  preliminary series  of experiments,  we  have  used  these  methods  to estimate information rates of hippocampal neu(cid:173) rons  in  slice  in  response  to somatic  current  injection.  These  pilot  experiments suggest  information rates as  high as  6.3  bits/spike. \n1 \nInformation rate of spike trains \nCortical neurons use  spike  trains  to communicate with other  neurons.  The output  of each  neuron  is  a  stochastic function of its input from  the  other neurons.  It is  of  interest  to know  how  much each  neuron  is  telling other neurons about its inputs. \nHow  much information does  the spike train provide about a signal?  Consider noise  net)  added  to  a  signal  set)  to  produce  some  total  input  yet)  =  set)  + net).  This  is  then  passed  through  a  (possibly  stochastic)  functional  F  to produce  the  output  spike  train  F[y(t)]  --+  z(t).  We  assume  that  all the  information contained  in  the  spike  train  can  be  represented  by  the  list  of spike  times;  that  is,  there  is  no extra  information contained in  properties  such  as spike  height or  width.  Note,  however,  that many characteristics of the spike  train such  as the mean or instantaneous rate \n76 \nC. STEVENS, A.  ZADOR \ncan be  derived  from  this  representation;  if such  a  derivative  property  turns  out  to  be  the relevant one,  then  this formulation can be specialized  appropriately. \nWe  will  be  interested,  then,  in  the  mutual  information 1(S(t); Z(t»  between  the  input signal ensemble S(t) and the output spike train ensemble Z(t) . This is defined  in  terms  of the  entropy  H(S)  of the  signal,  the  entropy  H(Z)  of the  spike  train,  and  their joint entropy  H(S, Z), \n1(S; Z) =  H(S) + H(Z) - H(S, Z). \n(1)  Note that the mutual information is  symmetric, 1(S; Z) =  1(Z; S),  since  the joint  entropy  H(S, Z)  =  H(Z, S).  Note  also  that  if the  signal  S(t)  and  the  spike  train  Z(t)  are  completely independent,  then  the mutual information is  0,  since  the joint  entropy is just the sum of the individual entropies H(S, Z) = H(S) + H(Z).  This is  completely in lin'e  with our intuition, since  in  this case  the  spike  train  can  provide  no  information about the signal. \nInformation estimation through stimulus reconstruction \n1.1  Bialek  and  colleagues  (Bialek  et  al.,  1991)  have  used  the  reconstruction  method  to  obtain  a  strict  lower  bound  on  the  mutual information in  an experimental set(cid:173) ting.  This method is  based on  an expression  mathematically equivalent  to eq.  (1)  involving the conditional entropy  H(SIZ) of the signal given the spike train,",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/fd2c5e4680d9a01dba3aada5ece22270-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/fd2c5e4680d9a01dba3aada5ece22270-Bibtex.bib",
            "SUPP": ""
        }
    },
    "149": {
        "TITLE": "Discriminant Adaptive Nearest Neighbor Classification and Regression",
        "AUTHORS": "Trevor Hastie, Robert Tibshirani",
        "ABSTRACT": "Nearest  neighbor  classification expects  the  class  conditional  prob(cid:173) abilities  to  be  locally  constant,  and  suffers  from  bias  in  high  di(cid:173) mensions  We  propose  a  locally  adaptive form  of nearest  neighbor  classification  to try  to finesse  this curse  of dimensionality.  We  use  a  local  linear  discriminant  analysis  to  estimate  an  effective  met(cid:173) ric  for  computing neighborhoods.  We  determine the local decision  boundaries  from  centroid  information,  and  then  shrink  neighbor(cid:173) hoods  in  directions  orthogonal to  these  local  decision  boundaries,  and  elongate  them  parallel  to  the  boundaries.  Thereafter,  any  neighborhood-based  classifier  can be employed, using the  modified  neighborhoods.  We  also  propose  a  method  for  global  dimension  reduction, that combines local dimension information. We  indicate  how  these  techniques  can be  extended  to the  regression  problem.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/fe709c654eac84d5239d1a12a4f71877-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/fe709c654eac84d5239d1a12a4f71877-Bibtex.bib",
            "SUPP": ""
        }
    },
    "150": {
        "TITLE": "A Predictive Switching Model of Cerebellar Movement Control",
        "AUTHORS": "Andrew G. Barto, James C. Houk",
        "ABSTRACT": "We present a hypothesis about how the cerebellum could partici(cid:173) pate in regulating movement in the presence of significant feedback  delays without resorting to a forward model of the motor plant. We  show how a simplified cerebellar model can learn to control end(cid:173) point positioning of a nonlinear spring-mass system with realistic  delays in both afferent and efferent pathways. The model's opera(cid:173) tion involves prediction, but instead of predicting sensory input, it  directly regulates movement by reacting in an anticipatory fashion  to input patterns that include delayed sensory feedback.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/feab05aa91085b7a8012516bc3533958-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/feab05aa91085b7a8012516bc3533958-Bibtex.bib",
            "SUPP": ""
        }
    },
    "151": {
        "TITLE": "Recurrent Neural Networks for Missing or Asynchronous Data",
        "AUTHORS": "Yoshua Bengio, Francois Gingras",
        "ABSTRACT": "In this paper we  propose recurrent neural networks with feedback into the input  units for  handling two  types  of data analysis problems.  On the one  hand,  this  scheme can be used for static data when some of the input variables are missing.  On  the  other  hand,  it can also  be  used  for  sequential  data,  when  some of the  input variables are missing or are available at different frequencies.  Unlike in the  case of probabilistic models (e.g.  Gaussian) of the missing variables, the network  does  not  attempt  to  model the  distribution  of the  missmg  variables given  the  observed variables.  Instead it is a  more \"discriminant\" approach that fills  in the  missing variables for  the sole purpose of minimizing a  learning criterion  (e.g.,  to  minimize an output error).",
        "CONFERENCE": "Advances in Neural Information Processing Systems 8  (NIPS 1995)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1995/file/ffeed84c7cb1ae7bf4ec4bd78275bb98-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1995/file/ffeed84c7cb1ae7bf4ec4bd78275bb98-Bibtex.bib",
            "SUPP": ""
        }
    }
}