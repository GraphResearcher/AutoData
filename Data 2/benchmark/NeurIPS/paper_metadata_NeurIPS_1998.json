{
    "0": {
        "TITLE": "Computational Differences between Asymmetrical and Symmetrical Networks",
        "AUTHORS": "Zhaoping Li, Peter Dayan",
        "ABSTRACT": "Symmetrically connected  recurrent networks have recently been  used as models of a host of neural computations.  However, be(cid:173) cause of the separation between excitation and inhibition, biolog(cid:173) ical  neural networks  are  asymmetrical.  We  study characteristic  differences between asymmetrical networks and their symmetri(cid:173) cal  counterparts,  showing  that  they  have  dramatically  different  dynamical behavior and also how the differences can be exploited  for computational ends.  We  illustrate our results in the case of a  network that is a selective amplifier.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/020c8bfac8de160d4c5543b96d1fdede-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/020c8bfac8de160d4c5543b96d1fdede-Bibtex.bib",
            "SUPP": ""
        }
    },
    "1": {
        "TITLE": "A Randomized Algorithm for Pairwise Clustering",
        "AUTHORS": "Yoram Gdalyahu, Daphna Weinshall, Michael Werman",
        "ABSTRACT": "We present a stochastic clustering algorithm based on pairwise sim(cid:173) ilarity  of datapoints.  Our  method  extends  existing  deterministic  methods, including agglomerative algorithms, min-cut graph  algo(cid:173) rithms,  and  connected  components.  Thus  it  provides  a  common  framework for  all  these  methods.  Our  graph-based  method  differs  from  existing  stochastic  methods  which  are  based  on  analogy  to  physical  systems.  The  stochastic  nature  of our  method  makes  it  more  robust  against  noise,  including  accidental  edges  and  small  spurious clusters.  We demonstrate the superiority of our algorithm  using  an example with  3 spiraling bands and  a  lot of noise.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/06a81a4fb98d149f2d31c68828fa6eb2-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/06a81a4fb98d149f2d31c68828fa6eb2-Bibtex.bib",
            "SUPP": ""
        }
    },
    "2": {
        "TITLE": "Risk Sensitive Reinforcement Learning",
        "AUTHORS": "Ralph Neuneier, Oliver Mihatsch",
        "ABSTRACT": "A directed generative model for binary data using a  small number  of hidden  continuous  units  is  investigated.  A  clipping  nonlinear(cid:173) ity distinguishes the model from conventional principal components  analysis.  The relationships between the correlations of the underly(cid:173) ing continuous Gaussian variables  and the binary output variables  are utilized  to learn  the  appropriate weights  of the network.  The  advantages of this  approach are illustrated on a  translationally in(cid:173) variant binary distribution and on handwritten digit  images.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/076023edc9187cf1ac1f1163470e479a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/076023edc9187cf1ac1f1163470e479a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "3": {
        "TITLE": "Convergence of the Wake-Sleep Algorithm",
        "AUTHORS": "Shiro Ikeda, Shun-ichi Amari, Hiroyuki Nakahara",
        "ABSTRACT": "The W-S (Wake-Sleep) algorithm is a simple learning rule for the models  with hidden variables.  It is  shown that this algorithm can be applied to  a factor  analysis model  which is  a linear version  of the  Helmholtz ma(cid:173) chine.  But even for a factor analysis model,  the general convergence is  not proved theoretically.  In this article, we describe the geometrical un(cid:173) derstanding of the W-S  algorithm in contrast with the EM (Expectation(cid:173) Maximization) algorithm and the em algorithm.  As the result, we prove  the convergence of the W-S  algorithm for the factor analysis model.  We  also show the condition for the convergence in general models.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/0771fc6f0f4b1d7d1bb73bbbe14e0e31-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/0771fc6f0f4b1d7d1bb73bbbe14e0e31-Bibtex.bib",
            "SUPP": ""
        }
    },
    "4": {
        "TITLE": "Mean Field Methods for Classification with Gaussian Processes",
        "AUTHORS": "Manfred Opper, Ole Winther",
        "ABSTRACT": "We discuss the application of TAP mean field methods known from  the Statistical Mechanics of disordered systems to Bayesian classifi(cid:173) cation models with Gaussian processes.  In contrast to previous ap(cid:173) proaches,  no  knowledge about the distribution of inputs is  needed.  Simulation results for  the Sonar data set are given. \n1  Modeling with Gaussian Processes \nBayesian models which are based on Gaussian prior distributions on function spaces  are promising non-parametric statistical tools.  They have been recently introduced  into the Neural  Computation community  (Neal  1996, Williams &  Rasmussen  1996,  Mackay  1997).  To  give their basic definition,  we  assume that the likelihood of the  output  or  target  variable  T  for  a  given  input  s  E  RN  can  be  written  in  the  form  p(Tlh(s))  where  h  : RN  --+  R  is  a  priori  assumed  to  be  a  Gaussian  random field.  If we  assume fields  with  zero  prior  mean,  the statistics  of h  is  entirely  defined  by  the second order correlations C(s, S')  ==  E[h(s)h(S')], where E  denotes expectations \n310 \nM  Opper and 0.  Winther \nwith respect  to the prior.  Interesting examples  are",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/08040837089cdf46631a10aca5258e16-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/08040837089cdf46631a10aca5258e16-Bibtex.bib",
            "SUPP": ""
        }
    },
    "5": {
        "TITLE": "Learning from Dyadic Data",
        "AUTHORS": "Thomas Hofmann, Jan Puzicha, Michael I. Jordan",
        "ABSTRACT": "Dyadzc  data  refers  to  a  domain  with  two  finite  sets  of objects  in  which  observations are  made for  dyads , i.e., pairs with one element  from  either  set.  This  type  of data  arises  naturally  in  many  ap(cid:173) plication  ranging  from  computational  linguistics  and  information  retrieval to preference  analysis and computer vision.  In this paper,  we  present  a  systematic,  domain-independent framework  of learn(cid:173) ing  from  dyadic  data by  statistical  mixture  models.  Our  approach  covers different models with fiat  and hierarchical latent class struc(cid:173) tures.  We  propose  an  annealed  version  of the  standard  EM  algo(cid:173) rithm for  model fitting  which  is  empirically evaluated  on  a  variety  of data sets from  different  domains.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/0c8ce55163055c4da50a81e0a273468c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/0c8ce55163055c4da50a81e0a273468c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "6": {
        "TITLE": "Call-Based Fraud Detection in Mobile Communication Networks Using a Hierarchical Regime-Switching Model",
        "AUTHORS": "Jaakko Hollm√©n, Volker Tresp",
        "ABSTRACT": "Fraud causes substantial losses to telecommunication carriers. Detec(cid:173) tion systems which automatically detect illegal use of the network can be  used to alleviate the problem. Previous approaches worked on features  derived from the call patterns of individual users. In this paper we present  a call-based detection system based on a hierarchical regime-switching  model. The detection problem is formulated as an inference problem on  the regime probabilities. Inference is implemented by applying the junc(cid:173) tion tree algorithm to the underlying graphical model. The dynamics are  learned from data using the EM algorithm and subsequent discriminative  training. The methods are assessed using fraud data from a real mobile  communication network.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/0c9ebb2ded806d7ffda75cd0b95eb70c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/0c9ebb2ded806d7ffda75cd0b95eb70c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "7": {
        "TITLE": "Analyzing and Visualizing Single-Trial Event-Related Potentials",
        "AUTHORS": "Tzyy-Ping Jung, Scott Makeig, Marissa Westerfield, Jeanne Townsend, Eric Courchesne, Terrence J. Sejnowski",
        "ABSTRACT": "Event-related  potentials (ERPs),  are portions  of electroencephalo(cid:173) graphic  (EEG)  recordings  that  are  both  time- and  phase-locked  to  experimental  events.  ERPs  are  usually  averaged  to  increase  their  signal/noise  ratio  relative  to  non-phase  locked  EEG  activ(cid:173) ity,  regardless  of the  fact  that  response  activity  in  single  epochs  may vary  widely  in  time course  and scalp distribution.  This study  applies a linear decomposition tool, Independent Component Anal(cid:173) ysis  (ICA)  [1],  to  multichannel single-trial  EEG  records  to  derive  spatial filters  that  decompose  single-trial  EEG epochs  into  a  sum  of temporally  independent  and spatially fixed  components  arising  from  distinct  or  overlapping  brain  or  extra-brain  networks.  Our  results  on  normal  and  autistic  subjects  show  that  ICA  can  sep(cid:173) arate  artifactual,  stimulus-locked,  response-locked,  and.  non-event  related  background  EEG  activities  into separate  components,  al(cid:173) lowing ( 1) removal of pervasive artifacts of all types from single-trial  EEG records,  and (2) identification of both stimulus- and response(cid:173) locked EEG components.  Second, this study proposes a new visual(cid:173) ization tool, the 'ERP image', for investigating variability in laten(cid:173) cies  and amplitudes of event-evoked responses  in spontaneous EEG  or  MEG  records.  We show  that sorting single-trial ERP epochs  in  order  of reaction  time  and  plotting  the  potentials  in  2-D  clearly  reveals  underlying  patterns  of response  variability  linked  to  per(cid:173) formance.  These  analysis  and  visualization  tools  appear  broadly  applicable to electrophyiological research  on both normal and clin(cid:173) ical  populations. \nAnalyzing and Visualizing Single-Trial Event-Related Potentials",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/0d4f4805c36dc6853edfa4c7e1638b48-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/0d4f4805c36dc6853edfa4c7e1638b48-Bibtex.bib",
            "SUPP": ""
        }
    },
    "8": {
        "TITLE": "Learning Nonlinear Dynamical Systems Using an EM Algorithm",
        "AUTHORS": "Zoubin Ghahramani, Sam T. Roweis",
        "ABSTRACT": "The Expectation-Maximization (EM) algorithm is an iterative pro(cid:173) cedure  for  maximum  likelihood  parameter  estimation  from  data  sets  with  missing  or  hidden  variables  [2].  It has  been  applied  to  system identification in linear stochastic state-space models,  where  the state variables are hidden from the observer and both the state  and  the  parameters  of  the  model  have  to  be  estimated  simulta(cid:173) neously  [9].  We  present  a  generalization  of the  EM  algorithm for  parameter estimation in  nonlinear dynamical systems.  The \"expec(cid:173) tation\" step makes use of Extended Kalman Smoothing to estimate  the state, while the  \"maximization\"  step re-estimates the parame(cid:173) ters using these uncertain state estimates. In general, the nonlinear  maximization step is difficult because it requires integrating out the  uncertainty  in  the states.  However,  if Gaussian  radial  basis  func(cid:173) tion  (RBF)  approximators  are  used  to  model  the  nonlinearities,  the integrals  become  tractable  and the  maximization step  can  be  solved  via systems of linear equations. \n1  Stochastic  Nonlinear Dynamical Systems \nWe  examine inference and learning in discrete-time dynamical systems with hidden  state  Xt,  inputs  Ut,  and  outputs  Yt. 1  The  state  evolves  according  to  stationary  nonlinear dynamics driven by the inputs  and by additive noise \n(1) \n1 All  lowercase  characters  (except indices)  denote  vectors.  Matrices  are represented by \nuppercase characters. \n432 \nZ.  Ghahramani and S.  T  Roweis \nwhere  w  is  zero-mean  Gaussian noise  with  covariance Q.  2  The outputs  are  non(cid:173) linearly related to the states and inputs by \nYt  =  g(Xt, Ut)  + v \n(2) \nwhere  v  is  zero-mean Gaussian noise with  covariance R.  The vector-valued non lin(cid:173) earities f  and 9  are assumed to be  differentiable,  but otherwise arbitrary.  Models  of this  kind  have been examined for  decades  in  various communities.  Most  notably,  nonlinear state-space models form  one of the cornerstones of modern sys(cid:173) tems  and  control  engineering.  In this  paper,  we  examine these  models  within  the  framework  of probabilistic graphical models  and derive  a  novel  learning algorithm  for  them  based  on  EM.  With one exception,3  this  is  to  the  best  of our  knowledge  the first  paper addressing learning of stochastic nonlinear dynamical systems of the  kind  we  have described  within  the framework of the EM algorithm.  The classical approach to system identification treats the parameters as hidden vari(cid:173) ables, and applies the Extended Kalman Filtering algorithm (described in section 2)  to  the  nonlinear  system  with  the  state  vector  augmented  by  the  parameters  [5]. 4  This approach is  inherently on-line, which may be important in certain applications.  Furthermore,  it  provides  an estimate of the  covariance  of the  parameters  at  each  time step.  In contrast, the EM algorithm we  present is  a  batch algorithm and does  not  attempt to estimate the covariance of the parameters. \nThere are three important advantages the  EM algorithm has  over the classical ap(cid:173) proach.  First, the EM algorithm provides a straightforward and  principled method  for  handing  missing  inputs  or  outputs.  Second,  EM  generalizes  readily  to  more  complex  models  with  combinations  of  discrete  and  real-valued  hidden  variables.  For example, one  can formulate EM for  a  mixture of nonlinear  dynamical systems.  Third,  whereas  it  is  often  very  difficult  to  prove  or  analyze  stability  within  the  classical on-line approach, the EM algorithm is  always attempting to maximize the  likelihood,  which  acts as a  Lyapunov function  for  stable learning. \nIn the next sections we will describe the basic components of the learning algorithm.  For the expectation step of the algorithm, we infer the conditional distribution of the  hidden states using Extended Kalman Smoothing (section 2).  For the maximization  step  we  first  discuss  the  general  case  (section  3)  and  then  describe  the  particular  case  where  the  nonlinearities  are  represented  using  Gaussian  radial  basis  function  (RBF;  [6])  networks  (section 4). \n2  Extended Kalman  Smoothing \nGiven  a  system  described  by  equations  (1)  and  (2),  we  need  to  infer  the  hidden  states from  a  history  of observed  inputs  and  outputs.  The  quantity  at the  heart  of this  inference problem is  the conditional density P(XtIUl,\"\"  UT, Yl,.' \"  YT),  for  1 ::;  t  ::;  T,  which  captures the fact  that the system is  stochastic and therefore our  inferences about  x  will  be uncertain. \n2The Gaussian noise assumption is  less restrictive for  nonlinear systems than for  linear \nsystems since the nonlinearity can  be  used  to  generate  non-Gaussian  state noise. \n3The authors have just become aware that Briegel and Tresp (this volume) have applied  EM to essentially the same model.  Briegel  and Tresp's method uses multilayer perceptrons  (MLP) to approximate the nonlinearities,  and requires sampling from the hidden states to  fit  the  MLP.  We  use  Gaussian  radial  basis functions  (RBFs)  to  model  the  nonlinearities,  which  can be fit  analytically without sampling  (see section  4) . \n41t  is  important not  to  confuse  this use  of the  Extended Kalman  algorithm,  to simul(cid:173)\ntaneously  estimate  parameters  and  hidden  states,  with  our use  of EKS,  to  estimate just  the hidden state as  part of the E  step of EM. \nLearning Nonlinear Dynamics Using EM \n433 \nFor linear dynamical systems with  Gaussian state evolution and observation noises,  this conditional  density  is  Gaussian and  the recursive algorithm for  computing its  mean  and  covariance is  known  as  Kalman  smoothing [4,  8].  Kalman  smoothing is  directly analogous to the forward-backward algorithm for computing the conditional  hidden  state  distribution  in  a  hidden  Markov  model,  and  is  also  a  special  case  of  the belief propagation algorithm. 5  For  nonlinear systems  this  conditional  density  is  in  general  non-Gaussian and  can  in  fact  be quite  complex.  Multiple  approaches  exist  for  inferring the  hidden state  distribution  of such  nonlinear  systems,  including  sampling  methods  [7]  and  varia(cid:173) tional approximations [3].  We focus  instead in this paper on a classic approach from  engineering,  Extended Kalman  Smoothing  (EKS). \nExtended Kalman Smoothing simply applies Kalman smoothing to a local lineariza(cid:173) tion  of the  nonlinear  system.  At  every  point  x in  x-space,  the  derivatives  of  the  vector-valued functions f  and 9 define the matrices, Ax ==  M I x=x  and ex ==  ~ I x=x'  respectively.  The dynamics  are linearized  about  Xt,  the mean of the Kalman filter  state estimate at time t: \nThe output equation  (2)  can be similarly linearized.  If the prior distribution of the  hidden state at t =  1 was  Gaussian, then, in this  linearized system, the conditional  distribution of the hidden state at any time t  given the history of inputs and outputs  will also be Gaussian.  Thus, Kalman smoothing can be used on the linearized system  to infer  this conditional distribution  (see  figure  1,  left  panel). \n(3)",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/0ebcc77dc72360d0eb8e9504c78d38bd-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/0ebcc77dc72360d0eb8e9504c78d38bd-Bibtex.bib",
            "SUPP": ""
        }
    },
    "9": {
        "TITLE": "Replicator Equations, Maximal Cliques, and Graph Isomorphism",
        "AUTHORS": "Marcello Pelillo",
        "ABSTRACT": "We  present  a  new  energy-minimization  framework  for  the  graph  isomorphism  problem  which  is  based  on  an  equivalent  maximum  clique formulation.  The approach is centered around a fundamental  result proved by  Motzkin and Straus in the mid-1960s, and recently  expanded in  various ways,  which  allows  us  to formulate the maxi(cid:173) mum clique problem in  terms of a standard quadratic program.  To  solve  the  program we  use  \"replicator\"  equations,  a  class  of simple  continuous- and discrete-time dynamical systems developed in var(cid:173) ious  branches  of theoretical  biology.  We  show  how,  despite  their  inability  to escape from  local  solutions,  they  nevertheless  provide  experimental results which are competitive with those obtained us(cid:173) ing more elaborate mean-field  annealing heuristics.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/0f3d014eead934bbdbacb62a01dc4831-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/0f3d014eead934bbdbacb62a01dc4831-Bibtex.bib",
            "SUPP": ""
        }
    },
    "10": {
        "TITLE": "Orientation, Scale, and Discontinuity as Emergent Properties of Illusory Contour Shape",
        "AUTHORS": "Karvel K. Thornber, Lance R. Williams",
        "ABSTRACT": "A  recent  neural  model  of  illusory  contour  formation  is  based  on  a  distribution  of natural  shapes  traced  by  particles  moving  with  constant speed in directions given by Brownian motions.  The input  to that model consists of pairs of position and direction constraints  and the  output  consists  of the  distribution  of contours joining all  such  pairs.  In general,  these contours will  not  be closed  and  their  distribution  will  not  be  scale-invariant.  In  this  paper,  we  show  how  to  compute  a  scale-invariant  distribution  of  closed  contours  given  position  constraints  alone  and  use  this  result  to  explain  a  well  known illusory contour effect.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/109d2dd3608f669ca17920c511c2a41e-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/109d2dd3608f669ca17920c511c2a41e-Bibtex.bib",
            "SUPP": ""
        }
    },
    "11": {
        "TITLE": "Active Noise Canceling Using Analog Neuro-Chip with On-Chip Learning Capability",
        "AUTHORS": "Jung-Wook Cho, Soo-Young Lee",
        "ABSTRACT": "A modular analogue neuro-chip set with  on-chip learning capability  is  developed  for  active  noise  canceling.  The  analogue  neuro-chip  set  incorporates  the  error  backpropagation  learning  rule  for  practical  applications,  and  allows  pin-to-pin  interconnections  for  multi-chip  boards.  The  developed  neuro-board  demonstrated  active  noise  canceling  without  any  digital  signal  processor.  Multi-path  fading  of  acoustic  channels,  random  noise,  and  nonlinear distortion  of the  loud  speaker  are  compensated  by  the  adaptive  learning  circuits  of  the  neuro-chips. Experimental  results  are  reported  for  cancellation  of car  noise in  real time.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/1373b284bc381890049e92d324f56de0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/1373b284bc381890049e92d324f56de0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "12": {
        "TITLE": "Coordinate Transformation Learning of Hand Position Feedback Controller by Using Change of Position Error Norm",
        "AUTHORS": "Eimei Oyama, Susumu Tachi",
        "ABSTRACT": "In  order  to  grasp  an  object,  we  need  to  solve  the  inverse  kine(cid:173) matics problem, i.e., the coordinate transformation from  the visual  coordinates  to the joint  angle  vector  coordinates  of the arm.  Al(cid:173) though  several models  of coordinate transformation learning have  been proposed, they suffer from  a number of drawbacks.  In human  motion  control,  the  learning  of the  hand  position  error  feedback  controller in the inverse kinematics solver is important.  This paper  proposes  a  novel  model  of the coordinate transformation  learning  of  the  human  visual  feedback  controller  that  uses  the  change  of  the joint  angle vector and the corresponding change of the square  of the  hand  position  error  norm.  The  feasibility  of the  proposed  model  is  illustrated using numerical simulations.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/1415db70fe9ddb119e23e9b2808cde38-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/1415db70fe9ddb119e23e9b2808cde38-Bibtex.bib",
            "SUPP": ""
        }
    },
    "13": {
        "TITLE": "Multi-Electrode Spike Sorting by Clustering Transfer Functions",
        "AUTHORS": "Dmitry Rinberg, Hanan Davidowitz, Naftali Tishby",
        "ABSTRACT": "A  new  paradigm  is  proposed  for  sorting  spikes  in  multi-electrode  data using ratios of transfer functions  between cells and electrodes.  It  is  assumed  that  for  every  cell  and  electrode  there  is  a  stable  linear  relation.  These are  dictated  by  the properties of the tissue,  the electrodes  and  their relative  geometries.  The main  advantage  of the method is that it is  insensitive to variations in the shape and  amplitude of a  spike.  Spike  sorting  is  carried  out  in  two  separate  steps.  First,  templates  describing  the statistics of each spike type  are generated by clustering transfer function  ratios then spikes are  detected  in  the  data  using  the  spike  statistics.  These  techniques  were  applied  to  data generated  in  the  escape  response  system  of  the cockroach.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/1714726c817af50457d810aae9d27a2e-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/1714726c817af50457d810aae9d27a2e-Bibtex.bib",
            "SUPP": ""
        }
    },
    "14": {
        "TITLE": "Fisher Scoring and a Mixture of Modes Approach for Approximate Inference and Learning in Nonlinear State Space Models",
        "AUTHORS": "Thomas Briegel, Volker Tresp",
        "ABSTRACT": "We  present Monte-Carlo generalized EM  equations for learning in  non(cid:173) linear state space models.  The difficulties lie in the Monte-Carlo E-step  which consists of sampling from  the posterior distribution of the hidden  variables given the observations. The new idea presented in this paper is  to generate samples from a Gaussian approximation to the true posterior  from  which it is easy to obtain independent samples.  The parameters  of  the Gaussian approximation are either derived from the extended Kalman  filter or the Fisher scoring algorithm. In case the posterior density is mul(cid:173) timodal  we propose to  approximate the posterior by  a sum of Gaussians  (mixture of modes approach).  We show that sampling from the approxi(cid:173) mate posterior densities obtained by the above algorithms leads to better  models  than  using  point estimates  for  the  hidden  states.  In  our exper(cid:173) iment,  the Fisher scoring algorithm obtained  a better approximation of  the posterior mode than the EKF. For a multimodal distribution, the mix(cid:173) ture of modes approach gave superior results.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/17e23e50bedc63b4095e3d8204ce063b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/17e23e50bedc63b4095e3d8204ce063b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "15": {
        "TITLE": "Probabilistic Modeling for Face Orientation Discrimination: Learning from Labeled and Unlabeled Data",
        "AUTHORS": "Shumeet Baluja",
        "ABSTRACT": "This paper presents probabilistic modeling methods to solve the problem of dis(cid:173) criminating between five facial  orientations with  very  little labeled data.  Three  models are explored. The first model maintains no inter-pixel dependencies, the  second model is capable of modeling a set of arbitrary pair-wise dependencies,  and the last model allows  dependencies  only  between  neighboring pixels. We  show that for all three of these models, the accuracy of the learned models can  be greatly improved by  augmenting a small number of labeled training images  with  a  large  set of unlabeled  images using  Expectation-Maximization.  This  is  important because it is often difficult to obtain image labels, while many unla(cid:173) beled images  are  readily  available.  Through  a  large  set  of empirical  tests,  we  examine the benefits  of unlabeled data  for  each  of the  models.  By  using only  two randomly selected labeled examples per class, we can discriminate between  the five  facial orientations with an accuracy of 94%; with six labeled examples,  we achieve an accuracy of 98%.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/18d10dc6e666eab6de9215ae5b3d54df-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/18d10dc6e666eab6de9215ae5b3d54df-Bibtex.bib",
            "SUPP": ""
        }
    },
    "16": {
        "TITLE": "Direct Optimization of Margins Improves Generalization in Combined Classifiers",
        "AUTHORS": "Llew Mason, Peter L. Bartlett, Jonathan Baxter",
        "ABSTRACT": "Cumulative training margin  dis(cid:173) tributions  for  AdaBoost  versus  our  \"Direct  Optimization  Of  Margins\"  (DOOM)  algorithm.  The dark curve is  AdaBoost, the  light  curve  is  DOOM.  DOOM  sacrifices  significant  training  er(cid:173) ror for  improved test error  (hori(cid:173) zontal marks on margin= 0 line)_ \n-1 \n-0.8  -0.6  -0.4  -0.2  0 \n0.2  0.4  0.6  0.8",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/18ead4c77c3f40dabf9735432ac9d97a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/18ead4c77c3f40dabf9735432ac9d97a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "17": {
        "TITLE": "A Neuromorphic Monaural Sound Localizer",
        "AUTHORS": "John G. Harris, Chiang-Jung Pu, Jos√© Carlos Pr√≠ncipe",
        "ABSTRACT": "We  describe the first  single microphone sound localization system  and its inspiration from theories of human monaural sound localiza(cid:173) tion.  Reflections and diffractions caused by the external ear (pinna)  allow  humans to estimate sound source elevations  using only one  ear.  Our single microphone localization model relies on a specially  shaped reflecting structure that serves the role  of the pinna.  Spe(cid:173) cially designed analog VLSI circuitry uses  echo-time processing to  localize the sound.  A CMOS integrated circuit has been designed,  fabricated,  and successfully demonstrated on actual sounds.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/1baff70e2669e8376347efd3a874a341-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/1baff70e2669e8376347efd3a874a341-Bibtex.bib",
            "SUPP": ""
        }
    },
    "18": {
        "TITLE": "Boxlets: A Fast Convolution Algorithm for Signal Processing and Neural Networks",
        "AUTHORS": "Patrice Simard, L√©on Bottou, Patrick Haffner, Yann LeCun",
        "ABSTRACT": "Signal  processing  and  pattern  recognition  algorithms make  exten(cid:173) sive  use  of convolution.  In  many cases, computational accuracy  is  not  as  important  as  computational  speed.  In  feature  extraction,  for  instance,  the  features  of interest  in  a  signal  are  usually  quite  distorted.  This form of noise justifies some level of quantization in  order  to  achieve  faster  feature  extraction .  Our  approach  consists  of approximating regions  of the  signal  with  low  degree  polynomi(cid:173) als,  and then differentiating the resulting signals in order to obtain  impulse functions  (or derivatives of impulse functions).  With this  representation,  convolution  becomes  extremely  simple and  can  be  implemented quite effectively.  The true  convolution can  be  recov(cid:173) ered  by  integrating  the  result  of  the  convolution.  This  method  yields  substantial speed  up  in feature  extraction  and is  applicable  to convolutional neural  networks.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/1bc0249a6412ef49b07fe6f62e6dc8de-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/1bc0249a6412ef49b07fe6f62e6dc8de-Bibtex.bib",
            "SUPP": ""
        }
    },
    "19": {
        "TITLE": "Experimental Results on Learning Stochastic Memoryless Policies for Partially Observable Markov Decision Processes",
        "AUTHORS": "John K. Williams, Satinder P. Singh",
        "ABSTRACT": "Partially Observable Markov Decision  Processes  (pO \"MOPs)  constitute  an  important  class  of reinforcement  learning problems  which  present  unique theoretical and computational difficulties.  In  the absence of the  Markov property,  popular reinforcement  learning  algorithms  such  as  Q-Iearning  may  no  longer  be  effective,  and  memory-based  methods  which remove partial observability via state-estimation  are notoriously  expensive.  An alternative approach is  to seek a stochastic memoryless  policy  which  for  each  observation  of  the  environment  prescribes  a  probability  distribution  over  available  actions  that  maximizes  the  average  reward  per  timestep.  A  reinforcement  learning  algorithm  which  learns  a locally optimal  stochastic memoryless  policy has  been  proposed by Jaakkola,  Singh and Jordan,  but not empirically verified.  We present  a variation  of this  algorithm,  discuss  its  implementation,  and demonstrate its viability using four test problems.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/1cd3882394520876dc88d1472aa2a93f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/1cd3882394520876dc88d1472aa2a93f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "20": {
        "TITLE": "Attentional Modulation of Human Pattern Discrimination Psychophysics Reproduced by a Quantitative Model",
        "AUTHORS": "Laurent Itti, Jochen Braun, Dale K. Lee, Christof Koch",
        "ABSTRACT": "We  previously  proposed  a  quantitative model  of early  visual  pro(cid:173) cessing  in  primates,  based  on  non-linearly interacting visual filters  and statistically efficient  decision.  We  now  use  this model to inter(cid:173) pret  the observed  modulation of a  range of human psychophysical  thresholds  with  and  without  focal  visual  attention.  Our  model  - calibrated  by  an  automatic fitting  procedure  - simultaneously  re(cid:173) produces  thresholds for  four  classical  pattern discrimination tasks,  performed while attention was engaged by another concurrent task.  Our model then predicts that the seemingly complex improvements  of certain  thresholds,  which  we  observed  when  attention  was  fully  available for  the  discrimination  tasks,  can  best  be  explained  by  a  strengthening  of competition among early  visual filters.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/204da255aea2cd4a75ace6018fad6b4d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/204da255aea2cd4a75ace6018fad6b4d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "21": {
        "TITLE": "Signal Detection in Noisy Weakly-Active Dendrites",
        "AUTHORS": "Amit Manwani, Christof Koch",
        "ABSTRACT": "Here we derive measures quantifying the information loss of a synaptic  signal due to the presence of neuronal noise sources, as it electrotonically  propagates along a weakly-active dendrite. We model the dendrite as an  infinite linear cable, with noise sources distributed along its length.  The  noise sources we consider are thermal noise, channel noise arising from  the stochastic nature of voltage-dependent ionic channels (K+ and Na+)  and synaptic noise due to spontaneous background activity. We assess the  efficacy of information transfer using a signal detection paradigm where  the objective is to detect the presence/absence of a presynaptic spike from  the post-synaptic membrane voltage. This allows us to analytically assess  the role of each of these noise sources in  information transfer.  For our  choice  of parameters,  we find  that the  synaptic  noise  is  the  dominant  noise source which limits the maximum length over which information  be reliably transmitted.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/21fe5b8ba755eeaece7a450849876228-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/21fe5b8ba755eeaece7a450849876228-Bibtex.bib",
            "SUPP": ""
        }
    },
    "22": {
        "TITLE": "Kernel PCA and De-Noising in Feature Spaces",
        "AUTHORS": "Sebastian Mika, Bernhard Sch√∂lkopf, Alex J. Smola, Klaus-Robert M√ºller, Matthias Scholz, Gunnar R√§tsch",
        "ABSTRACT": "Kernel  PCA  as  a  nonlinear feature  extractor has  proven powerful  as  a  preprocessing step for classification algorithms.  But it can also be con(cid:173) sidered  as  a  natural  generalization of linear principal  component anal(cid:173) ysis.  This  gives  rise  to  the  question  how  to  use  nonlinear features  for  data compression, reconstruction, and de-noising, applications common  in  linear PCA.  This is  a nontrivial  task,  as the results provided by  ker(cid:173) nel PCA live in some high dimensional feature space and need not have  pre-images in  input space.  This work presents ideas for finding approxi(cid:173) mate pre-images, focusing on Gaussian kernels, and shows experimental  results  using  these pre-images in  data reconstruction and de-noising on  toy examples as well as on real world data. \n1  peA and Feature Spaces  Principal  Component Analysis  (PC A)  (e.g.  [3])  is  an  orthogonal  basis  transformation.  The  new  basis  is  found  by  diagonalizing  the  centered  covariance  matrix  of a  data  set  {Xk  E  RNlk  = 1, ... ,f}, defined  by  C  = ((Xi  - nates in the Eigenvector basis are called principal components.  The size of an Eigenvalue  >.  corresponding to  an  Eigenvector v  of C equals the amount of variance in  the direction  of v.  Furthermore, the directions of the  first  n Eigenvectors corresponding to the biggest  n Eigenvalues cover as much variance as possible by n orthogonal directions. In many ap(cid:173) plications they contain the most interesting information: for instance, in data compression,  where we project onto the directions with biggest variance to  retain  as  much information  as possible, or in de-noising, where we deliberately drop directions with small variance. \n(Xk))T).  The coordi(cid:173)",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/226d1f15ecd35f784d2a20c3ecf56d7f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/226d1f15ecd35f784d2a20c3ecf56d7f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "23": {
        "TITLE": "Multiple Paired Forward-Inverse Models for Human Motor Learning and Control",
        "AUTHORS": "Masahiko Haruno, Daniel M. Wolpert, Mitsuo Kawato",
        "ABSTRACT": "Humans  demonstrate  a  remarkable  ability  to  generate  accurate  and  appropriate motor behavior under many different and oftpn  uncprtain  environmental  conditions.  This  paper  describes  a  new  modular  ap(cid:173) proach to human motor learning and control, baspd on multiple pairs of  inverse  (controller)  and forward  (prpdictor)  models.  This architecture  simultaneously learns the multiple inverse models necessary for control  as well as how to select the inverse models appropriate for a given em'i(cid:173) ronm0nt.  Simulations of object manipulation demonstrates the ability  to learn  mUltiple  objects,  appropriate  generalization  to  novel  objects  and  the  inappropriate  activation  of motor  programs  based  on  visual  cues,  followed  by on-line  correction, seen  in  the  \"size-weight  illusion\".",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/228499b55310264a8ea0e27b6e7c6ab6-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/228499b55310264a8ea0e27b6e7c6ab6-Bibtex.bib",
            "SUPP": ""
        }
    },
    "24": {
        "TITLE": "SMEM Algorithm for Mixture Models",
        "AUTHORS": "Naonori Ueda, Ryohei Nakano, Zoubin Ghahramani, Geoffrey E. Hinton",
        "ABSTRACT": "We  present  a  split and  merge  EM  (SMEM)  algorithm to overcome the local  maximum problem in  parameter estimation of finite  mixture  models.  In the  case  of mixture  models,  non-global  maxima often  involve  having  too  many  components of a  mixture  model  in one part of the space and  too  few  in  an(cid:173) other, widely separated part of the space.  To escape from such configurations  we  repeatedly perform simultaneous split and merge operations using a  new  criterion  for  efficiently  selecting  the  split  and  merge  candidates.  We  apply  the proposed algorithm to the training of Gaussian mixtures and mixtures of  factor  analyzers  using synthetic  and  real  data and show  the effectiveness  of  using  the  split  and  merge  operations  to  improve the  likelihood  of both  the  training data and of held-out test data.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/253f7b5d921338af34da817c00f42753-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/253f7b5d921338af34da817c00f42753-Bibtex.bib",
            "SUPP": ""
        }
    },
    "25": {
        "TITLE": "Learning Lie Groups for Invariant Visual Perception",
        "AUTHORS": "Rajesh P. N. Rao, Daniel L. Ruderman",
        "ABSTRACT": "One  of the  most important problems  in  visual  perception is  that of visual  in(cid:173) variance:  how are objects perceived to be the same despite undergoing transfor(cid:173) mations such as translations, rotations or scaling?  In this paper, we describe a  Bayesian method for learning invariances based on Lie group theory.  We show  that previous approaches based on first-order Taylor series expansions of inputs  can be regarded as special cases of the Lie group approach, the latter being ca(cid:173) pable of handling in principle arbitrarily large transfonnations. Using a matrix(cid:173) exponential based generative model of images,  we derive an unsupervised al(cid:173) gorithm for learning Lie group operators from  input data containing infinites(cid:173) imal transfonnations.  The on-line unsupervised learning algorithm maximizes  the posterior probability of generating the training data.  We provide experimen(cid:173) tal results suggesting that the proposed method can learn Lie group operators for  handling reasonably large I-D translations and 2-D rotations.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/277281aada22045c03945dcb2ca6f2ec-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/277281aada22045c03945dcb2ca6f2ec-Bibtex.bib",
            "SUPP": ""
        }
    },
    "26": {
        "TITLE": "Tractable Variational Structures for Approximating Graphical Models",
        "AUTHORS": "David Barber, Wim Wiegerinck",
        "ABSTRACT": "Graphical models provide a broad probabilistic framework with ap(cid:173) plications in speech recognition  (Hidden  Markov  Models),  medical  diagnosis  (Belief  networks)  and  artificial  intelligence  (Boltzmann  Machines).  However,  the  computing time is  typically  exponential  in the number of nodes in the graph.  Within the variational frame(cid:173) work for  approximating these models, we  present two classes of dis(cid:173) tributions,  decimatable  Boltzmann  Machines  and Tractable  Belief  Networks  that  go  beyond  the  standard  factorized  approach.  We  give  generalised  mean-field  equations  for  both  these  directed  and  undirected  approximations.  Simulation  results  on  a  small  bench(cid:173) mark problem suggest using these richer approximations compares  favorably  against others previously reported in the literature.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/297fa7777981f402dbba17e9f29e292d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/297fa7777981f402dbba17e9f29e292d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "27": {
        "TITLE": "A Micropower CMOS Adaptive Amplitude and Shift Invariant Vector Quantiser",
        "AUTHORS": "Richard Coggins, Raymond J. Wang, Marwan A. Jabri",
        "ABSTRACT": "In  this  paper we  describe the  architecture,  implementation and  experi(cid:173) mental results for an  Intracardiac Electrogram (ICEG) classification and  compression chip.  The chip processes  and  vector-quantises  30 dimen(cid:173) sional analogue vectors while consuming a maximum of 2.5  J-tW  power  for a heart rate of 60 beats per minute (1  vector per second) from a 3.3 V  supply.  This  represents  a significant advance on  previous  work  which  achieved ultra low power supervised morphology classification since the  template matching scheme used  in  this chip enables unsupervised blind  classification of abnonnal rhythms and the computational support for low  bit rate data compression.  The adaptive template matching scheme used  is tolerant to amplitude variations, and inter- and intra-sample time shifts.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/29921001f2f04bd3baee84a12e98098f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/29921001f2f04bd3baee84a12e98098f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "28": {
        "TITLE": "Convergence Rates of Algorithms for Visual Search: Detecting Visual Contours",
        "AUTHORS": "Alan L. Yuille, James M. Coughlan",
        "ABSTRACT": "This  paper  formulates  the  problem  of  visual  search  as  Bayesian  inference  and  defines  a  Bayesian  ensemble  of  problem  instances .  In  particular,  we  address  the  problem  of  the  detection  of  visual  contours  in  noise/clutter  by  optimizing  a  global  criterion  which  combines  local  intensity  and  geometry  information.  We  analyze  the  convergence  rates  of  A * search  algorithms  using  results  from  information theory to bound  the  probability of rare  events  within  the Bayesian ensemble.  This analysis determines characteristics of  the  domain ,  which  we  call  order  parameters,  that  determine  the  convergence  rates.  In  particular,  we  present  a  specific  admissible  A * algorithm with pruning which converges, with high probability,  with  expected  time  O(N)  in  the  size  of  the  problem.  In  addi(cid:173) tion,  we  briefly  summarize  extensions  of this  work  which  address  fundamental  limits  of target  contour  detectability  (Le.  algorithm  independent results)  and the use  of non-admissible heuristics.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/2b3bf3eee2475e03885a110e9acaab61-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/2b3bf3eee2475e03885a110e9acaab61-Bibtex.bib",
            "SUPP": ""
        }
    },
    "29": {
        "TITLE": "Learning to Find Pictures of People",
        "AUTHORS": "Sergey Ioffe, David A. Forsyth",
        "ABSTRACT": "Finding articulated objects, like people, in pictures present.s a par(cid:173) ticularly difficult object. recognition problem. We show how t.o  find people by finding putative body segments, and then construct.(cid:173) ing assemblies of those segments that are consist.ent with the con(cid:173) straints on the appearance of a person that result from kinematic  properties. Since a reasonable model of a person requires at. least  nine segments, it is not possible to present every group to a classi(cid:173) fier. Instead, the search can be pruned by using projected versions  of a classifier that accepts groups corresponding to people. We  describe an efficient projection algorithm for one popular classi(cid:173) fier , and demonstrate that our approach can be used to determine  whether images of real scenes contain people.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/309fee4e541e51de2e41f21bebb342aa-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/309fee4e541e51de2e41f21bebb342aa-Bibtex.bib",
            "SUPP": ""
        }
    },
    "30": {
        "TITLE": "Visualizing Group Structure",
        "AUTHORS": "Marcus Held, Jan Puzicha, Joachim M. Buhmann",
        "ABSTRACT": "Cluster  analysis  is  a  fundamental  principle  in  exploratory  data  analysis,  providing the user  with a  description  of the  group struc(cid:173) ture of given data.  A key  problem in  this context is the interpreta(cid:173) tion  and  visualization  of clustering  solutions  in  high- dimensional  or  abstract  data  spaces.  In  particular,  probabilistic  descriptions  of the  group structure,  essential  to  capture  inter-cluster  relation(cid:173) ships, are hardly assessable by simple inspection ofthe probabilistic  assignment  variables.  VVe  present  a  novel  approach  to  the  visual(cid:173) ization of group structure.  It is  based on  a statistical model of the  object  assignments  which  have  been  observed  or  estimated  by  a  probabilistic clustering  procedure.  The objects or  data points are  embedded  in  a  low  dimensional Euclidean space  by  approximating  the  observed  data statistics  with  a  Gaussian  mixture model.  The  algorithm provides a new approach to the visualization of the inher(cid:173) ent structure for a broad variety of data types,  e.g. histogram data,  proximity data and co-occurrence data.  To demonstrate the power  of the approach,  histograms of textured images are visualized as an  example of a  large-scale data mining application.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/351b33587c5fdd93bd42ef7ac9995a28-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/351b33587c5fdd93bd42ef7ac9995a28-Bibtex.bib",
            "SUPP": ""
        }
    },
    "31": {
        "TITLE": "Modeling Surround Suppression in V1 Neurons with a Statistically Derived Normalization Model",
        "AUTHORS": "Eero P. Simoncelli, Odelia Schwartz",
        "ABSTRACT": "We examine the statistics of natural monochromatic images decomposed \nusing a multi-scale wavelet basis.  Although the coefficients of this rep(cid:173)\nresentation are  nearly decorrelated,  they  exhibit important higher-order \nstatistical dependencies that cannot be eliminated with purely linear pro(cid:173)\nc~ssing. In particular, rectified coefficients corresponding to basis func(cid:173)\ntions at neighboring spatial positions, orientations and scales are highly \ncorrelated.  A method of removing these dependencies is  to divide each \ncoefficient by  a  weighted  combination of its  rectified  neighbors.  Sev(cid:173)\neral successful models of the steady -state behavior of neurons in primary \nvisual cortex are based on such \"divisive normalization\" computations, \nand thus our analysis provides a theoretical justification for these models. \nPerhaps more importantly, the statistical measurements explicitly specify \nthe  weights that should be used in computing the normalization signal. \nWe  demonstrate that this  weighting  is  qualitatively  consistent with  re(cid:173)\ncent physiological experiments  that  characterize the  suppressive effect \nof stimuli presented outside of the classical receptive field.  Our obser(cid:173)\nvations thus provide evidence for the hypothesis that early visual neural \nprocessing is well matched to these statistical properties of images.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/35309226eb45ec366ca86a4329a2b7c3-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/35309226eb45ec366ca86a4329a2b7c3-Bibtex.bib",
            "SUPP": ""
        }
    },
    "32": {
        "TITLE": "Familiarity Discrimination of Radar Pulses",
        "AUTHORS": "Eric Granger, Stephen Grossberg, Mark A. Rubin, William W. Streilein",
        "ABSTRACT": "The  ARTMAP-FD  neural  network  performs  both  identification  (placing  test  patterns in  classes  encountered during  training)  and  familiarity  discrimination  (judging  whether a  test  pattern belongs  to  any  of  the  classes  encountered  during  training).  The  perfor(cid:173) mance of ARTMAP-FD  is  tested  on  radar pulse  data obtained in  the field,  and compared to that of the nearest-neighbor-based NEN  algorithm and to a  k  > 1 extension of NEN.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/35464c848f410e55a13bb9d78e7fddd0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/35464c848f410e55a13bb9d78e7fddd0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "33": {
        "TITLE": "Graphical Models for Recognizing Human Interactions",
        "AUTHORS": "Nuria Oliver, Barbara Rosario, Alex Pentland",
        "ABSTRACT": "We describe  a real-time computer vision  and machine learning sys(cid:173) tem for  modeling and recognizing  human actions and interactions.  Two  different  domains  are  explored:  recognition  of  two-handed  motions in  the martial art  'Tai  Chi' , and multiple- person  interac(cid:173) tions  in a  visual  surveillance task.  Our system combines top-down  with  bottom-up information using  a  feedback  loop,  and  is  formu(cid:173) lated with  a  Bayesian  framework.  Two  different  graphical  models  (HMMs and Coupled HMMs) are used for modeling both individual  actions and multiple-agent interactions, and CHMMs are shown to  work  more efficiently  and  accurately  for  a  given  amount of train(cid:173) ing.  Finally,  to  overcome  the  limited  amounts  of training  data,  we  demonstrate  that  'synthetic  agents'  (Alife-style  agents)  can  be  used  to develop flexible  prior models of the person-to-person inter(cid:173) actions.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/3a20f62a0af1aa152670bab3c602feed-Bibtex.bib",
            "SUPP": ""
        }
    },
    "34": {
        "TITLE": "An Entropic Estimator for Structure Discovery",
        "AUTHORS": "Matthew Brand",
        "ABSTRACT": "We  introduce  a  novel  framework  for  simultaneous  structure  and  parameter  learning  in  hidden-variable conditional probability models,  based on an  en tropic  prior and  a solution  for its maximum a posteriori (MAP) estimator.  The MAP estimate minimizes uncertainty  in  all  respects:  cross-entropy  between  model  and  data;  entropy  of the  model ;  entropy  of  the  data's  descriptive  statistics.  Iterative  estimation  extinguishes  weakly  supported  parameters,  compressing and  sparsifying  the  model.  Trimming operators  accelerate  this  process  by  removing  excess  parameters  and,  unlike  most  pruning  schemes,  guarantee  an  increase  in  posterior  probability.  Entropic  estimation  takes  a  overcomplete  random  model  and simplifies  it,  inducing the  structure of relations  between  hidden  and  observed  variables.  Applied  to  hidden  Markov  models  (HMMs),  it  finds  a  concise  finite-state  machine  representing  the  hidden  structure  of a  signal.  We  entropically  model  music,  handwriting, and video time-series, and show that the resulting models are highly concise,  structured,  predictive,  and  interpretable:  Surviving  states  tend  to  be  highly  correlated  with meaningful partitions of the data, while surviving transitions provide a low-perplexity  model of the signal dynamics. \n1  . An entropic prior  In  entropic  estimation  we  seek to  maximize  the  information  content of parameters.  For  conditional  probabilities,  parameters  values  near  chance  add  virtually  no  information  to  the  model,  and  are  therefore  wasted  degrees  of  freedom.  In  contrast,  parameters  near  the  extrema  {O, I}  are  informative  because  they  impose  strong  constr¬∑aints  on  the  class  of signals  accepted  by  the  model.  In  Bayesian  terms,  our  prior should  assert  that  parameters that do not reduce uncertainty are improbable.  We  can capture this intuition in  a surprisingly simple form:  For a model of N  conditional probabilities 9  = {(h , . . . , () N  }  we write \n(1) \nwhence we can  see that the prior measures a model's freedom from ambiguity (H(9) is an  entropy measure).  Applying Pe (.)  to a multinomial yields the posterior \np  (LlI)  P(wI9)Pe (9)  e  U  W",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/3fb451ca2e89b3a13095b059d8705b15-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/3fb451ca2e89b3a13095b059d8705b15-Bibtex.bib",
            "SUPP": ""
        }
    },
    "35": {
        "TITLE": "The Effect of Correlations on the Fisher Information of Population Codes",
        "AUTHORS": "Hyoungsoo Yoon, Haim Sompolinsky",
        "ABSTRACT": "We  study  the  effect  of correlated  noise  on  the  accuracy  of popu(cid:173) lation  coding  using  a  model  of  a  population  of  neurons  that  are  broadly  tuned  to  an  angle  in  two-dimension.  The  fluctuations  in  the neuronal activity is  modeled  as  a  Gaussian noise with pairwise  correlations which decays exponentially with the difference between  the preferred orientations of the pair.  By calculating the Fisher in(cid:173) formation  of the  system,  we  show  that in  the biologically  relevant  regime of parameters positive correlations decrease the estimation  capability  of the  network  relative  to  the  uncorrelated  population.  Moreover strong positive  correlations  result  in  information  capac(cid:173) ity  which  saturates  to a  finite  value  as  the  number  of cells  in  the  population  grows.  In  contrast,  negative  correlations  substantially  increase the information capacity of the neuronal  population.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/41a60377ba920919939d83326ebee5a1-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/41a60377ba920919939d83326ebee5a1-Bibtex.bib",
            "SUPP": ""
        }
    },
    "36": {
        "TITLE": "Spike-Based Compared to Rate-Based Hebbian Learning",
        "AUTHORS": "Richard Kempter, Wulfram Gerstner, J. Leo van Hemmen",
        "ABSTRACT": "A  correlation-based learning  rule  at the  spike  level  is  formulated,  mathematically analyzed, and compared to learning in a firing-rate  description.  A  differential  equation  for  the  learning  dynamics  is  derived  under the assumption  that the time scales of learning and  spiking  can  be  separated.  For  a  linear  Poissonian  neuron  model  which receives time-dependent stochastic input we show that spike  correlations on  a  millisecond time scale play  indeed  a  role.  Corre(cid:173) lations between input and output spikes tend to stabilize structure  formation,  provided  that  the  form  of  the  learning  window  is  in  accordance with Hebb's  principle.  Conditions for  an intrinsic nor(cid:173) malization of the average synaptic weight are discussed.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/42a3964579017f3cb42b26605b9ae8ef-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/42a3964579017f3cb42b26605b9ae8ef-Bibtex.bib",
            "SUPP": ""
        }
    },
    "37": {
        "TITLE": "Contrast Adaptation in Simple Cells by Changing the Transmitter Release Probability",
        "AUTHORS": "P√©ter Adorj√°n, Klaus Obermayer",
        "ABSTRACT": "The contrast response function (CRF) of many neurons in the primary vi(cid:173) sual  cortex saturates and shifts towards higher contrast values following  prolonged presentation of high contrast visual stimuli.  Using a recurrent  neural  network of excitatory spiking neurons with adapting synapses we  show  that both effects could  be  explained  by  a fast  and  a slow compo(cid:173) nent in the synaptic adaptation.  (i) Fast synaptic depression leads to sat(cid:173) uration  of the CRF and  phase  advance  in  the cortical  response to  high  contrast stimuli.  (ii) Slow adaptation of the synaptic transmitter release  probability is derived such that the mutual information between the input  and  the output of a cortical neuron  is  maximal.  This component-given  by  infomax learning rule-explains contrast adaptation of the averaged  membrane  potential  (DC  component)  as  well  as  the surprising experi(cid:173) mental  result,  that  the  stimulus  modulated component  (Fl  component)  of a cortical cell's membrane potential adapts only weakly. Based on our  results,  we propose a new  experiment to estimate the strength of the ef(cid:173) fective  excitatory  feedback  to  a  cortical  neuron,  and  we  also  suggest a  relatively  simple experimental  test to justify our hypothesized synaptic  mechanism for contrast adaptation.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/42d6c7d61481d1c21bd1635f59edae05-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/42d6c7d61481d1c21bd1635f59edae05-Bibtex.bib",
            "SUPP": ""
        }
    },
    "38": {
        "TITLE": "Optimizing Correlation Algorithms for Hardware-Based Transient Classification",
        "AUTHORS": "R. Timothy Edwards, Gert Cauwenberghs, Fernando J. Pineda",
        "ABSTRACT": "The perfonnance of dedicated VLSI neural processing hardware depends  critically  on  the  design  of the  implemented  algorithms.  We  have  pre(cid:173) viously  proposed an  algorithm  for  acoustic  transient classification  [1].  Having implemented and demonstrated this algorithm in a mixed-mode  architecture,  we  now  investigate  variants  on  the algorithm,  using  time  and frequency channel differencing, input and output nonnalization, and  schemes to binarize and train the template values, with the goal of achiev(cid:173) ing optimal classification perfonnance for the chosen hardware.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/4462bf0ddbe0d0da40e1e828ebebeb11-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/4462bf0ddbe0d0da40e1e828ebebeb11-Bibtex.bib",
            "SUPP": ""
        }
    },
    "39": {
        "TITLE": "Information Maximization in Single Neurons",
        "AUTHORS": "Martin Stemmler, Christof Koch",
        "ABSTRACT": "Information from  the  senses must be compressed into the  limited range  of firing  rates  generated  by  spiking  nerve  cells.  Optimal  compression  uses all firing rates equally often, implying that the nerve cell's response  matches  the  statistics  of naturally  occurring  stimuli.  Since  changing  the  voltage-dependent ionic  conductances  in  the  cell  membrane  alters  the  flow  of information,  an  unsupervised,  non-Hebbian,  developmental  learning  rule  is  derived  to  adapt  the  conductances  in  Hodgkin-Huxley  model  neurons.  By  maximizing  the  rate  of information  transmission,  each firing rate within the model neuron's limited dynamic range is  used  equally often . \nAn  efficient neuronal representation of incoming sensory  information should take  advan(cid:173) tage  of the  regularity  and  scale  invariance  of stimulus  features  in  the  natural  world.  In  the  case  of vision,  this  regularity  is  reflected  in  the  typical  probabilities of encountering  particular visual contrasts, spatial orientations, or colors [1].  Given these probabilities, an  optimized neural  code  would eliminate  any  redundancy, while  devoting  increased  repre(cid:173) sentation to commonly encountered features. \nAt the level of a single spiking neuron, information about a potentially large range of stimuli  is compressed into a finite range of firing rates, since the maximum firing rate of a neuron is  limited.  Optimizing the information transmission through a single neuron in the  presence  of uniform, additive noise has an intuitive interpretation:  the most efficient representation  of the input uses every firing rate with equal probability.  An analogous principle for non(cid:173) spiking neurons has been tested experimentally by Laughlin [2], who matched the statistics \nInformation Maximization in Single Neurons \n161",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/452bf208bf901322968557227b8f6efe-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/452bf208bf901322968557227b8f6efe-Bibtex.bib",
            "SUPP": ""
        }
    },
    "40": {
        "TITLE": "Mechanisms of Generalization in Perceptual Learning",
        "AUTHORS": "Zili Liu, Daphna Weinshall",
        "ABSTRACT": "The learning of many visual perceptual tasks has been shown to be  specific  to practiced stimuli,  while  new  stimuli  require re-Iearning  from  scratch.  Here  we  demonstrate  generalization  using  a  novel  paradigm in motion discrimination  where learning has been previ(cid:173) ously  shown  to  be  specific.  We  trained  subjects  to  discriminate  the  directions  of  moving  dots,  and  verified  the  previous  results  that learning does not transfer from the trained direction to a  new  one.  However,  by  tracking  the  subjects'  performance  across  time  in  the new  direction, we  found  that their rate of learning doubled.  Therefore, learning generalized in  a  task previously considered too  difficult  for  generalization.  We  also  replicated,  in  the  second  ex(cid:173) periment, transfer following  training with  \"easy\"  stimuli.  The specificity  of perceptual learning and  the  dichotomy  between  learning of \"easy\"  vs.  \"difficult\"  tasks were hypothesized to involve  different  learning  processes,  operating  at  different  visual  cortical  areas.  Here we show how to interpret these results in terms of signal  detection  theory.  With  the  assumption  of limited  computational  resources,  we  obtain  the  observed  phenomena  - direct  transfer  and change of learning rate - for increasing levels of task 'difficulty.  It  appears  that  human  generalization  concurs  with  the  expected  behavior of a  generic discrimination system.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/46031b3d04dc90994ca317a7c55c4289-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/46031b3d04dc90994ca317a7c55c4289-Bibtex.bib",
            "SUPP": ""
        }
    },
    "41": {
        "TITLE": "Probabilistic Image Sensor Fusion",
        "AUTHORS": "Ravi K. Sharma, Todd K. Leen, Misha Pavel",
        "ABSTRACT": "We  present  a  probabilistic  method  for  fusion  of images produced  by multiple sensors.  The approach is  based on  an image formation  model in which  the sensor images are  noisy, locally linear functions  of an  underlying,  true  scene.  A  Bayesian framework  then  provides  for  maximum likelihood or  maximum a  posteriori  estimates of the  true  scene  from  the sensor  images.  Maximum likelihood estimates  of  the  parameters  of  the  image  formation  model  involve  (local)  second order image statistics, and thus are related to local principal  component  analysis.  We  demonstrate  the  efficacy  of the  method  on images from  visible-band  and infrared sensors.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/490640b43519c77281cb2f8471e61a71-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/490640b43519c77281cb2f8471e61a71-Bibtex.bib",
            "SUPP": ""
        }
    },
    "42": {
        "TITLE": "On-Line Learning with Restricted Training Sets: Exact Solution as Benchmark for General Theories",
        "AUTHORS": "H. C. Rae, Peter Sollich, Anthony C. C. Coolen",
        "ABSTRACT": "O(ws(s log d+log(dqh/ s))) and O(ws((h/ s) log q) +log(dqh/ s)) are  upper bounds for  the VC-dimension of a  set of neural networks of  units  with  piecewise  polynomial  activation  functions,  where  s  is  the  depth  of  the  network,  h  is  the  number  of  hidden  units,  w  is  the  number  of  adjustable  parameters,  q  is  the  maximum  of  the  number of polynomial segments of the activation function, and d is  the  maximum degree  of  the polynomials;  also  n(wslog(dqh/s))  is  a  lower  bound  for  the VC-dimension  of such  a  network set,  which  are tight for  the cases  s =  8(h)  and  s is  constant.  For the special  case q =  1,  the VC-dimension is  8(ws log d).",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/495dabfd0ca768a3c3abd672079f48b6-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/495dabfd0ca768a3c3abd672079f48b6-Bibtex.bib",
            "SUPP": ""
        }
    },
    "43": {
        "TITLE": "Phase Diagram and Storage Capacity of Sequence-Storing Neural Networks",
        "AUTHORS": "A. D√ºring, Anthony C. C. Coolen, D. Sherrington",
        "ABSTRACT": "We solve the dynamics of Hopfield-type neural networks which store se(cid:173) quences of patterns, close to saturation. The asymmetry of the interaction  matrix in such models leads to violation of detailed balance, ruling out an  equilibrium statistical mechanical analysis.  Using generating functional  methods we derive exact closed equations for dynamical order parame(cid:173) ters,  viz.  the  sequence overlap and  correlation and  response functions.  in  the  limit of an  infinite system size.  We  calculate the time  translation  invariant solutions of these equations. describing stationary limit-cycles.  which  leads  to  a phase diagram.  The  effective retarded  self-interaction  usually  appearing  in  symmetric  models  is  here  found  to  vanish,  which  causes  a  significantly  enlarged  storage  capacity  of eYe  ~ 0.269.  com(cid:173) pared to  eYe  ~ 0.139 for Hopfield networks s~oring static patterns.  Our  results  are  tested  against extensive  computer simulations and  excellent  agreement is found. \n212 \nA.  Diiring,  A.  C.  C.  Coo/en  and D. Sherrington \n1  INTRODUCTION AND DEFINITIONS \nWe  consider a system of N  neurons O'(t)  = {ai(t)  = ¬±1}, which can change their states  collectively  at  discrete  times  (parallel  dynamics).  Each  neuron  changes  its  state  with  a  probability Pi(t)  =  ~[l-tanh,Bai(t)[Lj Jijaj(t)+Oi(t)]], so that the transition matrix is  W[o'(s + l)IO'(s)]  = II e.BO',(s+l)[E;=l J, j O'} (s)+ o,( s)]-ln2cosh(i3[E; =1  J'JO'} ( s)+(J, (s )))",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/49af6c4e558a7569d80eee2e035e2bd7-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/49af6c4e558a7569d80eee2e035e2bd7-Bibtex.bib",
            "SUPP": ""
        }
    },
    "44": {
        "TITLE": "Example-Based Image Synthesis of Articulated Figures",
        "AUTHORS": "Trevor Darrell",
        "ABSTRACT": "We  present a  method  for  learning complex appearance mappings.  such  as  occur  with  images  of articulated  objects.  Traditional  interpolation  networks  fail  on  this  case since appearance is  not necessarily  a smooth  function  nor a linear manifold for articulated objects.  We  define an  ap(cid:173) pearance mapping from examples by constructing a set of independently  smooth interpolation networks; these networks can cover overlapping re(cid:173) gions of parameter space.  A  set  growing  procedure is  used  to  find  ex(cid:173) ample  clusters  which  are  well-approximated  within  their  convex  hull;  interpolation then proceeds only within these sets of examples. With this  method physically valid  images are produced even in regions of param(cid:173) eter space where nearby examples have different appearances.  We show  results generating both simulated and real arm images.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/49b8b4f95f02e055801da3b4f58e28b7-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/49b8b4f95f02e055801da3b4f58e28b7-Bibtex.bib",
            "SUPP": ""
        }
    },
    "45": {
        "TITLE": "Shrinking the Tube: A New Support Vector Regression Algorithm",
        "AUTHORS": "Bernhard Sch√∂lkopf, Peter L. Bartlett, Alex J. Smola, Robert C. Williamson",
        "ABSTRACT": "A new algorithm for Support Vector regression is  described.  For a priori  chosen 1/,  it automatically adjusts a flexible tube of minimal radius to the  data such that  at most a fraction  1/  of the data points lie outside.  More(cid:173) over,  it  is  shown  how  to  use  parametric  tube  shapes  with  non-constant  radius. The algorithm is analysed theoretically and experimentally.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/4d6e4749289c4ec58c0063a90deb3964-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/4d6e4749289c4ec58c0063a90deb3964-Bibtex.bib",
            "SUPP": ""
        }
    },
    "46": {
        "TITLE": "Lazy Learning Meets the Recursive Least Squares Algorithm",
        "AUTHORS": "Mauro Birattari, Gianluca Bontempi, Hugues Bersini",
        "ABSTRACT": "Lazy  learning  is  a  memory-based  technique  that,  once  a  query  is  re(cid:173) ceived, extracts a prediction interpolating locally the neighboring exam(cid:173) ples of the query which are considered relevant according to  a distance  measure.  In  this paper we propose a data-driven method to  select on a  query-by-query basis the optimal number of neighbors to be considered  for  each  prediction.  As  an  efficient way  to  identify  and  validate  local  models, the  recursive least  squares  algorithm  is  introduced  in  the  con(cid:173) text  of local  approximation and lazy learning.  Furthermore,  beside the  winner-takes-all strategy for model selection, a local combination of the  most promising models is explored.  The method  proposed  is  tested  on  six different datasets and compared with a state-of-the-art approach.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/4dcf435435894a4d0972046fc566af76-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/4dcf435435894a4d0972046fc566af76-Bibtex.bib",
            "SUPP": ""
        }
    },
    "47": {
        "TITLE": "Reinforcement Learning for Trading",
        "AUTHORS": "John E. Moody, Matthew Saffell",
        "ABSTRACT": "We propose to train trading systems by optimizing financial objec(cid:173) tive  functions  via  reinforcement  learning.  The  performance func(cid:173) tions  that  we  consider  are  profit  or  wealth,  the  Sharpe  ratio  and  our  recently  proposed  differential  Sharpe  ratio  for  online  learn(cid:173) ing.  In  Moody  &  Wu  (1997),  we  presented  empirical  results  that  demonstrate  the  advantages  of reinforcement  learning  relative  to  supervised  learning.  Here  we  extend  our  previous  work  to  com(cid:173) pare  Q-Learning to our  Recurrent  Reinforcement  Learning  (RRL)  algorithm.  We  provide  new  simulation  results  that  demonstrate  the presence  of predictability in the monthly S&P 500 Stock Index  for  the  25  year  period  1970  through  1994, as  well  as  a  sensitivity  analysis that provides economic insight into the trader's structure. \nIntroduction:  Reinforcement  Learning for  Thading \n1  The  investor's  or  trader's  ultimate goal  is  to  optimize  some  relevant  measure  of  trading  system  performance ,  such  as  profit,  economic  utility  or  risk-adjusted  re(cid:173) turn.  In  this paper , we  propose  to use  recurrent  reinforcement  learning  to directly  optimize  such  trading  system  performance  functions ,  and  we  compare  two  differ(cid:173) ent  reinforcement  learning methods.  The first,  Recurrent  Reinforcement  Learning,  uses  immediate rewards  to train the trading systems, while the second  (Q-Learning  (Watkins 1989))  approximates discounted future rewards.  These methodologies can  be applied to optimizing systems designed  to trade a single security or to trade port(cid:173) folios .  In  addition , we  propose  a  novel  value function  for  risk-adjusted  return  that  enables  learning to be done  online:  the  differential  Sharpe  ratio. \nTrading system profits depend upon sequences  of interdependent decisions,  and are  thus  path-dependent.  Optimal  trading  decisions  when  the  effects  of transactions  costs, market impact and taxes are included require knowledge of the current system  state.  In  Moody,  Wu,  Liao  &  Saffell  (1998),  we  demonstrate  that  reinforcement  learning  provides  a  more  elegant  and  effective  means for  training  trading  systems  when  transaction costs are included , than do more standard supervised  approaches. \n‚Ä¢ The authors  are also  with Nonlinear  Prediction  Systems. \n918 \nJ.  Moody and M  Saffell \nThough much theoretical progress has been made in recent  years in the area of rein(cid:173) forcement  learning, there have been  relatively few  successful,  practical applications  of the  techniques.  Notable  examples  include  Neuro-gammon  (Tesauro  1989),  the  asset  trader of Neuneier  (1996),  an elevator scheduler  (Crites  & Barto  1996)  and a  space-shuttle payload scheduler  (Zhang & Dietterich  1996). \nIn  this  paper  we  present  results  for  reinforcement  learning  trading  systems  that  outperform the S&P 500 Stock Index over a 25-year test period, thus demonstrating  the presence of predictable structure in US  stock prices.  The reinforcement learning  algorithms compared here include our new recurrent  reinforcement learning  (RRL)  method  (Moody &  Wu  1997,  Moody  et  ai.  1998)  and  Q-Learning  (Watkins 1989). \n2  Trading  Systems and  Financial Performance Functions  2.1  Structure, Profit and Wealth for  Trading Systems  We  consider  performance functions  for  systems  that  trade  a  single  1  security  with  price  series  Zt.  The trader  is  assumed  to take only long,  neutral  or  short  positions  Ft  E  {-I , 0, I}  of constant  magnitude.  The  constant  magnitude  assumption  can  be  easily  relaxed  to  enable  better  risk  control.  The  position  Ft  is  established  or  maintained  at  the  end  of each  time  interval  t,  and  is  re-assessed  at  the  end  of  period  t + 1.  A  trade  is  thus  possible  at  the  end  of each  time  period,  although  nonzero  trading  costs  will  discourage  excessive  trading.  A  trading  system  return  R t  is  realized  at the end of the time interval  (t - 1, t]  and includes  the profit or loss  resulting from  the position F t - 1  held during that interval and any  transaction cost  incurred  at  time t  due to a  difference  in the  positions Ft- 1 and  Ft. \nIn order to properly incorporate the effects of transactions costs, market impact and  taxes in  a  trader's decision making, the trader must have internal state information  and  must  therefore  be  recurrent.  An  example  of  a  single  asset  trading  system  that takes into account transactions costs and market impact has following decision  function:  Ft = F((}t; Ft-l. It)  with  It  = {Zt, Zt-1, Zt-2,¬∑¬∑.; Yt, Yt-1, Yt-2, ... } where  (}t  denotes the (learned)  system parameters at time t and It  denotes the information  set  at  time t,  which  includes  present  and  past  values  of the  price  series  Zt  and  an  arbitrary number of other external  variables denoted  Yt.  Trading systems can  be  optimized by  maximizing performance functions  U 0 such  as  profit,  wealth,  utility functions  of wealth  or  performance ratios  like  the  Sharpe  ratio.  The  simplest  and  most  natural  performance  function  for  a  risk-insensitive  trader  is  profit.  The transactions cost rate  is  denoted  6. \nAdditive profits are  appropriate  to consider  if each  trade  is  for  a  fixed  number  of shares  or  contracts  of  security  Zt.  This  is  often  the  case,  for  example,  when  trading small futures accounts or when trading standard US$ FX contracts in dollar(cid:173) denominated  foreign  currencies.  With  the  definitions  rt  = Zt  - Zt-1  and  r{  =  4 - 4-1  for  the price returns of a  risky  (traded)  asset  and a  risk-free  asset  (like T(cid:173) Bills) respectively,  the additive profit accumulated over T  time periods with trading  position size  Jl  > 0 is  then  defined  as:",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/4e6cd95227cb0c280e99a195be5f6615-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/4e6cd95227cb0c280e99a195be5f6615-Bibtex.bib",
            "SUPP": ""
        }
    },
    "48": {
        "TITLE": "Distributional Population Codes and Multiple Motion Models",
        "AUTHORS": "Richard S. Zemel, Peter Dayan",
        "ABSTRACT": "Most theoretical and empirical studies of population codes make  the assumption that underlying neuronal activities is a unique and  unambiguous value of an encoded quantity. However, population  activities can contain additional information about such things as  multiple values of or uncertainty about the quantity. We have pre(cid:173) viously suggested a method to recover extra information by treat(cid:173) ing the activities  of the population of cells  as coding for  a com(cid:173) plete distribution over the coded quantity rather than just a single  value.  We  now  show how  this approach bears on psychophys(cid:173) ical and neurophysiological studies of population codes for mo(cid:173) tion direction in tasks involving transparent motion stimuli.  We  show that, unlike standard approaches, it is able to recover mul(cid:173) tiple motions from population responses, and also that its output  is consistent with both correct and erroneous human performance  on psychophysical tasks. \nA  population code can be defined  as a  set of units  whose  activities  collectively  encode some underlying variable (or variables).  The standard view is that popu(cid:173) lation codes are useful for accurately encoding the underlying variable when the  individual units are noisy.  Current statistical approaches to interpreting popula(cid:173) tion activity reflect this view, in that they determine the optimal single value that  explains the observed activity pattern given a particular model of the noise (and  possibly a loss function).  In our work, we have pursued an alternative hypothesis, that the population en(cid:173) codes  additional information  about  the  underlying  variable,  including  multiple  values and uncertainty.  The Distributional Population Coding (DPC) framework  finds the best probability distribution across values that fits the population activity  (Zemel, Dayan, & Pouget, 1998).  The DPC framework is appealing since it makes clear how extra information can  be conveyed in a population code.  In this paper, we use it to address a particu-\nDistributional Population  Codes and Multiple Motion Models",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/4e9cec1f583056459111d63e24f3b8ef-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/4e9cec1f583056459111d63e24f3b8ef-Bibtex.bib",
            "SUPP": ""
        }
    },
    "49": {
        "TITLE": "Using Collective Intelligence to Route Internet Traffic",
        "AUTHORS": "David Wolpert, Kagan Tumer, Jeremy Frank",
        "ABSTRACT": "A  COllective INtelligence  (COIN)  is  a  set of interacting reinforce(cid:173) ment  learning  (RL)  algorithms  designed  in  an  automated fashion  so that their collective behavior optimizes a global utility function.  We summarize the theory of COINs, then present experiments us(cid:173) ing that theory to design COINs to control internet traffic routing.  These experiments  indicate that COINs  outperform all previously  investigated RL-based, shortest path routing algorithms.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/5129a5ddcd0dcd755232baa04c231698-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/5129a5ddcd0dcd755232baa04c231698-Bibtex.bib",
            "SUPP": ""
        }
    },
    "50": {
        "TITLE": "Sparse Code Shrinkage: Denoising by Nonlinear Maximum Likelihood Estimation",
        "AUTHORS": "Aapo Hyv√§rinen, Patrik O. Hoyer, Erkki Oja",
        "ABSTRACT": "Sparse  coding  is  a  method  for  finding  a  representation  of data in  which  each  of the  components of the representation is  only  rarely  significantly  active.  Such  a  representation is  closely  related  to re(cid:173) dundancy reduction and independent component analysis,  and has  some  neurophysiological  plausibility.  In  this  paper,  we  show  how  sparse coding can be used for denoising.  Using maximum likelihood  estimation of nongaussian variables corrupted by gaussian noise, we  show  how  to  apply a  shrinkage nonlinearity on  the components  of  sparse coding so  as  to reduce noise.  Furthermore,  we  show  how  to  choose the optimal sparse coding basis for  denoising.  Our method  is  closely  related  to  the  method  of wavelet  shrinkage,  but  has  the  important benefit over wavelet methods that both the features and  the shrinkage parameters are estimated directly from  the data.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/52947e0ade57a09e4a1386d08f17b656-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/52947e0ade57a09e4a1386d08f17b656-Bibtex.bib",
            "SUPP": ""
        }
    },
    "51": {
        "TITLE": "Finite-Dimensional Approximation of Gaussian Processes",
        "AUTHORS": "Giancarlo Ferrari-Trecate, Christopher K. I. Williams, Manfred Opper",
        "ABSTRACT": "Gaussian process (GP)  prediction suffers from  O(n3)  scaling with the  data set size n.  By using a finite-dimensional basis to approximate the  GP predictor,  the computational complexity  can be reduced.  We  de(cid:173) rive optimal finite-dimensional  predictors under a  number of assump(cid:173) tions,  and show  the superiority of these predictors over the  Projected  Bayes  Regression method  (which  is  asymptotically optimal).  We  also  show  how  to  calculate  the  minimal  model  size  for  a  given  n.  The  calculations are backed up  by numerical experiments.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/55c567fd4395ecef6d936cf77b8d5b2b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/55c567fd4395ecef6d936cf77b8d5b2b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "52": {
        "TITLE": "Controlling the Complexity of HMM Systems by Regularization",
        "AUTHORS": "Christoph Neukirchen, Gerhard Rigoll",
        "ABSTRACT": "This paper introduces a method for regularization ofHMM systems that  avoids parameter overfitting caused by insufficient training data.  Regu(cid:173) larization is  done by augmenting the  EM  training method by a penalty  term  that  favors  simple  and  smooth HMM  systems.  The penalty  term  is  constructed as  a mixture model of negative exponential distributions  that is assumed to generate the state dependent emission probabilities of  the HMMs.  This new method is the successful transfer of a well known  regularization approach in neural networks to the HMM domain and can  be interpreted as a generalization of traditional state-tying for HMM sys(cid:173) tems.  The effect of regularization is demonstrated for continuous speech  recognition tasks by improving overfitted triphone models and by speaker  adaptation with limited training data.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/5607fe8879e4fd269e88387e8cb30b7e-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/5607fe8879e4fd269e88387e8cb30b7e-Bibtex.bib",
            "SUPP": ""
        }
    },
    "53": {
        "TITLE": "Scheduling Straight-Line Code Using Reinforcement Learning and Rollouts",
        "AUTHORS": "Amy McGovern, J. Eliot B. Moss",
        "ABSTRACT": "In  1986, Tanner and Mead [1] implemented an interesting constraint sat(cid:173) isfaction  circuit  for  global  motion  sensing  in  a VLSI.  We  report  here  a  new  and  improved a VLSI implementation that provides smooth optical  flow as well as global motion in a two dimensional visual field.  The com(cid:173) putation of optical flow  is  an ill-posed problem, which expresses itself as  the aperture problem.  However, the optical flow  can be estimated by the  use of regularization methods, in  which additional constraints are intro(cid:173) duced in  terms of a global energy functional that must be minimized . We  show how the algorithmic constraints of Hom and Schunck [2]  on com(cid:173) puting smooth optical flow can be mapped onto the physical constraints  of an equivalent electronic network.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/596f713f9a7376fe90a62abaaedecc2d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/596f713f9a7376fe90a62abaaedecc2d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "54": {
        "TITLE": "Learning to Estimate Scenes from Images",
        "AUTHORS": "William T. Freeman, Egon C. Pasztor",
        "ABSTRACT": "We  seek  the  scene  interpretation  that  best  explains  image  data.  For example,  we  may want to infer the projected velocities  (scene)  which  best  explain  two  consecutive  image  frames  (image).  From  synthetic data , we  model the relationship between image and scene  patches, and between a scene patch and neighboring scene patches.  Given' a  new  image,  we  propagate likelihoods in  a  Markov network  (ignoring  the  effect  of loops)  to  infer  the  underlying  scene.  This  yields  an  efficient  method  to form  low-level  scene  interpretations.  We  demonstrate the technique for  motion analysis  and estimating  high  resolution images from  low-resolution ones.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/5c50b4df4b176845cd235b6a510c6903-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/5c50b4df4b176845cd235b6a510c6903-Bibtex.bib",
            "SUPP": ""
        }
    },
    "55": {
        "TITLE": "Learning Curves for Gaussian Processes",
        "AUTHORS": "Peter Sollich",
        "ABSTRACT": "I  consider the problem of calculating learning curves  (i.e., average  generalization performance) of Gaussian processes used for  regres(cid:173) sion.  A  simple  expression  for  the  generalization  error in  terms of  the eigenvalue decomposition of the covariance function  is  derived,  and  used  as  the starting point for  several  approximation schemes.  I  identify  where  these  become  exact,  and  compare  with  existing  bounds  on  learning  curves;  the  new  approximations,  which  can  be used for  any input space dimension,  generally get  substantially  closer  to the truth. \n1 \nINTRODUCTION:  GAUSSIAN PROCESSES \nWithin  the  neural  networks  community,  there  has  in  the  last  few  years  been  a  good  deal  of excitement  about  the  use  of  Gaussian  processes  as  an  alternative to  feedforward  networks  [lJ.  The  advantages  of  Gaussian  processes  are  that  prior  assumptions  about  the  problem  to  be  learned  are  encoded  in  a  very  transparent  way,  and  that inference-at least in  the  case  of regression  that  I  will  consider-is  relatively  straightforward.  One crucial  question  for  applications  is  then  how  'fast'  Gaussian processes learn,  i.e.,  how many training examples are needed to achieve a  certain level  of generalization performance.  The typical  (as  opposed to worst case)  behaviour is  captured in  the learning  curve,  which gives  the average generalization  error  ‚Ç¨  as  a  function  of the  number  of training examples  n.  Several  workers  have  [2,3, 4J  or studied its large n asymptotics.  As I will illustrate  derived bounds on ‚Ç¨(n)  below, however, the existing bounds are often far from tight;  and asymptotic results  will  not  necessarily  apply for  realistic  sample sizes  n.  My  main  aim  in  this  paper  is  therefore to derive  approximations  to  ‚Ç¨(  n)  which  get  closer  to the true learning  curves than existing bounds, and apply  both for  small  and large n.  In  its  simplest form,  the  regression  problem  that I  am considering  is  this:  We  are  trying  to  learn  a  function  0*  which  maps  inputs  x  (real-valued  vectors)  to  (real(cid:173) valued scalar)  outputs O*(x) .  We  are given a set of training data D, consisting of n \n'Present address:  Department of Mathematics, King's College London, Strand, London \nWC2R 2LS,  U.K.  Email peter.sollicMlkcl.ac . uk \nLearning Curves for Gaussian Processes \n345 \ninput-output pairs (Xl, yt);  the training outputs Yl  may differ from the 'clean' target  outputs 9* (xL)  due to corruption by noise.  Given  a test input x,  we  are then asked  to come up with a prediction 9(x)  for  the corresponding output, expressed either in  the simple form  of a mean prediction 9(x)  plus error bars, or more comprehensively  in terms of a 'predictive distribution' P(9(x)lx, D).  In a Bayesian setting, we do this  by  specifying  a  prior  P(9)  over  our hypothesis  functions,  and a  likelihood  P(DI9)  with which each 9 could have generated the training data; from  this  we  deduce the  posterior distribution P(9ID)  ex  P(DI9)P(9).  In  the case of feedforward  networks,  where the hypothesis functions 9 are parameterized by a set of network weights, the  predictive distribution then needs to be extracted by integration over this posterior,  either by  computationally intensive  Monte  Carlo techniques  or  by  approximations  which lead to analytically tractable integrals.  For a Gaussian process, on the other  hand,  obtaining  the  predictive  distribution  is  trivial  (see  below);  one  reason  for  this  is  that the  prior  P(9)  is  defined  directly  over input-output  functions  9.  How  is  this  done?  Any  9  is  uniquely  determined  by  its  output  values  9(x)  for  all  x  from  the  input  domain,  and  for  a  Gaussian  process,  these  are  simply  assumed  to  have  a  joint  Gaussian  distribution  (hence  the  name).  This  distribution  can  be  specified  by  the  mean  values  (9(x))o  (which  I  assume to  be  zero  in  the  following,  as  is  commonly  done),  and  the  covariances  (9(x)9(x' ))o  =  C(x, x');  C(x, x')  is  called  the  covariance  function  of the  Gaussian  process.  It  encodes  in  an  easily  interpretable way prior assumptions about the function to be learned.  Smoothness,  for  example,  is  controlled  by  the behaviour of C(x, x')  for  x'  -+  x:  The  Ornstein(cid:173) Uhlenbeck (OU) covariance function C(x, x') ex  exp( -IX-X'l/l) produces very rough  (non-differentiable) functions, while functions sampled from the squared exponential  (SE)  prior  with  C(X,X')  ex  exp(-Ix - x' 12/(2l2))  are  infinitely  differentiable.  The  'length scale' parameter l, on the other hand, corresponds directly to the distance in  input space over which  we  expect  our function  to vary significantly.  More complex  properties  can  also  be encoded;  by  replacing l  with different  length scales  for  each  input component, for  example, relevant  (smalll) and irrelevant (large l)  inputs can  be  distinguished.  How does inference with Gaussian processes work?  I only give a brief summary here  and refer to existing reviews on the subject (see  e.g.  [5,  1])  for  details.  It  is simplest  to  assume  that  outputs  yare  generated  from  the  'clean'  values  of  a  hypothesis  function  9(x)  by  adding  Gaussian  noise  of  x-independent  variance  0'2.  The joint  distribution  of a  set  of training  outputs  {yd  and  the function  values  9(x)  is  then  also  Gaussian,  with covariances given by \nhere I  have defined  an n  x n  matrix K  and x-dependent n-component vectors k(x).  The  posterior distribution  P(9ID)  is  then obtained by  simply  conditioning on  the  {Yl}.  It is  again Gaussian and has mean  and variance",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/5cbdfd0dfa22a3fca7266376887f549b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/5cbdfd0dfa22a3fca7266376887f549b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "56": {
        "TITLE": "Facial Memory Is Kernel Density Estimation (Almost)",
        "AUTHORS": "Matthew N. Dailey, Garrison W. Cottrell, Thomas A. Busey",
        "ABSTRACT": "We  compare the  ability  of three exemplar-based memory models,  each  using  three  different  face  stimulus  representations,  to  account  for  the  probability a human subject responded \"old\" in an old/new facial mem(cid:173) ory experiment.  The models are  1) the  Generalized Context Model, 2)  SimSample,  a  probabilistic  sampling  model,  and  3)  MMOM,  a  novel  model related to kernel density estimation that explicitly encodes stim(cid:173) ulus  distinctiveness.  The representations  are  1)  positions of stimuli  in  MDS \"face space,\" 2) projections of test faces onto the  \"eigenfaces\" of  the study set, and 3) a representation based on response to a grid of Gabor  filter jets.  Of the 9 model/representation combinations, only the distinc(cid:173) tiveness model in  MDS  space predicts the observed \"morph familiarity  inversion\" effect, in  which the subjects'  false alarm rate for morphs be(cid:173) tween similar faces  is higher than their hit rate for many of the  studied  faces.  This evidence is consistent with the hypothesis that human mem(cid:173) ory for faces is a kernel density estimation task, with the caveat that dis(cid:173) tinctive faces require larger kernels than do typical faces.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/5cce8dede893813f879b873962fb669f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/5cce8dede893813f879b873962fb669f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "57": {
        "TITLE": "VLSI Implementation of Motion Centroid Localization for Autonomous Navigation",
        "AUTHORS": "Ralph Etienne-Cummings, Viktor Gruev, Mohammed Abdel Ghani",
        "ABSTRACT": "A  circuit  for  fast,  compact and  low-power focal-plane  motion  centroid  localization  is  presented.  This  chip,  which  uses  mixed  signal  CMOS  components  to  implement  photodetection,  edge  detection,  ON-set  detection  and  centroid  localization,  models  the  retina  and  superior  colliculus.  The  centroid  localization  circuit  uses  time-windowed  asynchronously  triggered  row  and  column  address  events  and  two  linear  resistive  grids  to  provide  the  analog  coordinates  of the  motion  centroid.  This  VLSI  chip  is  used  to  realize  fast  lightweight  autonavigating  vehicles.  The  obstacle  avoiding  line-following  algorithm is  discussed.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/6490791e7abf6b29a381288cc23a8223-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/6490791e7abf6b29a381288cc23a8223-Bibtex.bib",
            "SUPP": ""
        }
    },
    "58": {
        "TITLE": "Discontinuous Recall Transitions Induced by Competition Between Short- and Long-Range Interactions in Recurrent Networks",
        "AUTHORS": "N. S. Skantzos, C. F. Beckmann, Anthony C. C. Coolen",
        "ABSTRACT": "We  present exact analytical  equilibrium solutions  for  a  class  of recur(cid:173) rent neural network models,  with  both sequential and parallel neuronal  dynamics,  in  which  there  is  a  tunable  competition  between  nearest(cid:173) neighbour  and  long-range  synaptic  interactions.  This  competition  is  found  to induce novel coexistence phenomena as  well  as  discontinuous  transitions between pattern recall states, 2-cycles and non-recall states.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/655ea4bd3b5736d88afc30c9212ccddf-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/655ea4bd3b5736d88afc30c9212ccddf-Bibtex.bib",
            "SUPP": ""
        }
    },
    "59": {
        "TITLE": "An Integrated Vision Sensor for the Computation of Optical Flow Singular Points",
        "AUTHORS": "Charles M. Higgins, Christof Koch",
        "ABSTRACT": "A robust, integrative algorithm is presented for computing the position of  the focus  of expansion or axis of rotation (the singular point)  in optical  flow  fields  such  as  those  generated  by  self-motion.  Measurements  are  shown of a fully parallel CMOS analog VLSI motion sensor array which  computes the direction of local motion (sign of optical flow) at each pixel  and can directly implement this algorithm.  The flow  field  singular point  is  computed in real time  with a power consumption of less than 2 m W.  Computation of the singular point for  more general flow  fields  requires  measures of field  expansion and  rotation,  which  it  is  shown can also  be  computed in real-time hardware, again using only the sign of the optical  flow  field.  These measures, along with the location of the singular point,  provide robust real-time self-motion information for the visual guidance  of a moving platform such as a robot.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/69d658d0b2859e32cd4dc3b970c8496c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/69d658d0b2859e32cd4dc3b970c8496c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "60": {
        "TITLE": "Robust, Efficient, Globally-Optimized Reinforcement Learning with the Parti-Game Algorithm",
        "AUTHORS": "Mohammad A. Al-Ansari, Ronald J. Williams",
        "ABSTRACT": "Parti-game (Moore 1994a; Moore 1994b; Moore and Atkeson  1995) is a  reinforcement learning (RL) algorithm that has a lot of promise in over(cid:173) coming the curse of dimensionality that can plague RL algorithms when  applied to high-dimensional problems.  In  this paper we  introduce mod(cid:173) ifications to  the algorithm that further improve its performance and ro(cid:173) bustness. In addition, while parti-game solutions can be improved locally  by standard local path-improvement techniques, we introduce an add-on  algorithm in  the same spirit  as  parti-game that instead tries  to  improve  solutions in a non-local manner.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/6b8eba43551742214453411664a0dcc8-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/6b8eba43551742214453411664a0dcc8-Bibtex.bib",
            "SUPP": ""
        }
    },
    "61": {
        "TITLE": "Learning a Hierarchical Belief Network of Independent Factor Analyzers",
        "AUTHORS": "Hagai Attias",
        "ABSTRACT": "Many  belief  networks  have  been  proposed  that  are  composed  of  binary units.  However,  for  tasks such  as  object  and speech  recog(cid:173) nition  which  produce real-valued  data,  binary network models  are  usually inadequate.  Independent component analysis  (ICA)  learns  a  model  from  real  data,  but  the  descriptive  power  of this  model  is  severly  limited.  We  begin  by  describing the  independent  factor  analysis  (IFA)  technique,  which  overcomes some of the limitations  of ICA.  We  then  create  a  multilayer  network  by  cascading single(cid:173) layer  IFA  models.  At  each  level,  the  IFA  network  extracts  real(cid:173) valued  latent  variables  that  are  non-linear  functions  of the  input  data  with  a  highly  adaptive  functional  form,  resulting  in  a  hier(cid:173) archical  distributed  representation  of  these  data.  Whereas  exact  maximum-likelihood  learning of the network  is  intractable,  we  de(cid:173) rive an algorithm that maximizes a  lower  bound on the likelihood,  based on a  variational approach.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/6ba3af5d7b2790e73f0de32e5c8c1798-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/6ba3af5d7b2790e73f0de32e5c8c1798-Bibtex.bib",
            "SUPP": ""
        }
    },
    "62": {
        "TITLE": "A Reinforcement Learning Algorithm in Partially Observable Environments Using Short-Term Memory",
        "AUTHORS": "Nobuo Suematsu, Akira Hayashi",
        "ABSTRACT": "We  describe  a  Reinforcement Learning  algorithm  for  partially  observ(cid:173) able environments using short-term memory, which  we call BLHT. Since  BLHT learns a stochastic model  based on Bayesian Learning, the over(cid:173) fitting  problem  is  reasonably  solved.  Moreover,  BLHT has  an  efficient  implementation. This paper shows that the model learned by BLHT con(cid:173) verges  to  one  which  provides the  most accurate  predictions of percepts  and rewards, given short-term memory.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/6d3a1e06d6a06349436bc054313b648c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/6d3a1e06d6a06349436bc054313b648c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "63": {
        "TITLE": "Maximum-Likelihood Continuity Mapping (MALCOM): An Alternative to HMMs",
        "AUTHORS": "David A. Nix, John E. Hogden",
        "ABSTRACT": "We describe Maximum-Likelihood Continuity Mapping (MALCOM), an  alternative to  hidden Markov models (HMMs) for processing sequence  data such as speech.  While HMMs have a discrete \"hidden\" space con(cid:173) strained by  a fixed  finite-automaton architecture, MALCOM has a con(cid:173) tinuous hidden space-a continuity map-that is  constrained only  by  a  smoothness requirement on paths through the space.  MALCOM fits  into  the same probabilistic framework for speech recognition as  HMMs, but  it represents  a  more  realistic  model  of the  speech  production  process.  To  evaluate the extent to which MALCOM captures speech production  information, we  generated continuous speech continuity maps  for three  speakers  and  used  the  paths  through  them  to  predict measured  speech  articulator data.  The median  correlation  between  the MALCOM paths  obtained from  only the  speech  acoustics  and  articulator measurements  was 0.77 on an  independent test set not used  to train MALCOM or the  predictor.  This  unsupervised  model  achieved  correlations  over speak(cid:173) ers and articulators only 0.02 to 0.15 lower than those obtained using an  analogous supervised method which used articulatory measurements as  well as acoustics ..",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/6dd4e10e3296fa63738371ec0d5df818-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/6dd4e10e3296fa63738371ec0d5df818-Bibtex.bib",
            "SUPP": ""
        }
    },
    "64": {
        "TITLE": "Semiparametric Support Vector and Linear Programming Machines",
        "AUTHORS": "Alex J. Smola, Thilo-Thomas Frie√ü, Bernhard Sch√∂lkopf",
        "ABSTRACT": "Semiparametric models  are  useful  tools  in the case  where  domain  knowledge exists about the function to be estimated or emphasis is  put onto understandability of the  model.  We  extend two learning  algorithms  - Support  Vector  machines  and  Linear  Programming  machines  to  this  case  and  give  experimental  results  for  SV  ma(cid:173) chines.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/70efba66d3d8d53194fb1a8446ae07fa-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/70efba66d3d8d53194fb1a8446ae07fa-Bibtex.bib",
            "SUPP": ""
        }
    },
    "65": {
        "TITLE": "Regularizing AdaBoost",
        "AUTHORS": "Gunnar R√§tsch, Takashi Onoda, Klaus R. M√ºller",
        "ABSTRACT": "Boosting methods  maximize  a  hard  classification  margin  and  are  known as powerful techniques that do not exhibit overfitting for low  noise  cases.  Also for noisy data boosting will  try to enforce a  hard  margin and thereby give  too much  weight  to outliers,  which  then  leads to the dilemma of non-smooth fits  and overfitting.  Therefore  we  propose three algorithms to allow for  soft margin classification  by introducing regularization with slack variables into the boosting  concept:  (1)  AdaBoostreg  and  regularized  versions  of  (2)  linear  and  (3)  quadratic programming AdaBoost.  Experiments show  the  usefulness of the proposed algorithms in comparison to another soft  margin classifier:  the support vector machine. \n1",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/71a58e8cb75904f24cde464161c3e766-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/71a58e8cb75904f24cde464161c3e766-Bibtex.bib",
            "SUPP": ""
        }
    },
    "66": {
        "TITLE": "Graph Matching for Shape Retrieval",
        "AUTHORS": "Benoit Huet, Andrew D. J. Cross, Edwin R. Hancock",
        "ABSTRACT": "We  propose a new in-sample cross validation based method (randomized  GACV) for choosing smoothing or bandwidth parameters that govern the  bias-variance or fit-complexity tradeoff in  'soft' classification.  Soft clas(cid:173) sification refers to  a  learning procedure which  estimates the probability  that an example with a given attribute vector is  in  class  1 vs  class O.  The  target  for  optimizing  the  the  tradeoff is  the  Kullback-Liebler distance  between  the  estimated  probability  distribution  and  the  'true'  probabil(cid:173) ity  distribution,  representing  knowledge of an  infinite  population.  The  method uses a randomized estimate of the trace of a Hessian and mimics  cross validation at the cost of a single relearning with perturbed outcome  data.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/729c68884bd359ade15d5f163166738a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/729c68884bd359ade15d5f163166738a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "67": {
        "TITLE": "DTs: Dynamic Trees",
        "AUTHORS": "Christopher K. I. Williams, Nicholas J. Adams",
        "ABSTRACT": "In  this  paper we  introduce a new  class of image models,  which  we  call  dynamic trees or  DTs.  A dynamic tree model specifies  a prior  over a  large number of trees, each one of which is  a tree-structured  belief  net  (TSBN).  Experiments  show  that  DTs  are  capable  of  generating images that are less blocky, and the models have better  translation  invariance properties  than  a  fixed,  \"balanced\"  TSBN.  We  also show that Simulated Annealing is effective at finding trees  which  have high  posterior probability.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/748ba69d3e8d1af87f84fee909eef339-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/748ba69d3e8d1af87f84fee909eef339-Bibtex.bib",
            "SUPP": ""
        }
    },
    "68": {
        "TITLE": "Adding Constrained Discontinuities to Gaussian Process Models of Wind Fields",
        "AUTHORS": "Dan Cornford, Ian T. Nabney, Christopher K. I. Williams",
        "ABSTRACT": "Gaussian Processes provide good prior models for  spatial data,  but can  be  too  smooth.  In  many  physical  situations  there  are  discontinuities  along bounding surfaces, for example fronts in near-surface wind fields.  We  describe  a  modelling  method  for  such  a  constrained  discontinuity  and demonstrate how to  infer the  model parameters in wind fields  with  MCMC sampling.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/77f959f119f4fb2321e9ce801e2f5163-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/77f959f119f4fb2321e9ce801e2f5163-Bibtex.bib",
            "SUPP": ""
        }
    },
    "69": {
        "TITLE": "The Belief in TAP",
        "AUTHORS": "Yoshiyuki Kabashima, David Saad",
        "ABSTRACT": "We  show  the  similarity  between  belief propagation  and  TAP,  for  decoding  corrupted  messages  encoded  by  Sourlas's  method.  The  latter is  a  special  case of the Gallager  error-correcting code, where  the code word comprises products of J{ bits selected randomly from  the original message.  We examine the efficacy of solutions obtained  by the two methods for various values of J{ and show that solutions  for  J{  2':  3  may  be  sensitive  to  the  choice  of initial  conditions  in  the  case  of unbiased patterns.  Good  approximations  are obtained  generally  for  J{ = 2  and  for  biased  patterns  in  the  case  of  J{ 2':  3,  especially when  Nishimori's  temperature is  being used.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/7949e456002b28988d38185bd30e77fd-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/7949e456002b28988d38185bd30e77fd-Bibtex.bib",
            "SUPP": ""
        }
    },
    "70": {
        "TITLE": "Synergy and Redundancy among Brain Cells of Behaving Monkeys",
        "AUTHORS": "Itay Gat, Naftali Tishby",
        "ABSTRACT": "Determining the relationship between  the activity of a single nerve  cell  to that of an entire  population is  a  fundamental question  that  bears  on  the  basic  neural  computation  paradigms.  In  this  paper  we  apply  an  information theoretic  approach  to  quantify  the  level  of cooperative  activity  among cells  in  a  behavioral  context.  It is  possible to discriminate between  synergetic  activity of the cells  vs .  redundant  activity, depending on the difference  between  the infor(cid:173) mation  they  provide  when  measured  jointly  and  the  information  they  provide independently.  We define  a synergy  value that is  pos(cid:173) itive in the first  case and negative in  the second  and show  that the  synergy value can be measured by detecting the behavioral mode of  the animal from  simultaneously recorded  activity of the  cells.  We  observe  that  among  cortical  cells  positive  synergy  can  be  found,  while cells  from the  basal ganglia, active during the same task, do  not  exhibit similar synergetic  activity. \ntitay,tishby}@cs.huji.ac.il  Permanent  address:  Institute  of Computer  Science  and  Center for  Neural  Computa(cid:173)\ntion,  The  Hebrew  University,  Jerusalem  91904,  Israel.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/7a6a74cbe87bc60030a4bd041dd47b78-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/7a6a74cbe87bc60030a4bd041dd47b78-Bibtex.bib",
            "SUPP": ""
        }
    },
    "71": {
        "TITLE": "Classification on Pairwise Proximity Data",
        "AUTHORS": "Thore Graepel, Ralf Herbrich, Peter Bollmann-Sdorra, Klaus Obermayer",
        "ABSTRACT": "We  investigate the problem of learning a classification task on data  represented in terms of their pairwise proximities.  This representa(cid:173) tion does  not refer  to an explicit feature  representation of the data  items and  is  thus  more  general than the standard approach  of us(cid:173) ing Euclidean feature vectors,  from  which  pairwise proximities can  always  be  calculated.  Our  first  approach  is  based  on  a  combined  linear  embedding  and  classification  procedure  resulting  in  an  ex(cid:173) tension of the Optimal Hyperplane algorithm to pseudo-Euclidean  data.  As  an  alternative  we  present  another  approach  based  on  a  linear threshold model in the proximity values themselves, which is  optimized using Structural Risk Minimization.  We  show that prior  knowledge about the problem can be incorporated by the choice of  distance measures and examine different metrics W.r.t.  their gener(cid:173) alization.  Finally, the algorithms are successfully applied to protein  structure  data  and  to  data from  the  cat's  cerebral  cortex.  They  show  better performance than K-nearest-neighbor classification.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/7bd28f15a49d5e5848d6ec70e584e625-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/7bd28f15a49d5e5848d6ec70e584e625-Bibtex.bib",
            "SUPP": ""
        }
    },
    "72": {
        "TITLE": "Using Analytic QP and Sparseness to Speed Training of Support Vector Machines",
        "AUTHORS": "John C. Platt",
        "ABSTRACT": "Training a Support Vector Machine (SVM) requires the solution of a very  large quadratic programming (QP) problem.  This paper proposes an  al(cid:173) gorithm for training SVMs:  Sequential Minimal Optimization,  or SMO.  SMO breaks the large QP problem into a series of smallest possible QP  problems which  are  analytically  solvable.  Thus,  SMO does not require  a numerical QP library.  SMO's computation time is dominated by eval(cid:173) uation  of the  kernel,  hence  kernel  optimizations  substantially  quicken  SMO. For the MNIST database, SMO is  1.7 times as fast as PCG chunk(cid:173) ing;  while for  the  UCI Adult database  and  linear SVMs,  SMO can  be  1500 times faster than the PCG chunking algorithm.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/7e1d842d0f7ee600116ffc6b2d87d83f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/7e1d842d0f7ee600116ffc6b2d87d83f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "73": {
        "TITLE": "Dynamically Adapting Kernels in Support Vector Machines",
        "AUTHORS": "Nello Cristianini, Colin Campbell, John Shawe-Taylor",
        "ABSTRACT": "The kernel-parameter is one of the few  tunable parameters in  Sup(cid:173) port  Vector  machines,  controlling  the  complexity  of the  resulting  hypothesis.  Its  choice  amounts to model  selection  and  its  value  is  usually  found  by  means  of a  validation  set.  We  present  an  algo(cid:173) rithm which  can automatically perform  model  selection with little  additional computational cost and with no need of a validation set .  In  this  procedure  model  selection  and  learning  are  not  separate,  but  kernels  are  dynamically  adjusted  during  the  learning  process  to find  the kernel parameter which  provides the best possible upper  bound on  the  generalisation error.  Theoretical  results  motivating  the  approach  and  experimental  results  confirming  its  validity  are  presented.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/7fb8ceb3bd59c7956b1df66729296a4c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/7fb8ceb3bd59c7956b1df66729296a4c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "74": {
        "TITLE": "Temporally Asymmetric Hebbian Learning, Spike liming and Neural Response Variability",
        "AUTHORS": "L. F. Abbott, Sen Song",
        "ABSTRACT": "Recent experimental data indicate that the strengthening or weakening of  synaptic connections between neurons depends on  the relative timing of  pre- and postsynaptic action potentials. A Hebbian synaptic modification  rule based on these data leads to a stable state in which the excitatory and  inhibitory inputs to a neuron are balanced, producing an irregular pattern  of firing.  It  has  been  proposed  that  neurons  in  vivo  operate  in  such  a  mode.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/806beafe154032a5b818e97b4420ad98-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/806beafe154032a5b818e97b4420ad98-Bibtex.bib",
            "SUPP": ""
        }
    },
    "75": {
        "TITLE": "Learning Mixture Hierarchies",
        "AUTHORS": "Nuno Vasconcelos, Andrew Lippman",
        "ABSTRACT": "The hierarchical representation of data has various applications in do(cid:173) mains such as data mining, machine vision, or information retrieval. In  this paper we introduce an extension of the Expectation-Maximization  (EM) algorithm that learns mixture hierarchies in a computationally ef(cid:173) ficient manner. Efficiency is achieved by progressing in a bottom-up  fashion, i.e. by clustering the mixture components of a given level in the  hierarchy to obtain those of the level above. This cl ustering requires onl y  knowledge of the mixture parameters, there being no need to resort to  intermediate samples. In addition to practical applications, the algorithm  allows a new interpretation of EM that makes clear the relationship with  non-parametric kernel-based estimation methods, provides explicit con(cid:173) trol over the trade-off between the bias and variance of EM estimates, and  offers new insights about the behavior of deterministic annealing methods  commonly used with EM to escape local minima of the likelihood.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/819c9fbfb075d62a16393b9fe4fcbaa5-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/819c9fbfb075d62a16393b9fe4fcbaa5-Bibtex.bib",
            "SUPP": ""
        }
    },
    "76": {
        "TITLE": "On the Optimality of Incremental Neural Network Algorithms",
        "AUTHORS": "Ron Meir, Vitaly Maiorov",
        "ABSTRACT": "We  study the approximation of functions by  two-layer feedforward neu(cid:173) ral  networks,  focusing  on  incremental  algorithms  which  greedily  add  units,  estimating  single  unit  parameters  at  each  stage.  As  opposed  to  standard algorithms for fixed architectures, the optimization at each stage  is  performed over a small  number of parameters, mitigating many of the  difficult numerical problems inherent in  high-dimensional non-linear op(cid:173) timization.  We  establish  upper bounds on  the  error incurred by  the  al(cid:173) gorithm, when approximating functions from the  Sobolev class, thereby  extending previous results  which only provided rates of convergence for  functions in certain convex hulls of functional spaces. By comparing our  results to recently derived lower bounds,  we  show  that the greedy algo(cid:173) rithms  are  nearly  optimal.  Combined  with  estimation  error results  for  greedy algorithms, a strong case can be made for this type of approach. \n1  Introduction and background \nA  major problem  in  the  application  of neural  networks  to  real  world  problems is  the ex(cid:173) cessively long time required for training large networks of a fixed  architecture. Moreover,  theoretical results establish the intractability of such training in  the worst case  [9][4].  Ad(cid:173) ditionally,  the problem of determining the  architecture and size of the network required to  solve  a certain  task  is  left open.  Due  to  these  problems,  several  authors  have  considered  incremental algorithms  for  constructing the  network by  the  addition  of hidden units, and  estimation of each unit's parameters incrementally.  These  approaches  possess  two desir(cid:173) able  attributes:  first,  the  optimization  is  done  step-wise,  so  that  only  a  small  number of  parameters  need  to  be  optimized at  each  stage;  and  second,  the  structure  of the  network \n-This work  was supported in part by the a grant from the  Israel  Science Foundation  tThe author was  partially supported by  the center for Absorption  in  Science, Ministry of Immi(cid:173)\ngrant Absorption,  State of Israel. \n296 \nR.  Meir and V Maiorov \nis  established concomitantly with the learning, rather than specifying it in advance.  How(cid:173) ever,  until  recently these algorithms have been rather heuristic in  nature, as  no  guaranteed  performance bounds had been established.  Note that  while there  has  been  a recent surge  of interest  in  these  types  of algorithms,  they  in  fact  date back  to  work done  in  the  early  seventies (see [3]  for a historical survey). \nThe first theoretical result establishing performance bounds for incremental approximations  in  Hilbert space, was given by Jones  [8].  This work  was  later extended by Barron [2],  and  applied to  neural network approximation of functions characterized by  certain  conditions  on  their Fourier coefficients.  The  work  of Barron  has  been  extended  in  two  main direc(cid:173) tions.  First, Lee et at.  [10]  have considered approximating general functions using Hilbert  space  techniques,  while  Donahue et al.  [7]  have  provided powerful  extensions of Jones'  and  Barron's results  to general  Banach spaces.  One of the  most impressive results of the  latter work is the demonstration that iterative algorithms can, in many cases, achieve nearly  optimal rates of convergence, when approximating convex hulls. \nWhile this paper is concerned mainly with issues of approximation, we  comment that it is  highly relevant to  the statistical  problem  of learning  from  data in  neural  networks.  First,  Lee et at.  [10]  give estimation error bounds  for algorithms performing incremental  opti(cid:173) mization  with  respect to  the  training  error.  Under certain  regularity  conditions,  they  are  able to achieve rates of convergence comparable to those obtained by the much more com(cid:173) putationally  demanding algorithm  of empirical  error minimization.  Moreover,  it  is  well  known  that  upper bounds  on  the  approximation  error are  needed  in  order to  obtain  per(cid:173) formance  bounds,  both  for  parametric  and  nonparametric estimation,  where  the  latter  is  achieved using  the  method of complexity regularization.  Finally,  as  pointed out by  Don(cid:173) ahue et al.  [7],  lower bounds on  the approximation error are crucial  in  establishing  worst  case speed limitations for learning. \nThe main contribution of this paper is  as follows.  For functions belonging to  the Sobolev  class (see definition below), we establish, under appropriate conditions, near-optimal rates  of convergence for the incremental approach, and obtain explicit bounds on the parameter  values of the network.  The latter bounds are often crucial for establishing estimation error  rates.  In  contrast  to  the  work  in  [10]  and  [7],  we  characterize  approximation  rates  for  functions belonging to standard smoothness classes, such as  the Sobolev class.  The former  work establishes rates  of convergence with  respect to  the convex  hulls  of certain  subsets  of functions,  which do not relate in  a any  simple way to  standard functional classes (such  as  Lipschitz, Sobolev,  Holder, etc.).  As  far  as  we  are  aware,  the results reported here are  the  first  to  report on  such bounds for  incremental  neural  network procedures.  A detailed  version of this  work, complete with the detailed proofs, is available in  [13]. \n2  Problem statement \nWe  make use  of the  nomenclature and definitions  from  [7].  Let H  be  a Banach  space of  functions  with norm  II  . II.  For concreteness we  assume henceforth that the  norm is  given  by  the  Lq  norm,  1 < q  < 00, denoted by II  . Ilq. Let linn H  consist of all  sums of the form  L~=l aigi , gi  E H  and arbitrary ai, and COn H  is  the set of such sums  with ai  E  [0,1]  and  L~=l ai = 1.  The distances, measured in  the  Lq  norm, from  a function f  are given by \ndist(1innH,f) =  inf {l lh - fllq  :  hE linnH},  dist(conH , f) =  inf {l lh - fllq  :  hE conH}. \nThe linear span of H  is  given by  linH  = Un linn H,  while  the convex-hull of H  is  coH =  Uncon H. We  follow standard notation and denote closures of sets by a bar, e.g.  coH is  the  closure of the convex hull of H. In this work we focus on the special case where \nH  =  H1}  ~ {g  : g(x)  =  eCJ(aT x + b),  lei::;  1}, IICJ(¬∑)llq  ::;  I},",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/81c8727c62e800be708dbf37c4695dff-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/81c8727c62e800be708dbf37c4695dff-Bibtex.bib",
            "SUPP": ""
        }
    },
    "77": {
        "TITLE": "Optimizing Admission Control while Ensuring Quality of Service in Multimedia Networks via Reinforcement Learning",
        "AUTHORS": "Timothy X. Brown, Hui Tong, Satinder P. Singh",
        "ABSTRACT": "This  paper  examines  the  application  of  reinforcement  learning  to  a  telecommunications networking problem . The problem requires that rev(cid:173) enue  be  maximized  while  simultaneously  meeting  a  quality  of service  constraint  that  forbids  entry  into  certain  states.  We  present  a  general  solution  to  this  multi-criteria  problem  that  is  able  to  earn  significantly  higher revenues than alternatives.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/83e8ef518174e1eb6be4a0778d050c9d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/83e8ef518174e1eb6be4a0778d050c9d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "78": {
        "TITLE": "Coding Time-Varying Signals Using Sparse, Shift-Invariant Representations",
        "AUTHORS": "Michael S. Lewicki, Terrence J. Sejnowski",
        "ABSTRACT": "A common way to represent a time series is to divide it into short(cid:173) duration blocks,  each of which is  then represented by a set of basis  functions.  A limitation of this approach, however, is  that the tem(cid:173) poral alignment of the basis functions with the underlying structure  in the time series is arbitrary.  We present an algorithm for encoding  a time series that does not require blocking the data.  The algorithm  finds  an efficient  representation by inferring the best temporal po(cid:173) sitions  for  functions  in  a  kernel  basis.  These  can  have  arbitrary  temporal  extent  and  are  not  constrained  to  be  orthogonal.  This  allows the model to capture structure in the signal that may occur  at arbitrary temporal positions and preserves the relative temporal  structure of underlying events.  The model is shown to be equivalent  to a  very sparse and highly over complete basis.  Under this model,  the mapping from  the data to the representation is  nonlinear,  but  can  be  computed  efficiently.  This form  also  allows  the  use  of ex(cid:173) isting methods for  adapting the basis itself to data.  This approach  is  applied to speech data and results in a shift invariant, spike-like  representation that resembles coding in the cochlear nerve.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/83f2550373f2f19492aa30fbd5b57512-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/83f2550373f2f19492aa30fbd5b57512-Bibtex.bib",
            "SUPP": ""
        }
    },
    "79": {
        "TITLE": "Evidence for a Forward Dynamics Model in Human Adaptive Motor Control",
        "AUTHORS": "Nikhil Bhushan, Reza Shadmehr",
        "ABSTRACT": "Based  on  computational  principles,  the  concept  of  an  internal  model for  adaptive control has been divided  into a  forward  and an  inverse model.  However, there is as yet little evidence that learning  control by the eNS is through adaptation of one or the other.  Here  we  examine  two  adaptive control  architectures, one  based only on  the inverse model and other based on a combination of forward and  inverse  models.  We  then show that for  reaching movements of the  hand  in  novel  force  fields,  only  the  learning of the  forward  model  results  in  key  characteristics of performance  that match  the  kine(cid:173) matics of human subjects.  In  contrast, the adaptive control system  that relies only on the inverse model fails to produce the kinematic  patterns observed  in  the subjects,  despite the  fact  that it  is  more  stable.  Our results  provide evidence that learning control of novel  dynamics is  via formation of a  forward  model.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/86df7dcfd896fcaf2674f757a2463eba-Bibtex.bib",
            "SUPP": ""
        }
    },
    "80": {
        "TITLE": "Restructuring Sparse High Dimensional Data for Effective Retrieval",
        "AUTHORS": "Charles Lee Isbell Jr., Paul A. Viola",
        "ABSTRACT": "The task in text retrieval is to find  the subset of a collection of documents relevant  to  a user's  information request,  usually expressed as  a set of words.  Classically,  documents and queries are represented as  vectors of word counts.  In  its  simplest  form, relevance is  defined to be the dot product between a document and  a query  vector-a measure  of the  number of common  terms.  A  central  difficulty  in  text  retrieval  is  that  the  presence or absence  of a  word  is  not sufficient to  determine  relevance to a query. Linear dimensionality reduction has been proposed as a tech(cid:173) nique for extracting underlying structure from  the document collection.  In  some  domains  (such  as  vision)  dimensionality  reduction  reduces  computational com(cid:173) plexity.  In  text retrieval  it is  more often  used  to  improve retrieval  performance.  We  propose  an  alternative  and  novel  technique  that  produces  sparse  represen(cid:173) tations  constructed  from  sets  of highly-related  words.  Documents  and  queries  are  represented  by  their distance  to  these  sets,  and relevance  is  measured by  the  number of common clusters.  This technique significantly improves retrieval  per(cid:173) formance,  is  efficient  to  compute  and  shares  properties  with  the  optimal  linear  projection operator and the independent components of documents.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/87ec2f451208df97228105657edb717f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/87ec2f451208df97228105657edb717f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "81": {
        "TITLE": "Classification in Non-Metric Spaces",
        "AUTHORS": "Daphna Weinshall, David W. Jacobs, Yoram Gdalyahu",
        "ABSTRACT": "A  key question in vision  is  how to represent our knowledge  of previously  encountered objects to classify new ones.  The answer depends on how we  determine  the similarity  of two  objects.  Similarity  tells  us how  relevant  each previously seen object is in determining the category to which a new  object belongs.  Here a  dichotomy  emerges.  Complex notions  of similar(cid:173) ity  appear necessary for  cognitive models  and applications,  while  simple  notions of similarity  form a tractable basis for current computational ap(cid:173) proaches  to classification.  We  explore  the  nature of this dichotomy  and  why  it  calls  for  new  approaches  to  well-studied  problems  in  learning.  We  begin  this  process  by  demonstrating  new  computational  methods  for  supervised  learning  that  can  handle  complex  notions  of similarity.  (1)  We  discuss  how  to  implement  parametric  met.hods  that  represent  a  class  by  its  mean  when  using  non-metric  similarity  functions;  and  (2)  We review  non-parametric  methods  that we  have developed  using  near(cid:173) est  neighbor  classification  in  non-metric  spaces.  Point  (2) ,  and  some of  the  background  of our  work have been described  in  more detail  in  [8]. \n1  Supervised Learning and  Non-Metric Distances \nHow  can  one  represent  one 's  knowledge  of previously  encountered  objects  in  order  to  classify new  objects?  We study this question within the framework of supel vised  learning:  it is assumed that one is given a number of training objects, each labeled as  belonging to a category; one wishes to use this experience to label new  test instances  of objects.  This  problem emerges  both in  the modeling of cognitive  processes  and  in  many  practical  applications.  For  example,  one  might  want  to  identify  risky  applicants for  credit  based  on  past  experience  with clients  who  have  proven  to  be  good or  bad credit  risks.  Our  work  is  motivated by computer vision  applications. \nMost current computational approaches to supervised  learning suppose that objects  can  be  thought  of as  vectors  of numbers,  or  equivalently  as  points  lying  in  an  n(cid:173) dimensional space.  They further suppose that the similarity between objects can be  determined from  the Euclidean distance  between  these  vectors, or from some other  simple metric.  This classic notion of similarity as Euclidean or metric distance leads \nClassification in Non-Metric Spaces \n839 \nto considerable mathematical and computational simplification . \nHowever,  work  in  cognitive  psychology  has  challenged  such  simple notions of sim(cid:173) ilarity  as  models  of human  judgment,  while  applications  frequently  employ  non(cid:173) Euclidean distances  to measure object similarity.  We consider  the need for  similar(cid:173) ity measures that are not only non-Euclidean , but that are non-metric.  We focus on  proposed similarities that violate one requirement of a metric distance, the triangle  inequality.  This  states  that  if we  denote  the  distance  between  objects  A  and  B  by  d(A , B) , then :  VA , B , C  : d(A, B) + d(B, C)  ~ d(A , C) .  Distances  violating the  triangle inequality must also be non-Euclidean. \nData from  cognitive  psychology  has  demonstrated  that  similarity judgments  may  not  be  well  modeled  by  Euclidean  distances.  Tversky  [12]  has  demonstrated  in(cid:173) stances  in  which  similarity judgments may  violate  the  triangle inequality.  For  ex(cid:173) ample, close  similarity between  Jamaica and  Cuba and  between  Cuba and  Russia  does  not  imply close  similarity between  Jamaica and  Russia  (see  also  [10]) .  Non(cid:173) metric  similarity measures  are  frequently  employed  for  practical  reasons,  too  (cf.  [5]) .  In  part,  work  in  robust statistics  [7]  has shown  that methods that will survive  the  presence  of outliers, which  are extraneous  pieces  of information or  information  containing extreme errors, must employ non-Euclidean distances that in fact violate  the  triangle inequality ; related  insights  have spurred  the  widespread  use  of robust  methods in computer vision  (reviewed  in  [5]  and  [9]). \nWe are interested in handling a wide range of non-metric distance functions,  includ(cid:173) ing  those  that are so  complex that they  must  be  treated  as  a  black  box .  However,  to be concrete,  we  will focus  here  on  two  simple examples of such  distances: \nmedian distance:  This  distance  assumes  that  objects  are  representable  as  a  set  of features  whose  individual  differences  can  be  measured,  so  that  the  difference  between  two  objects  is  representable  as  a  vector:  J = (d1 , d2 ,  .. . dn ).  The  median  distance  between  the two objects  is just the median value in  this  vector.  Similarly,  one can define a k-median distance by choosing the k'th lowest element in this list.  k(cid:173) median distances are often used in applications (cf.  [9]) , because they are unaffected  by the exact values of the most extreme differences  between  the objects .  Only these  features  that  are  most  similar  determine  its  value.  The  k-median  distance  can  violate  the triangle inequality  to  an  arbitrary degree  (i.e. , there  are  no constraints  on  the  pairwise distances  between  three  points) .  robust  non-metric  LP  distances:  Given  a  difference  vector  J,  an  LP  distance  has  the form: \n(1) \nand is  non-metric for  p < 1.  Figure  1  illustrates  why  these  distances  present  significant  new  challenges  in  su(cid:173) pervised  learning.  Suppose  that given  some datapoints  (two in  Fig.  1) , we  wish  to  classify  each  new  point  as coming from  the  same category  as  its  nearest  neighbor.  Then  we  need  to determine the Voronoi  diagram generated  by our data:  a  division  of the  plane  into  regions  in  which  the  points  all  have  the  same  nearest  neighbor.  Fig.  1 shows  how  the Voronoi diagram changes  with  the function  used  to compute  the distance between datapoints; the non-metric diagrams (rightmost three pictures  in  Fig.  1)  are more complex and  more likely  to make non-intuitive predictions.  In  fact , very  little is  known about  the computation of non-metric  Voronoi diagrams. \nWe  now  describe  new  parametric methods for  supervised  learning with  non-metric \n840 \nD. Weins hall,  D.  W Jacobs and Y.  Gdalyahu \nFigure  1:  The Voronoi  diagram  for  two  points  using,  from  left  to right,  p-distances  with  p  =  2  (Euclidean),  p  =  1  (  Manhattan,  which  is  still  metric),  the  non-metric  distances  arising  from  p  =  0.5,  p  =  0.2,  and  the min  (I-median)  distance.  The min distance  in  2-D  illustrates  the behavior of the other median distances  in  higher  dimensions.  The region of  the  plane  closer  to one point  is  shown  in  black, and closer  to the other in  white. \ndistances,  and review  non-parametric methods that we  described  in  [8]. \n2  Parametric methods:  what  should  replace the  mean \nParametric  methods  typically  represent  objects  as  vectors  in  a  high-dimensional  space,  and  represent  classes  and  the  boundaries  between  them  in  this  space  us(cid:173) ing  geometric  constructions  or  probability  distributions  with  a  limited  number  of  parameters.  One  can  attempt  to  extend  these  techniques  to  specific  non-metric  distances,  such  as  the  median  distance ,  or  non-metric  LP  distances.  We  discuss  the  example  of the  mean  of a  class  below.  One  can  also  redefine  geometric  ob(cid:173) jects such  as  linear separators,  for  specific  non-metric distances.  However,  existing  algorithms for  finding  such  objects  in  Euclidean  spaces  will  no  longer  be  directly  suitable,  nor  will  theoretical  results  about  such  representations  hold.  Many  prob(cid:173) lems  are  therefore  open  in  determining  how  to  best  apply  parametric supervised  learning techniques  to specific  non-metric distances. \n1 \nWe analyze k-means clustering where each class is  represented  by its average mem(cid:173) ber;  new elements are then classified according to which of these  prototypical exam(cid:173) ples  is  nearest .  In  Euclidean space,  the mean is  the  point q whose  sum of squared  distances  to all  the class members {qdr=l  - (2:~1 d(ij, qi)2)2  - is  minimized.  Suppose  now  that  our  data  come  from  a  vector  space  where  the  correct  distance  is  the  LP  distance  from  (1).  Using  the  natural  extension  of the  above  definition,  we  should  represent  each  class  by  the  point  ij  whose  sum  of distances  to  all  the  class  members - (2:~=1 d(ij, qi)P) p  - is  minimal.  It is  now  possible  to  show  (proof  is  omitted)  that for  p  <  1  (the  non-metric cases),  the  exact  value  of every  feature  of the representative  point ij must have already appeared in  at least one element in  the  class.  Moreover,  the  value of these features  can be determined separately  with  complexity O(n 2 ),  and  total  complexity of O(dn 2 )  given  d features .  ij  is  therefore  determined  by  a  mixture  of up  to  d  exemplars,  where  d is  the  dimension  of the  vector  space.  Thus there  are efficient  algorithms for  finding  the  \"mean\"  element of  a  class,  even  using certain non-metric distances. \n1 \nWe  will  illustrate  these  results  with  a  concrete  example using  the  corel  database,  a  commercial database of images pre-labeled  by categories  (such  as  \"lions\"), where  non-metric distance functions have  proven  effective  in  determining the similarity of  images [1] .  The corel database is very large, making the use of prototypes desirable. \nWe  represent  each  image  using  a  vector  of  11  numbers  describing  general  image  properties,  such  as color histograms, as described  in  [1] .  We consider  the Euclidean \nClassification in  Non-Metric Spaces \n841 \nand  L0 5  distances,  and  their  corresponding  prototypes:  the  mean  and  the  LO.5_  prototype computed according  to the  result  above.  Given  the first  45  classes, each  containing 100 images , we found their corresponding prototypes; we  then computed  the percentage of images in each class that are closest  to their own  prototype, using  either the Euclidean or the L 0.5  distance and one of the two prototypes.  The results  are the following: \nmean  d existing features",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/88a199611ac2b85bd3f76e8ee7e55650-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/88a199611ac2b85bd3f76e8ee7e55650-Bibtex.bib",
            "SUPP": ""
        }
    },
    "82": {
        "TITLE": "Approximate Learning of Dynamic Models",
        "AUTHORS": "Xavier Boyen, Daphne Koller",
        "ABSTRACT": "Inference is a key  component in  learning probabilistic models from par(cid:173) tially  observable  data.  When  learning  temporal  models,  each  of the  many  inference phases requires a  traversal  over an  entire long data se(cid:173) quence;  furthermore,  the  data structures manipulated are exponentially  large, making this process computationally expensive.  In [2], we describe  an approximate inference algorithm for monitoring stochastic processes,  and prove bounds on its approximation error.  In this paper, we apply this  algorithm as an approximate forward propagation step in an EM algorithm  for learning temporal Bayesian networks.  We provide a related approxi(cid:173) mation for the backward step, and prove error bounds for the  combined  algorithm.  We  show empirically that,  for a real-life domain,  EM  using  our inference algorithm  is  much  faster  than  EM  using exact inference,  with almost no degradation in quality of the learned model.  We  extend  our analysis  to  the  online learning  task,  showing a  bound  on  the  error  resulting  from  restricting  attention  to  a  small  window  of observations.  We  present an  online EM  learning algorithm for  dynamic systems,  and  show that it learns much faster than standard offline EM.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/894b77f805bd94d292574c38c5d628d5-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/894b77f805bd94d292574c38c5d628d5-Bibtex.bib",
            "SUPP": ""
        }
    },
    "83": {
        "TITLE": "Independent Component Analysis of Intracellular Calcium Spike Data",
        "AUTHORS": "Klaus Prank, Julia B√∂rger, Alexander von zur M√ºhlen, Georg Brabant, Christof Sch√∂fl",
        "ABSTRACT": "Calcium (Ca2+)is an ubiquitous intracellular messenger which reg(cid:173) ulates cellular processes, such as secretion, contraction, and cell  proliferation. A number of different cell types respond to hormonal  stimuli with periodic oscillations of the intracellular free calcium  concentration ([Ca2+]i). These Ca2+ signals are often organized  in complex temporal and spatial patterns even under conditions  of sustained stimulation. Here we study the spatio-temporal as(cid:173) pects of intracellular calcium ([Ca 2+]i) oscillations in clonal J3-cells  (hamster insulin secreting cells, HIT) under pharmacological stim(cid:173) ulation (Schofi et al., 1996). We use a novel fast fixed-point al(cid:173) gorithm (Hyvarinen and Oja, 1997) for Independent Component  Analysis (ICA) to blind source separation of the spatio-temporal  dynamics of [Ca2+]i in a HIT-cell. Using this approach we find two  significant independent components out of five differently mixed in(cid:173) put signals: one [Ca2+]i signal with a mean oscillatory period of  68s and a high frequency signal with a broadband power spectrum  with considerable spectral density. This results is in good agree(cid:173) ment with a study on high-frequency [Ca2+]j oscillations (Palus  et al., 1998) Further theoretical and experimental studies have to  be performed to resolve the question on the functional impact of  intracellular signaling of these independent [Ca2+]i signals. \n932",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/89ae0fe22c47d374bc9350ef99e01685-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/89ae0fe22c47d374bc9350ef99e01685-Bibtex.bib",
            "SUPP": ""
        }
    },
    "84": {
        "TITLE": "A Model for Associative Multiplication",
        "AUTHORS": "G. Bjorn Christianson, Suzanna Becker",
        "ABSTRACT": "Despite the fact that mental arithmetic is  based on only a few  hun(cid:173) dred basic facts  and  some simple  algorithms, humans have a  diffi(cid:173) cult time  mastering the subject,  and  even  experienced  individuals  make  mistakes.  Associative  multiplication,  the  process  of doing  multiplication  by  memory  without the  use  of rules  or algorithms,  is  especially  problematic.  Humans  exhibit  certain  characteristic  phenomena in  performing associative  multiplications,  both in  the  type of error and  in  the error frequency.  We  propose a  model  for  the  process  of associative  multiplication,  and  compare  its  perfor(cid:173) mance  in  both  these  phenomena  with  data from  normal  humans  and from  the model proposed  by  Anderson  et al (1994).",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/8a146f1a3da4700cbf03cdc55e2daae6-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/8a146f1a3da4700cbf03cdc55e2daae6-Bibtex.bib",
            "SUPP": ""
        }
    },
    "85": {
        "TITLE": "Batch and On-Line Parameter Estimation of Gaussian Mixtures Based on the Joint Entropy",
        "AUTHORS": "Yoram Singer, Manfred K. Warmuth",
        "ABSTRACT": "We  describe  a  new  iterative method  for parameter estimation of Gaus(cid:173) sian  mixtures.  The new method is based on a framework developed by  Kivinen and Warmuth for supervised on-line learning. In contrast to gra(cid:173) dient descent and EM, which estimate the mixture's covariance matrices,  the proposed method estimates the inverses  of the covariance matrices.  Furthennore, the new parameter estimation procedure can  be applied in  both on-line and  batch settings.  We show experimentally that it is typi(cid:173) cally faster than EM, and usually requires about half as  many  iterations  as EM.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/8c00dee24c9878fea090ed070b44f1ab-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/8c00dee24c9878fea090ed070b44f1ab-Bibtex.bib",
            "SUPP": ""
        }
    },
    "86": {
        "TITLE": "Learning Macro-Actions in Reinforcement Learning",
        "AUTHORS": "Jette Randlov",
        "ABSTRACT": "We present a method for automatically constructing macro-actions from  scratch from primitive actions during the reinforcement learning process.  The  overall  idea is  to reinforce  the  tendency  to  perform action b after  action a  if such  a  pattern  of actions  has  been rewarded.  We  test  the  method on a bicycle task, the car-on-the-hill task, the race-track task and  some grid-world tasks.  For the  bicycle and race-track tasks  the use  of  macro-actions approximately halves the learning time, while for one of  the grid-world tasks  the learning time is  reduced by a  factor of 5.  The  method did not work for the car-on-the-hill task for reasons we discuss  in the conclusion.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/8f19793b2671094e63a15ab883d50137-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/8f19793b2671094e63a15ab883d50137-Bibtex.bib",
            "SUPP": ""
        }
    },
    "87": {
        "TITLE": "Fast Neural Network Emulation of Dynamical Systems for Computer Animation",
        "AUTHORS": "Radek Grzeszczuk, Demetri Terzopoulos, Geoffrey E. Hinton",
        "ABSTRACT": "Computer animation through the numerical simulation of physics-based  graphics  models offers  unsurpassed realism,  but  it can be computation(cid:173) ally demanding. This paper demonstrates the possibility of replacing the  numerical simulation of nontrivial dynamic  models  with  a dramatically  more  efficient  \"NeuroAnimator\"  that  exploits  neural  networks.  Neu(cid:173) roAnimators  are  automatically  trained  off-line  to  emulate  physical  dy(cid:173) namics through  the observation  of physics-based models in  action.  De(cid:173) pending on  the model,  its  neural  network emulator can yield physically  realistic  animation  one  or  two  orders  of magnitude faster  than  conven(cid:173) tional  numerical simulation.  We  demonstrate NeuroAnimators for a va(cid:173) riety of physics-based models.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/92af93f73faf3cefc129b6bc55a748a9-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/92af93f73faf3cefc129b6bc55a748a9-Bibtex.bib",
            "SUPP": ""
        }
    },
    "88": {
        "TITLE": "Neural Networks for Density Estimation",
        "AUTHORS": "Malik Magdon-Ismail, Amir F. Atiya",
        "ABSTRACT": "We  introduce two  new  techniques for  density estimation.  Our ap(cid:173) proach poses the problem as  a  supervised learning task which  can  be  performed  using  Neural  Networks.  We  introduce  a  stochas(cid:173) tic method for  learning the cumulative distribution  and an  analo(cid:173) gous  deterministic technique.  We  demonstrate convergence of our  methods  both theoretically and experimentally, and provide com(cid:173) parisons with the Parzen estimate.  Our theoretical results demon(cid:173) strate better convergence properties than the Parzen estimate. \n1 \nIntroduction and Background \nA  majority of problems in  science  and engineering have  to  be modeled  in  a  prob(cid:173) abilistic manner.  Even  if the  underlying  phenomena are  inherently  deterministic,  the complexity of these phenomena often makes a probabilistic formulation the only  feasible  approach from  the computational point of view.  Although quantities such  as  the mean, the variance, and possibly higher order moments of a random variable  have often been sufficient to characterize a particular problem, the quest for  higher  modeling accuracy,  and for  more realistic assumptions drives  us  towards modeling  the available random variables using their probability density.  This of course leads  us  to the problem of density estimation (see  [6]). \nThe most common approach for  density estimation is the nonparametric approach,  where  the  density  is  determined according to  a  formula  involving the  data  points  available.  The most  common non  parametric methods  are the  kernel  density  esti(cid:173) mator,  also known as  the Parzen window  estimator  [4]  and the  k-nearest  neighbor  technique  [1].  Non  parametric density  estimation belongs to  the  class  of ill-posed  problems in  the sense that  small changes in the  data can  lead to large changes  in \n\"To whom correspondence should be addressed. \nNeural Networks for Density Estimation \n523 \nthe estimated density.  Therefore it is important to have methods that are robust to  slight changes in the data.  For this reason some amount of regularization is  needed  [7].  This regularization is embedded in the choice of the smoothing parameter (ker(cid:173) nel width or k).  The problem with these non-parametric techniques is their extreme  sensitivity to the choice of the smoothing parameter.  A  wrong  choice  can  lead  to  either undersmoothing or oversmoothing. \nIn  spite of  the  importance of the  density  estimation  problem,  proposed  methods  using  neural  networks  have  been  very  sporadic.  We  propose  two  new  methods  for  density  estimation  which  can  be  implemented  using  multilayer  networks.  In  addition to being able to approximate any function to any given precision, multilayer  networks give  us  the flexibility to choose an error function  to suit our application.  The methods developed here are based on approximating the distribution function,  in contrast to most previous works which focus  on approximating the density itself.  Straightforward differentiation gives  us  the  estimate of the  density  function.  The  distribution  function  is  often  useful  in  its  own  right  - one  can  directly  evaluate  quantiles or the probability that the random variable occurs in a particular interval. \nOne of the techniques is a stochastic algorithm (SLC), and the second is a determin(cid:173) istic  technique  based  on  learning  the  cumulative  (SIC).  The  stochastic  technique  will  generally  be  smoother  on  smaller  numbers  of  data  points,  however,  the  de(cid:173) terministic  technique  is  faster  and  applies  to  more  that  one  dimension.  We  will  present a result on the consistency and the convergence rate of the estimation error  for  our  methods  in  the  univariate  case.  When  the  unknown  density  is  bounded  and  has  bounded  derivatives  up  to order  K,  we  find  that  the  estimation error  is  O((loglog(N)/N)-(l-t¬ª), where N  is the number of data points.  As  a comparison,  for  the kernel density estimator (with non-negative kernels), the estimation error is  O(N-4 / 5 }, under the assumptions that the unknown density has a square integrable  second  derivative  (see  [6]),  and that the  optimal kernel  width is  used,  which is  not  possible in practice because computing the optimal kernel width requires knowledge  of the  true density.  One  can  see  that for  smooth  density  functions  with  bounded  derivatives, our methods achieve an error rate that approaches O(N- 1 ). \n2  New Density Estimation Techniques \nTo  illustrate our  methods,  we  will  use  neural  networks,  but  stress  that  any  suf(cid:173) ficiently  general  learning  model  will  do  just  as  well.  The  network's  output  will  represent  an  estimate  of  the  distribution  function,  and  its  derivative  will  be  an  estimate of the density.  We  will  now  proceed to a  description of the two methods. \n2.1  SLC  (Stochastic Learning of the Cumulative) \nLet  Xn  E  R,  n  =  1, ... , N  be  the  data points.  Let  the  underlying  density  be  g(x)  and its distribution function  G(x) = J~oog(t)dt.  Let  the neural network output be  H (x, w), where w represents the set of weights of the network.  Ideally, after training  the neural network,  we  would like to have H (x, w)  =  G (x).  It can easily be shown  that the density of the random variable G(x)  (x  being generated according to g(x))  is  uniform  in  [0,1].  Thus,  if  H(x,w)  is  to  be  as  close  as  possible  to  G(x),  then  the network output should have a  density that is  close to uniform in  [0,1].  This is  what  our goal  will be.  We  will  attempt to train the  network  such  that  its output  density  is  uniform,  then  the  network  mapping  should  represent  the  distribution  function  G(x).  The basic idea behind the proposed algorithm is  to use  the N  data  points  as  inputs  to  the network.  For every  training cycle,  we  generate a  different  set of N  network targets randomly from a  uniform distribution in [0, 1],  and adjust \n524 \nM  Magdon-Ismail and A. Atiya \nthe weights  to map the data points  (sorted in ascending order)  to these  generated  targets  (also sorted in ascending order).  Thus we  are training the network to map  the data to a  uniform distribution. \nBefore describing the steps of the algorithm, we note that the resulting network has  to represent a monotonically non decreasing mapping, otherwise it will not represent  a  legitimate distribution  function.  In  our  simulations,  we  used  a  hint  penalty  to  enforce monotonicity [5].  The algorithm is  as follows. \n\nLet  Xl  S  X2  S  ...  S  XN  be  the  data  points.  Set  t  =  1,  where  t  is  the \n\ntraining cycle number.  Initialize the weights  (usually randomly)  to w(l). \n\nGenerate randomly from a uniform distribution in  [0,1]  N  points (and sort \n\nthem):  UI  S U2  S  '\"  S UN¬∑  The point Un  is  the target output for  X n¬∑  3.  Adjust the network weights according to the backpropagation scheme: \na¬£  w(t + 1) = w(t)  - 17(t) aw \n(1) \nwhere  ¬£  is  the  objective  function  that  includes  the  error  term  and  the  monotonicity hint  penalty term [5]:",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/9327969053c0068dd9e07c529866b94d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/9327969053c0068dd9e07c529866b94d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "89": {
        "TITLE": "Improved Switching among Temporally Abstract Actions",
        "AUTHORS": "Richard S. Sutton, Satinder P. Singh, Doina Precup, Balaraman Ravindran",
        "ABSTRACT": "In robotics and other control applications it is commonplace to have a pre(cid:173) existing  set  of controllers  for  solving  subtasks,  perhaps  hand-crafted  or  previously learned or planned, and still face  a difficult problem of how to  choose and switch among the controllers to solve an overall task as well as  possible.  In this paper we present a framework based on Markov decision  processes and semi-Markov decision processes for phrasing this problem,  a basic theorem regarding the improvement in performance that can be ob(cid:173) tained by switching flexibly between given controllers, and example appli(cid:173) cations of the theorem.  In particular, we show how an  agent can plan with  these high-level controllers and then use the results of such planning to find  an even better plan, by modifying the existing controllers, with negligible  additional cost and no re-planning. In one of our examples, the complexity  of the problem is  reduced from  24 billion state-action pairs to  less than a  million state-controller pairs. \nIn many applications, solutions to parts of a task are known, either because they were hand(cid:173) crafted  by  people  or because  they  were  previously  learned  or  planned.  For  example,  in  robotics applications, there may exist controllers for moving joints to  positions,  picking up  objects, controlling eye movements, or navigating along hallways.  More generally, an intelli(cid:173) gent system may have available to it several temporally extended courses of action to choose  from.  In such cases, a key challenge is  to  take full  advantage of the existing temporally ex(cid:173) tended actions, to choose or switch among them effectively, and to plan at their level rather  than at the level of individual actions. \nRecently, several researchers have begun to address these challenges within the framework of  reinforcement learning and Markov decision processes (e.g., Singh,  1992; Kaelbling,  1993;  Dayan & Hinton,  1993; Thrun and Schwartz,  1995;  Sutton,  1995;  Dietterich,  1998; Parr &  Russell,  1998;  McGovern, Sutton & Fagg,  1997).  Common to much of this recent work is  the  modeling  of a  temporally  extended action  as  a  policy  (controller)  and  a  condition  for  terminating,  which  we  together refer to  as  an  option  (Sutton,  Precup &  Singh,  1998).  In  this paper we consider the problem of effectively combining given  options into  one overall  policy, generalizing prior work by Kaelbling (1993).  Sections  1-3 introduce the framework;  our new results are in Sections 4 and 5. \nImproved Switching among Temporally Abstract Actions",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/9597353e41e6957b5e7aa79214fcb256-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/9597353e41e6957b5e7aa79214fcb256-Bibtex.bib",
            "SUPP": ""
        }
    },
    "90": {
        "TITLE": "Dynamics of Supervised Learning with Restricted Training Sets",
        "AUTHORS": "Anthony C. C. Coolen, David Saad",
        "ABSTRACT": "We  study  the  dynamics  of supervised  learning  in  layered  neural  net(cid:173) works,  in  the regime where the size p of the training set is  proportional  to the number N  of inputs.  Here the local fields  are no longer described  by  Gaussian  distributions.  We  use  dynamical  replica theory  to  predict  the  evolution  of macroscopic  observables,  including  the  relevant  error  measures, incorporating the old formalism  in the limit piN --t  00.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/95d309f0b035d97f69902e7972c2b2e6-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/95d309f0b035d97f69902e7972c2b2e6-Bibtex.bib",
            "SUPP": ""
        }
    },
    "91": {
        "TITLE": "Efficient Bayesian Parameter Estimation in Large Discrete Domains",
        "AUTHORS": "Nir Friedman, Yoram Singer",
        "ABSTRACT": "Abstract Unavailable",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/962e56a8a0b0420d87272a682bfd1e53-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/962e56a8a0b0420d87272a682bfd1e53-Bibtex.bib",
            "SUPP": ""
        }
    },
    "92": {
        "TITLE": "A Polygonal Line Algorithm for Constructing Principal Curves",
        "AUTHORS": "Bal√°zs K√©gl, Adam Krzyzak, Tam√°s Linder, Kenneth Zeger",
        "ABSTRACT": "Principal curves have  been  defined  as  \"self consistent\" smooth  curves  which pass through the \"middle\" of a d-dimensional probability distri(cid:173) bution or data cloud.  Recently, we  [1]  have offered a new approach by  defining principal curves as continuous curves of a  given length which  minimize the expected squared distance between the curve and points of  the space randomly chosen according to  a given distribution.  The new  definition made it possible to carry out a theoretical analysis of learning  principal curves from training data.  In this paper we propose a practical  construction based on the new definition.  Simulation results demonstrate  that the new algorithm compares favorably with previous methods both  in terms of performance and computational complexity.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/97d0145823aeb8ed80617be62e08bdcc-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/97d0145823aeb8ed80617be62e08bdcc-Bibtex.bib",
            "SUPP": ""
        }
    },
    "93": {
        "TITLE": "Neuronal Regulation Implements Efficient Synaptic Pruning",
        "AUTHORS": "Gal Chechik, Isaac Meilijson, Eytan Ruppin",
        "ABSTRACT": "Human and animal studies show  that mammalian brain undergoes  massive synaptic pruning during childhood , removing about half of  the  synapses  until  puberty.  We  have  previously  shown  that main(cid:173) taining network  memory performance  while  synapses  are  deleted,  requires  that  synapses  are  properly  modified  and  pruned,  remov(cid:173) ing the weaker synapses.  We  now show that neuronal regulation , a  mechanism recently  observed  to maintain the average neuronal in(cid:173) put field , results in weight-dependent synaptic modification . Under  the  correct  range  of the  degradation  dimension  and  synaptic  up(cid:173) per  bound,  neuronal  regulation  removes  the  weaker  synapses  and  judiciously  modifies  the  remaining  synapses .  It implements  near  optimal synaptic modification, and  maintains the  memory perfor(cid:173) mance  of a  network  undergoing  massive  synaptic  pruning.  Thus ,  this  paper shows  that  in  addition to the  known effects  of Hebbian  changes,  neuronal  regulation  may  play  an  important  role  in  the  self-organization of brain networks during development.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/98986c005e5def2da341b4e0627d4712-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/98986c005e5def2da341b4e0627d4712-Bibtex.bib",
            "SUPP": ""
        }
    },
    "94": {
        "TITLE": "Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms",
        "AUTHORS": "Michael J. Kearns, Satinder P. Singh",
        "ABSTRACT": "In  this  paper,  we  address  two  issues  of long-standing  interest  in  the  re(cid:173) inforcement  learning  literature.  First,  what  kinds  of performance  guar(cid:173) antees  can  be  made  for  Q-learning  after  only  a  finite  number  of actions?  Second,  what  quantitative  comparisons  can  be  made  between  Q-learning  and  model-based  (indirect)  approaches,  which  use  experience  to estimate  next-state  distributions  for  off-line  value  iteration?  We  first  show  that  both  Q-learning  and  the  indirect  approach  enjoy  rather  rapid  convergence  to  the  optimal  policy  as  a  function  of the num(cid:173) ber  of  state  transitions  observed.  In  particular,  on  the  order  of  only  (Nlog(1/c)/c2 )(log(N) + loglog(l/c))  transitions  are  sufficient  for  both  algorithms  to  come  within  c  of the  optimal  policy,  in  an  idealized  model  that  assumes  the  observed  transitions  are  \"well-mixed\"  throughout  an  N-state  MDP.  Thus,  the  two  approaches  have  roughly  the  same  sample  complexity.  Perhaps  surprisingly,  this  sample  complexity  is  far less  than  what is required for the model-based approach to actually construct a good  approximation  to  the  next-state  distribution.  The  result  also  shows  that  the  amount  of memory required  by  the model-based  approach  is  closer  to  N  than  to N 2 ‚Ä¢  For  either  approach,  to  remove  the  assumption  that  the  observed  tran(cid:173) sitions  are  well-mixed,  we  consider  a  model  in  which  the  transitions  are  determined by a fixed,  arbitrary exploration policy.  Bounds on the number  of transitions  required  in  order  to  achieve  a  desired  level  of performance  are  then  related  to  the  stationary  distribution  and  mixing  time  of  this  policy.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/99adff456950dd9629a5260c4de21858-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/99adff456950dd9629a5260c4de21858-Bibtex.bib",
            "SUPP": ""
        }
    },
    "95": {
        "TITLE": "Global Optimisation of Neural Network Models via Sequential Sampling",
        "AUTHORS": "Jo√£o F. G. de Freitas, Mahesan Niranjan, Arnaud Doucet, Andrew H. Gee",
        "ABSTRACT": "We  propose a novel strategy for  training neural networks using se(cid:173) quential sampling-importance resampling algorithms.  This global  optimisation strategy allows  us  to learn the  probability distribu(cid:173) tion of the network  weights  in  a  sequential framework.  It is  well  suited to applications involving on-line, nonlinear, non-Gaussian or  non-stationary signal processing.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/9c19a2aa1d84e04b0bd4bc888792bd1e-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/9c19a2aa1d84e04b0bd4bc888792bd1e-Bibtex.bib",
            "SUPP": ""
        }
    },
    "96": {
        "TITLE": "Non-Linear PI Control Inspired by Biological Control Systems",
        "AUTHORS": "Lyndon J. Brown, Gregory E. Gonye, James S. Schwaber",
        "ABSTRACT": "A  non-linear  modification  to  PI  control  is  motivated  by  a  model  of a signal transduction pathway active in mammalian blood pres(cid:173) sure regulation.  This control algorithm,  labeled  PII  (proportional  with  intermittent integral),  is  appropriate for  plants requiring ex(cid:173) act set-point matching and disturbance attenuation in the presence  of infrequent  step  changes  in  load  disturbances or  set-point.  The  proportional  aspect  of the  controller is  independently designed  to  be  a  disturbance  attenuator  and  set-point  matching  is  achieved  by  intermittently invoking an integral controller.  The mechanisms  observed in the Angiotensin 11/ AT1  signaling pathway are used to  control the switching of the integral control.  Improved performance  over  PI control is  shown  on  a  model  of cyc1opentenol  production.  A sign change in  plant gain at the desirable operating point causes  traditional  PI  control  to  result  in  an  unstable  system.  Applica(cid:173) tion  of this  new  approach  to  this  problem  results  in  stable exact  set-point matching for  achievable set-points. \nBiological processes have evolved sophisticated mechanisms for solving difficult con(cid:173) trol  problems.  By analyzing and understanding these natural systems it is  possible  that principles can be derived which are applicable to general control systems.  This  approach has already been the basis for  the field of artificial neural networks, which  are loosely  based on a  model of the electrical signaling of neurons.  A suitable can(cid:173) didate system for  analysis is  blood pressure control.  Tight control of blood pressure  is  critical for  survival  of an  animal.  Chronically high  levels  can lead to  premature  death.  Low blood pressure can lead to oxygen and nutrient deprivation and sudden  load changes must be quickly responded to or loss of consciousness can result.  The  baroreflex,  reflexive  change  of heart  rate  in  response  to  blood  pressure  challenge,  has been previously studied in order to develop some insights into biological control  systems  [1,  2,  3]. \n¬∑Jyndon.j .brown@usa.dupont.com \nAddress correspondence to this author \nGregory.E.Gonye-PHD@usa.dupont.com James.S.Scwhaber@usa.dupont.com \n976 \nL. J.  Brown,  G. E. Gonye and J.  S.  Schwaber \nNeurons  exhibit  complex  dynamic  behavior  that  is  not  directly  revealed  by  their  electrical  behavior,  but  is  incorporated  in  biochemical  signal  transduction  path(cid:173) ways.  This is  an important basis for  plasticity of neural  networks.  The area of the  brain to which  the baroreceptor afferents  project is  the nucleus of tractus solitarus  (NTS).  The neurons in the NTS  are  rich  with  diverse receptors for  signaling path(cid:173) ways.  It  is  logical  that  this  richness  and  diversity  playa crucial  role  in  the signal  processing  that  occurs  here.  Hormonal  and  neurotransmitter signals  can  activate  signal  transduction  pathways  in  the  cell,  which  result  in  physical  modification  of  some components of a cell, or altered gene regulation.  Fuxe et al  [4]  have shown the  presence of the angiotensin 11/ AT!  receptor pathway in NTS  neurons, and Herbert  [5]  has demonstrated its ability to affect the baroreflex. \nTo  develop understanding of the effects of biochemical pathways,  a detailed kinetic  model  of the  angiotensin/AT!  pathway  was  developed.  Certain  features  of  this  model and the baroreflex have interesting characteristics from  a control engineering  perspective.  These  features  have  been  used  to  develop  a  novel  control  strategy.  The resulting control algorithm utilizes a proportional controller that intermittently  invokes  integral  action  to  achieve  set-point  matching.  Thus  the  controller  will  be  labeled PII. \nThe  use  of integral  control  is  popular  as  it  guarantees  cancellation  of offsets  and  ensures  exact  set-point  matching.  However,  the  use  of integral  control  does  have  drawbacks.  It  introduces  significant  lag  in  the  feedback  system,  which  limits  the  bandwidth of the system.  Increasing the integral gain, in order to improve response  time,  can  lead  to  systems  with  excessive  overshoot,  excessive  settling  times,  and  less  robustness  to  plant  changes  or  uncertainty.  Many  processes  in  the  chemical  industry  have  a  steady-state  response  curve  with  a  maximum  and  frequently,  the  optimal operating condition is  at this peak.  Unfortunately,  any controller with true  integral action will  be unstable at this operating point. \nIn a crude sense, the integrator learns the constant control action required to achieve  set-point matching.  If the integral control is viewed as a simple learning device, than  a  logical  step  is  to remove it from  the feedback  loop  once  the  necessary  offset  has  been learned.  If the offset is  being successfully compensated for,  only noise remains  as a source for  learning.  It has been well established that learning based on nothing  but  noise  leads  to undesirable  results.  The maxim,  'garbage in,  garbage out'  will  apply.  Without integral control,  the  proportional controller  can be made more  ag(cid:173) gressive while maintaining stability margins and/or control actions at similar levels.  This control strategy will  be appropriate for  plants with infrequent step changes in  set-points or loads.  The challenge becomes deciding when,  and how  to perform this  switching so  that the resulting controller provides significant improvements. \n1  Angiotensin III ATI receptor  Signal Transduction Model \nRegulation of blood pressure is  a vital control problem in mammals.  Blood pressure  is  sensed by stretch sensitive  cells  in  the aortic  arch and carotid sinus.  These cells  transmit signals to neurons in the NTS which  are combined with other signals from  the  central  nervous  system  (CNS)  resulting  in  changes  to the  cardiac output  and  vascular tone [6].  This control is  implemented by two  parallel systems in  the CNS,  the  sympathetic  and  parasympathetic  nervous  systems.  The  sympathetic  system  primarily affects  the vascular tone  and the parasympathetic system affects  cardiac  output  [7].  Cardiac  control  can  have  a  larger  and  faster  effect,  but  long  term  application of this control is  injurious to the overall health of the animal.  Pottman  et  al  [2]  have  suggested  that  these  two  systems  separately  control  for  long  term  set-point control and fast  disturbance rejection. \nNon-Linear PI Control Inspired by Biological Control Systems \n977 \nOne  receptor  in  NTS  neuronal  cells  is  the  AT1  receptor  which  binds  Angiotensin  II.  The  NTS  is  located in  the brain stem where  much of the  processing of the au(cid:173) tonomic regulatory systems reside.  Angiotensin infusion in this region of the brain  has  been  shown  to  significantly  affect  blood  pressure  control.  In  order  to  under(cid:173) stand  this  aspect  of neuronal  behavior,  a  detailed  kinetic  model  of this  signaling  pathway  was  developed.  The  pathway is  presented  in  Figure  2.  The outputs  can  be  considered  to  be  the  concentrations  of Gq¬∑GTP,  GO-y,  activated  protein  kinase  C,  and/or calmodulin dependent  protein kinase. \nSeveral reactions  in  the cascade are of interest.  The binding of phospholipase C  is  significantly  slower  than  the  other  steps  in  the  reaction.  This  can  be  modeled  as  a  first  order  transfer  function  with  a  long  time  constant  or  as  a  pure  integrator.  The  IP3 receptor  is  a  ligand  gated  channel  on  the  membrane  of the  endoplasmic  reticulum  (ER).  As  Figure  2  shows,  when  IP3  binds  to  this  receptor,  calcium  is  released  from  the  ER  into  the  cells  cytoplasm.  However  the  IP3  receptor  also  has  2  binding  sites  on  its  cytoplasmic  domain  for  binding  calcium.  The  first  has  relatively  fast  dynamics  and  causes  a  substantial  increase  in  the  channel  opening.  The second calcium  binding site has slower  dynamics  and inactivates  the  channel.  The effect of this first  binding site is to introduce positive feedback into the model.  In traditional control literature, positive feedback is  generally undesirable.  Thus it  is  very interesting to see  positive feedback in neuronal  control systems. \nA typical surface response for  the model,  comparing the time response of activated  calmodulin  versus  the  peak  concentration  of  a  pulse  of  angiotensin,  is  shown  in  Figure  1.  The  results  are  consistent  with  behavior  of  cells  measured  by  Li  and  Guyenet  [8].  The  output  level  is  seen  to  abruptly  rise  after  a  delay,  which  is  a  decreasing function of the magnitude of the input.  Unlike a linear system, both the  magnitude and speed of the response of the system are functions  of the magnitude  of the  input.  Further,  the  relaxing of the system to  its  equilibrium  is  a  very slow  response  as  compared  to  its  activation.  This  behavior  can  be  attributed  to  the  positive  feedback  response  inherent  to  the  IP3  receptor.  The  effect  of  the  slow  dynamics of the phospholipase C binding,  and the IP3 receptor dynamics results in  an activation behavior similar to a threshold detector on the integrated input signal.  However, removal of the input results in a slow recovery back to zero.  The activation  of the calcium calmodulin dependent protein kinase  can lead to phosphorilation of  channels  that result  in  synaptic conductance changes that are functionally  related  to  the  amount  of activated  kinase.  The  activation  of calcium  calmodulin  can  also  lead to changes in gene regulation that could potentially result in long term changes  in  the neurons  synaptic conductances. \n2  Proportional with Intermittent Integral  Control \nKey features  from  the model  that are incorporated in  the control  law  are: \n\nseparate controllers for  set-point control and disturbance attenuation;  2.  activation of set-point controller when integrated error exceeds threshold;  3.  strength of integral  action  when  activated  will  be  a  function  of the  speed \n\nwith  which  activation was  achieved; \n\nsmooth removal of integral action,  without disruption of control action. \n\nThe PII controller begins initially as a proportional controller with a nominal offset  added  to  its  output.  The  integrated  error  is  monitored.  The  integral  controller  is  turned  on  when  the  integrated  error  exceeds  a  threshold.  Once  the  integral  control action is  activated, it remains active as  long as the error is  excessive.  Once  the  error  is  not  significant,  then  the  integral  control  action  can  be  removed  in  a \n978 \nL. J.  Brown,  G.  E.  Gonye and J.  S.  Schwaber",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/9e984c108157cea74c894b5cf34efc44-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/9e984c108157cea74c894b5cf34efc44-Bibtex.bib",
            "SUPP": ""
        }
    },
    "97": {
        "TITLE": "Linear Hinge Loss and Average Margin",
        "AUTHORS": "Claudio Gentile, Manfred K. Warmuth",
        "ABSTRACT": "We  describe a unifying method for proving relative loss  bounds for on(cid:173) line linear threshold classification algorithms, such as the Perceptron and  the Winnow algorithms.  For classification problems the discrete loss  is  used,  i.e., the total  number of prediction mistakes.  We  introduce a con(cid:173) tinuous loss function, called the \"linear hinge loss\", that can be employed  to derive the updates of the algorithms.  We  first  prove bounds w.r.t.  the  linear hinge loss  and  then  convert them  to  the  discrete  loss.  We  intro(cid:173) duce a notion of \"average margin\" of a set of examples .  We  show how  relative loss  bounds based on the  linear hinge  loss can  be converted to  relative loss bounds i.t.o.  the discrete loss using the average margin.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/a14ac55a4f27472c5d894ec1c3c743d2-Bibtex.bib",
            "SUPP": ""
        }
    },
    "98": {
        "TITLE": "Learning Instance-Independent Value Functions to Enhance Local Search",
        "AUTHORS": "Robert Moll, Andrew G. Barto, Theodore J. Perkins, Richard S. Sutton",
        "ABSTRACT": "Reinforcement learning methods can be used to improve the performance  of local  search  algorithms  for  combinatorial  optimization  by  learning  an  evaluation  function  that  predicts  the  outcome  of search.  The  eval(cid:173) uation  function  is  therefore  able  to  guide  search  to  low-cost  solutions  better than  can  the original  cost function.  We  describe a  reinforcement  learning method for enhancing local search that combines aspects of pre(cid:173) vious work by Zhang and Dietterich (1995) and Boyan and Moore (1997,  Boyan  1998).  In  an  off-line  learning  phase,  a  value function  is  learned  that is  useful for guiding search for multiple problem sizes and instances.  We illustrate our technique by  developing several  such functions for the  Dial-A-Ride Problem.  Our learning-enhanced local  search algorithm ex(cid:173) hibits  an  improvement of more  then  30%  over  a  standard  local  search  algorithm.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/a1afc58c6ca9540d057299ec3016d726-Bibtex.bib",
            "SUPP": ""
        }
    },
    "99": {
        "TITLE": "Support Vector Machines Applied to Face Recognition",
        "AUTHORS": "P. Jonathon Phillips",
        "ABSTRACT": "Face recognition is a K  class problem. where K  is the number of known  individuals;  and  support vector machines  (SVMs)  are a binary  classi(cid:173) fication method. By reformulating the face recognition problem and re(cid:173) interpreting the output of the SVM classifier. we developed a SVM -based  face recognition algorithm.  The face recognition problem is formulated  as a problem in difference space.  which models dissimilarities between  two facial images. In difference space we formulate face recognition as a  two class problem.  The classes are:  dissimilarities between faces  of the  same person.  and dissimilarities between faces  of different people.  By  modifying the interpretation of the decision surface generated by SVM.  we generated a similarity metric between faces  that is learned from ex(cid:173) amples of differences between faces.  The SVM-based algorithm is com(cid:173) pared with a principal component analysis (PeA) based algorithm on a  difficult set of images from the FEREf database. Performance was mea(cid:173) sured for both verification and identification scenarios. The identification  performance for SVM is 77-78% versus 54% for PCA. For verification.  the equal error rate is 7% for SVM and 13 % for PCA.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/a2cc63e065705fe938a4dda49092966f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/a2cc63e065705fe938a4dda49092966f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "100": {
        "TITLE": "A Theory of Mean Field Approximation",
        "AUTHORS": "Toshiyuki Tanaka",
        "ABSTRACT": "I present a theory of mean field  approximation based on information ge(cid:173) ometry.  This  theory  includes  in  a  consistent way  the  naive  mean  field  approximation, as well as the TAP approach and the  linear response the(cid:173) orem  in  statistical physics, giving clear information-theoretic interpreta(cid:173) tions to them.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/a368b0de8b91cfb3f91892fbf1ebd4b2-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/a368b0de8b91cfb3f91892fbf1ebd4b2-Bibtex.bib",
            "SUPP": ""
        }
    },
    "101": {
        "TITLE": "Unsupervised and Supervised Clustering: The Mutual Information between Parameters and Observations",
        "AUTHORS": "Didier Herschkowitz, Jean-Pierre Nadal",
        "ABSTRACT": "Recent  works  in  parameter  estimation  and  neural  coding  have  demonstrated that optimal performance are related to the  mutual  information between parameters and data.  We  consider the mutual  information in  the case where  the dependency in the parameter (a  vector  8)  of  the  conditional  p.d.f.  of each  observation  (a  vector  0, is  through  the  scalar  product  8.~ only.  We  derive  bounds  and  asymptotic behaviour for  the mutual information and compare with  results  obtained on  the same model with the\" replica technique\" .",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/a981f2b708044d6fb4a71a1463242520-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/a981f2b708044d6fb4a71a1463242520-Bibtex.bib",
            "SUPP": ""
        }
    },
    "102": {
        "TITLE": "Stationarity and Stability of Autoregressive Neural Network Processes",
        "AUTHORS": "Friedrich Leisch, Adrian Trapletti, Kurt Hornik",
        "ABSTRACT": "We  analyze  the  asymptotic behavior of autoregressive  neural  net(cid:173) work  (AR-NN)  processes  using techniques from Markov chains and  non-linear time series  analysis.  It is  shown  that standard AR-NNs  without shortcut connections are  asymptotically stationary.  If lin(cid:173) ear  shortcut  connections  are  allowed,  only  the  shortcut  weights  determine whether the overall system is stationary, hence standard  conditions for  linear AR processes  can be  used.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/aa486f25175cbdc3854151288a645c19-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/aa486f25175cbdc3854151288a645c19-Bibtex.bib",
            "SUPP": ""
        }
    },
    "103": {
        "TITLE": "A Principle for Unsupervised Hierarchical Decomposition of Visual Scenes",
        "AUTHORS": "Michael Mozer",
        "ABSTRACT": "Structure in  a visual  scene can  be  described at many levels of granular(cid:173)\nity.  At a coarse level,  the  scene is  composed of objects;  at  a finer level, \neach object is made up of parts, and the parts of subparts. In this  work, I \npropose a simple principle by  which  such hierarchical  structure can  be \nextracted from  visual scenes: Regularity in the relations among different \nparts of an  object is  weaker than in  the  internal  structure of a part. This \nprinciple  can  be  applied  recursively  to  define  part-whole  relationships \namong elements  in  a scene.  The principle does  not make use  of object \nmodels,  categories,  or  other  sorts  of  higher-level  knowledge;  rather, \npart-whole relationships  can  be  established based  on  the  statistics  of a \nset of sample visual  scenes. I illustrate with a model that performs unsu(cid:173)\npervised  decomposition  of simple  scenes.  The  model  can  account  for \nthe  results from  a human  learning experiment on  the  ontogeny  of part(cid:173)\nwhole relationships.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/ab541d874c7bc19ab77642849e02b89f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/ab541d874c7bc19ab77642849e02b89f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "104": {
        "TITLE": "Gradient Descent for General Reinforcement Learning",
        "AUTHORS": "Leemon C. Baird III, Andrew W. Moore",
        "ABSTRACT": "A  simple  learning  rule  is  derived,  the  VAPS  algorithm,  which  can  be  instantiated  to  generate  a  wide  range  of  new  reinforcement(cid:173) learning  algorithms.  These  algorithms  solve  a  number  of  open  problems, define several new approaches to reinforcement learning,  and  unify  different  approaches  to  reinforcement  learning  under  a  single  theory.  These  algorithms  all  have  guaranteed  convergence,  and  include  modifications  of several  existing  algorithms  that  were  known  to  fail  to  converge  on  simple  MOPs.  These  include  Q(cid:173) In  addition  to  these  learning,  SARSA,  and  advantage  learning.  it  also  generates  pure  policy-search  value-based  algorithms  reinforcement-learning  algorithms,  which  learn  optimal  policies  without  learning  a  value  function.  search  and  value-based  algorithms  to  be  combined,  thus  unifying  two  very  different  approaches  to  reinforcement  learning  into  a  single  Value  and  Policy  Search  (V APS)  algorithm.  And  these  algorithms converge for  POMDPs without requiring a  proper belief  state .  Simulations  results  are  given,  and  several  areas  for  future  research are discussed. \nIn  addition,  it  allows  policy(cid:173)\n1  CONVERGENCE  OF  GREEDY  EXPLORATION \nMany  reinforcement-learning  algorithms  are  known  that  use  a  parameterized  function  approximator  to  represent  a  value  function,  and  adjust  the  weights  include  Q-learning,  SARSA,  and  incrementally  during  advantage  learning.  There  are  simple  MOPs  where  the  original  form  of  these  algorithms  fails  to  converge,  as  summarized  in  Table  1.  For the  cases  with..J,  the  algorithms  are  guaranteed  to  converge  under  reasonable  assumptions  such  as",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/af5afd7f7c807171981d443ad4f4f648-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/af5afd7f7c807171981d443ad4f4f648-Bibtex.bib",
            "SUPP": ""
        }
    },
    "105": {
        "TITLE": "Vertex Identification in High Energy Physics Experiments",
        "AUTHORS": "Gideon Dror, Halina Abramowicz, David Horn",
        "ABSTRACT": "In High Energy Physics experiments one has to sort through a high  flux  of events, at a  rate of tens of MHz,  and select  the few  that are  of interest.  One  of the  key  factors  in  making  this  decision  is  the  location of the vertex  where  the  interaction , that led  to the event ,  took  place.  Here  we  present  a  novel  solution  to  the  problem  of  finding  the  location  of the  vertex,  based  on  two  feedforward  neu(cid:173) ral  networks with fixed  architectures, whose parameters are chosen  so  as  to  obtain  a  high  accuracy.  The  system  is  tested  on  simu(cid:173) lated  data sets , and is  shown  to  perform better  than  conventional  algorithms.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/aff0a6a4521232970b2c1cf539ad0a19-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/aff0a6a4521232970b2c1cf539ad0a19-Bibtex.bib",
            "SUPP": ""
        }
    },
    "106": {
        "TITLE": "Probabilistic Visualisation of High-Dimensional Binary Data",
        "AUTHORS": "Michael E. Tipping",
        "ABSTRACT": "We  present a  probabilistic latent-variable framework for  data visu(cid:173) alisation,  a  key  feature  of which  is  its  applicability  to  binary and  categorical data types for  which few  established  methods exist.  A  variational approximation to the likelihood is exploited to derive a  fast  algorithm for  determining the model parameters.  Illustrations  of application to real and synthetic binary data sets are given.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/b132ecc1609bfcf302615847c1caa69a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/b132ecc1609bfcf302615847c1caa69a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "107": {
        "TITLE": "Computation of Smooth Optical Flow in a Feedback Connected Analog Network",
        "AUTHORS": "Alan Stocker, Rodney J. Douglas",
        "ABSTRACT": "In  1986, Tanner and Mead [1] implemented an interesting constraint sat(cid:173) isfaction  circuit  for  global  motion  sensing  in  a VLSI.  We  report  here  a  new  and  improved a VLSI implementation that provides smooth optical  flow as well as global motion in a two dimensional visual field.  The com(cid:173) putation of optical flow  is  an ill-posed problem, which expresses itself as  the aperture problem.  However, the optical flow  can be estimated by the  use of regularization methods, in  which additional constraints are intro(cid:173) duced in  terms of a global energy functional that must be minimized . We  show how the algorithmic constraints of Hom and Schunck [2]  on com(cid:173) puting smooth optical flow can be mapped onto the physical constraints  of an equivalent electronic network.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/b2dd140336c9df867c087a29b2e66034-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/b2dd140336c9df867c087a29b2e66034-Bibtex.bib",
            "SUPP": ""
        }
    },
    "108": {
        "TITLE": "Applications of Multi-Resolution Neural Networks to Mammography",
        "AUTHORS": "Clay Spence, Paul Sajda",
        "ABSTRACT": "We  have  previously  presented  a  coarse-to-fine  hierarchical  pyra(cid:173) mid/neural  network  (HPNN)  architecture  which  combines  multi(cid:173) scale  image  processing  techniques  with  neural  networks.  In  this  paper  we  present  applications  of  this  general  architecture  to  two  problems  in  mammographic  Computer-Aided  Diagnosis  (CAD).  The  first  application  is  the  detection  of  microcalcifications.  The  <:oarse-to-fine  HPNN  was  designed  to learn  large-scale  context  in(cid:173) formation  for  detecting  small objects  like  microcalcifications.  Re(cid:173) ceiver  operating  characteristic  (ROC)  analysis  suggests  that  the  hierarchical  architecture  improves  detection  performance  of a  well  established  CAD  system by  roughly  50 %.  The second  application  is  to detect mammographic masses directly.  Since masses are large,  extended objects,  the coarse-to-fine HPNN architecture is  not suit(cid:173) able for  this  problem.  Instead we  construct  a fine-to-coarse  HPNN  architecture which  is  designed  to  learn  small-scale detail  structure  associated  with  the extended  objects.  Our initial  results  applying  the  fine-to-coarse  HPNN  to  mass  detection  are  encouraging,  with  detection  performance  improvements  of about  36 %.  We  conclude  that the ability of the HPNN architecture to integrate information  across  scales,  both  coarse-to-fine  and  fine-to-coarse,  makes  it  well  suited  for  detecting  objects  which  may  have  contextual  clues  or  detail structure occurring at scales  other than  the  natural scale of  the object.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/b5488aeff42889188d03c9895255cecc-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/b5488aeff42889188d03c9895255cecc-Bibtex.bib",
            "SUPP": ""
        }
    },
    "109": {
        "TITLE": "Divisive Normalization, Line Attractor Networks and Ideal Observers",
        "AUTHORS": "Sophie Den√®ve, Alexandre Pouget, Peter E. Latham",
        "ABSTRACT": "Gain  control  by  divisive  inhibition,  a.k.a.  divisive  normalization,  has  been  proposed  to  be  a  general  mechanism  throughout  the  vi(cid:173) sual  cortex.  We  explore  in  this  study  the  statistical  properties  of this  normalization in  the  presence  of noise.  Using  simulations,  we  show  that divisive  normalization is  a  close  approximation to  a  maximum likelihood estimator, which, in the context of population  coding, is the same as an ideal observer.  We also demonstrate ana(cid:173) lytically that this is  a general property of a  large class of nonlinear  recurrent  networks  with  line  attractors.  Our  work  suggests  that  divisive  normalization  plays  a  critical  role  in  noise  filtering,  and  that every cortical layer may be an ideal observer of the activity in  the preceding  layer. \nInformation  processing  in  the  cortex  is  often  formalized  as  a  sequence  of a  linear  stages followed  by  a  nonlinearity.  In the visual cortex,  the nonlinearity is  best de(cid:173) scribed by squaring combined with a divisive pooling of local activities.  The divisive  part of the nonlinearity has  been  extensively studied by  Heeger  and colleagues  [1],  and several authors have explored the role of this normalization in the computation  of high  order visual features  such  as  orientation of edges or first  and  second order  motion[ 4].  We show in this paper that divisive normalization can also playa role in  noise filtering.  More specifically, we demonstrate through simulations that networks  implementing this normalization come close to performing maximum likelihood es(cid:173) timation.  We  then demonstrate analytically that the ability  to  perform maximum  likelihood estimation, and thus efficiently extract information from  a  population of  noisy neurons,  is  a  property exhibited  by  a  large class of networks. \nMaximum  likelihood  estimation  is  a  framework  commonly  used  in  the  theory  of  ideal observers.  A recent example comes from the work of Itti et al.,  1998, who have  shown  that it  is  possible  to  account  for  the behavior of human  subjects  in simple  discrimination  tasks.  Their  model  comprised  two  distinct  stages:  1)  a  network \nDivisive Normalization.  Line Attractor Networks and Ideal Observers \n105 \nwhich  models  the noisy  response of neurons  with  tuning  curves to  orientation and  spatial frequency  combined with divisive normalization, and 2)  an ideal observer (a  maximum likelihood estimator)  to read out the population activity of the network. \nOur  work  suggests  that  there  is  no  need  to  distinguish  between  these  two  stages,  since,  as  we  will show,  divisive normalization comes close to  providing a  maximum  likelihood estimation.  More generally,  we  propose that there may  not  be any  part  of the cortex that acts as  an  ideal observer for  patterns of activity in sensory areas  but, instead, that each cortical layer acts as an ideal observer of the activity in  the  preceding layer.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/b5a1fc2085986034e448d2ccc5bb9703-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/b5a1fc2085986034e448d2ccc5bb9703-Bibtex.bib",
            "SUPP": ""
        }
    },
    "110": {
        "TITLE": "Robot Docking Using Mixtures of Gaussians",
        "AUTHORS": "Matthew M. Williamson, Roderick Murray-Smith, Volker Hansen",
        "ABSTRACT": "This  paper applies  the  Mixture  of Gaussians  probabilistic model,  com(cid:173) bined  with  Expectation Maximization  optimization  to  the  task  of sum(cid:173) marizing three dimensional range data for a mobile robot.  This provides  a flexible way of dealing with uncertainties in sensor information, and al(cid:173) lows the introduction of prior knowledge into low-level perception mod(cid:173) ules.  Problems with the basic approach were solved in  several ways:  the  mixture of Gaussians was reparameterized to reflect the types of objects  expected  in  the  scene,  and  priors  on  model  parameters  were  included  in  the  optimization  process.  Both approaches force  the  optimization  to  find  'interesting'  objects,  given the  sensor and object characteristics.  A  higher level  classifier  was  used  to  interpret the  results  provided by  the  model, and to reject spurious solutions.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/b60c5ab647a27045b462934977ccad9a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/b60c5ab647a27045b462934977ccad9a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "111": {
        "TITLE": "Semi-Supervised Support Vector Machines",
        "AUTHORS": "Kristin P. Bennett, Ayhan Demiriz",
        "ABSTRACT": "We  introduce  a  semi-supervised  support  vector  machine  (S3yM)  method.  Given  a  training  set  of labeled  data  and  a  working  set  of unlabeled  data,  S3YM  constructs  a  support vector  machine us(cid:173) ing  both  the  training  and  working  sets.  We  use  S3 YM  to  solve  the  transduction  problem  using  overall  risk  minimization  (ORM)  posed  by  Yapnik.  The  transduction  problem  is  to  estimate  the  value of a  classification function at the given points in the working  set.  This  contrasts  with  the  standard inductive  learning  problem  of estimating the  classification  function  at  all  possible values  and  then  using  the  fixed  function  to  deduce  the classes  of the working  set  data.  We  propose  a  general  S3YM  model  that minimizes both  the  misclassification  error  and  the  function  capacity  based  on  all  the available data.  We show  how the S3YM  model  for  I-norm lin(cid:173) ear  support  vector  machines  can  be  converted  to  a  mixed-integer  program  and then solved exactly using  integer  programming.  Re(cid:173) sults  of S3YM  and  the  standard  I-norm  support  vector  machine  approach  are  compared  on  ten  data sets.  Our  computational  re(cid:173) sults  support  the statistical  learning  theory  results  showing  that  incorporating  working  data  improves  generalization  when  insuffi(cid:173) cient  training information is  available.  In every case,  S3YM  either  improved or showed no significant difference in generalization com(cid:173) pared to the traditional approach. \nSemi-Supervised Support  Vector Machines",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/b710915795b9e9c02cf10d6d2bdb688c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/b710915795b9e9c02cf10d6d2bdb688c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "112": {
        "TITLE": "A Phase Space Approach to Minimax Entropy Learning and the Minutemax Approximations",
        "AUTHORS": "James M. Coughlan, Alan L. Yuille",
        "ABSTRACT": "There  has  been  much  recent  work  on  measuring  image  statistics  and  on  learning  probability  distributions  on  images.  We  observe  that  the  mapping  from  images  to  statistics  is  many-to-one  and  show  it  can  be  quantified  by  a  phase  space  factor.  This  phase  space approach throws light on the Minimax Entropy technique for  learning Gibbs distributions on images with potentials derived from  image statistics and elucidates the ambiguities that are inherent to  determining the potentials.  In  addition, it shows that if the phase  factor  can  be  approximated  by  an  analytic  distribution  then  this  approximation  yields  a  swift  \"Minutemax\"  algorithm  that  vastly  reduces  the  computation  time  for  Minimax entropy  learning.  An  illustration  of this  concept,  using  a  Gaussian  to  approximate  the  phase  factor,  gives  a  good  approximation  to  the  results  of  Zhu  and  Mumford  (1997)  in  just  seconds  of  CPU  time.  The  phase  space  approach  also  gives  insight  into  the  multi-scale  potentials  found  by  Zhu and Mumford  (1997)  and suggests that the forms  of  the potentials are influenced greatly by phase space considerations.  Finally,  we  prove  that probability distributions  learned  in  feature  space  alone  are  equivalent  to  Minimax  Entropy  learning  with  a  multinomial approximation of the phase factor.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/bc573864331a9e42e4511de6f678aa83-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/bc573864331a9e42e4511de6f678aa83-Bibtex.bib",
            "SUPP": ""
        }
    },
    "113": {
        "TITLE": "Almost Linear VC Dimension Bounds for Piecewise Polynomial Networks",
        "AUTHORS": "Peter L. Bartlett, Vitaly Maiorov, Ron Meir",
        "ABSTRACT": "We  compute  upper  and  lower  bounds  on  the  VC  dimension  of  feedforward  networks  of  units  with  piecewise  polynomial  activa(cid:173) tion functions.  We  show that if the number of layers is  fixed,  then  the  VC  dimension  grows  as  W log W,  where  W  is  the  number of  parameters in the network.  This result stands in opposition to the  case  where  the  number of  layers  is  unbounded,  in  which  case  the  VC  dimension grows as W 2 ‚Ä¢",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/bc7316929fe1545bf0b98d114ee3ecb8-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/bc7316929fe1545bf0b98d114ee3ecb8-Bibtex.bib",
            "SUPP": ""
        }
    },
    "114": {
        "TITLE": "Bayesian Modeling of Facial Similarity",
        "AUTHORS": "Baback Moghaddam, Tony Jebara, Alex Pentland",
        "ABSTRACT": "In  previous work  [6, 9, 10],  we  advanced a  new technique for  direct  visual  matching  of  images  for  the  purposes  of  face  recognition  and  image  retrieval ,  using  a  probabilistic  measure  of  similarity  based  primarily  on  a  Bayesian  (MAP)  analysis  of image  differ(cid:173) ences,  leading  to  a  \"dual\"  basis  similar  to  eigenfaces  [13].  The  performance  advantage  of  this  probabilistic  matching  technique  over  standard  Euclidean  nearest-neighbor  eigenface  matching was  recently demonstrated using results from DARPA's 1996  \"FERET\"  face  recognition  competition, in  which  this  probabilistic matching  algorithm  was  found  to  be  the  top  performer.  We  have  further  developed  a  simple  method  of replacing  the  costly  com put ion  of  nonlinear  (online)  Bayesian  similarity  measures  by  the  relatively  inexpensive  computation  of  linear  (offline)  subspace  projections  and simple (online)  Euclidean norms, thus resulting in a significant  computational speed-up for  implementation with very  large image  databases  as typically encountered  in  real-world  applications.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/bcb41ccdc4363c6848a1d760f26c28a0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/bcb41ccdc4363c6848a1d760f26c28a0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "115": {
        "TITLE": "The Bias-Variance Tradeoff and the Randomized GACV",
        "AUTHORS": "Grace Wahba, Xiwu Lin, Fangyu Gao, Dong Xiang, Ronald Klein, Barbara Klein",
        "ABSTRACT": "We  propose a new in-sample cross validation based method (randomized  GACV) for choosing smoothing or bandwidth parameters that govern the  bias-variance or fit-complexity tradeoff in  'soft' classification.  Soft clas(cid:173) sification refers to  a  learning procedure which  estimates the probability  that an example with a given attribute vector is  in  class  1 vs  class O.  The  target  for  optimizing  the  the  tradeoff is  the  Kullback-Liebler distance  between  the  estimated  probability  distribution  and  the  'true'  probabil(cid:173) ity  distribution,  representing  knowledge of an  infinite  population.  The  method uses a randomized estimate of the trace of a Hessian and mimics  cross validation at the cost of a single relearning with perturbed outcome  data.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/bffc98347ee35b3ead06728d6f073c68-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/bffc98347ee35b3ead06728d6f073c68-Bibtex.bib",
            "SUPP": ""
        }
    },
    "116": {
        "TITLE": "The Role of Lateral Cortical Competition in Ocular Dominance Development",
        "AUTHORS": "Christian Piepenbrock, Klaus Obermayer",
        "ABSTRACT": "Lateral competition within a layer of neurons sharpens and localizes the  response to an input stimulus. Here, we investigate a model for the ac(cid:173) tivity dependent development of ocular dominance maps which allows  to vary the degree of lateral competition. For weak competition, it re(cid:173) sembles a correlation-based learning model and for strong competition,  it becomes a self-organizing map. Thus, in the regime of weak compe(cid:173) tition the receptive fields are shaped by the second order statistics of the  input patterns, whereas in the regime of strong competition, the higher  moments and \"features\" of the individual patterns become important.  When correlated localized stimuli from two eyes drive the cortical de(cid:173) velopment we find (i) that a topographic map and binocular, localized  receptive fields emerge when the degree of competition exceeds a critical  value and (ii) that receptive fields exhibit eye dominance beyond a sec(cid:173) ond critical value. For anti-correlated activity between the eyes, the sec(cid:173) ond order statistics drive the system to develop ocular dominance even  for weak competition, but no topography emerges. Topography is estab(cid:173) lished only beyond a critical degree of competition.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/c559da2ba967eb820766939a658022c8-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/c559da2ba967eb820766939a658022c8-Bibtex.bib",
            "SUPP": ""
        }
    },
    "117": {
        "TITLE": "General Bounds on Bayes Errors for Regression with Gaussian Processes",
        "AUTHORS": "Manfred Opper, Francesco Vivarelli",
        "ABSTRACT": "Based on a  simple convexity lemma, we  develop bounds for  differ(cid:173) ent types of Bayesian prediction errors for regression with Gaussian  processes.  The basic bounds are formulated for a fixed training set.  Simpler expressions are obtained for sampling from an input distri(cid:173) bution  which  equals  the  weight  function  of the covariance kernel,  yielding  asymptotically  tight  results.  The  results  are  compared  with numerical experiments.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/c7af0926b294e47e52e46cfebe173f20-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/c7af0926b294e47e52e46cfebe173f20-Bibtex.bib",
            "SUPP": ""
        }
    },
    "118": {
        "TITLE": "Bayesian PCA",
        "AUTHORS": "Christopher M. Bishop",
        "ABSTRACT": "The technique of principal component analysis (PCA) has recently been  expressed  as  the  maximum  likelihood  solution  for  a  generative  latent  variable  model.  In  this  paper  we  use  this  probabilistic  reformulation  as  the basis  for a  Bayesian treatment of PCA.  Our key  result  is  that  ef(cid:173) fective  dimensionality  of the  latent  space  (equivalent to  the  number of  retained principal components) can be determined automatically as  part  of the  Bayesian  inference  procedure.  An  important application  of this  framework  is  to  mixtures  of probabilistic  PCA  models,  in  which  each  component can determine its own effective complexity.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/c88d8d0a6097754525e02c2246d8d27f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "119": {
        "TITLE": "A Precise Characterization of the Class of Languages Recognized by Neural Nets under Gaussian and Other Common Noise Distributions",
        "AUTHORS": "Wolfgang Maass, Eduardo D. Sontag",
        "ABSTRACT": "We  consider recurrent analog  neural  nets  where each gate is  subject to  Gaussian noise, or any other common noise distribution whose probabil(cid:173) ity density function is nonzero on a large set.  We  show that many regular  languages cannot be  recognized by  networks of this  type,  for  example  the  language  {w  E  {O, I} * I w begins with O},  and  we  give  a  precise  characterization of those languages which can be recognized. This result  implies severe constraints on possibilities for constructing recurrent ana(cid:173) log neural  nets that are robust against realistic types of analog noise.  On  the other hand we present a method for constructing feed forward analog  neural nets that are robust with regard to analog noise of this type.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/cb8acb1dc9821bf74e6ca9068032d623-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/cb8acb1dc9821bf74e6ca9068032d623-Bibtex.bib",
            "SUPP": ""
        }
    },
    "120": {
        "TITLE": "General-Purpose Localization of Textured Image Regions",
        "AUTHORS": "Ruth Rosenholtz",
        "ABSTRACT": "We  suggest a working  definition  of texture:  Texture  is  stuff that is  more  compactly represented  by  its  statistics  than  by  specifying  the  configuration  of  its  parts.  This  definition  suggests  that  to  fmd  texture  we  look  for  outliers  to  the  local  statistics,  and  label  as  texture  the  regions  with  no  outliers.  We  present a  method,  based  upon  this  idea, for  labeling points  in  natural scenes  as  belonging to  texture  regions,  while  simultaneously  allowing  us  to  label  low(cid:173) level,  bottom-up  cues  for  visual  attention.  This  method  is  based  upon  recent  psychophysics  results  on  processing  of texture  and  popout. \n1  WHAT  IS  TEXTURE,  AND  WHY  DO  WE  WANT  TO  FIND  IT? \nIn  a  number  of problems  in  computer  VlSlon  and  image  processing,  one  must  distinguish  between  image  regions  that  correspond  to  objects  and  those  which  correspond to  texture,  and  perform different processing depending  upon  the  type  of  region.  Current  computer  vision  algorithms  assume  one  magically  knows  this  region  labeling.  But  what  is  texture?  We  have  the  notion  that texture  involves  a  pattern  that  is  somehow  homogeneous,  or  in  which  signal  changes  are  \"too  complex\"  to  describe,  so  that  aggregate  properties  must  be  used  instead  (Saund,  1998).  There is  by no means a firm  division between texture and  objects; rather, the  characterization often depends upon  the scale of interest (Saund, 1998). \n‚Ä¢ Email: rruth@parc.xerox.com \n818",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/cda72177eba360ff16b7f836e2754370-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/cda72177eba360ff16b7f836e2754370-Bibtex.bib",
            "SUPP": ""
        }
    },
    "121": {
        "TITLE": "Making Templates Rotationally Invariant. An Application to Rotated Digit Recognition",
        "AUTHORS": "Shumeet Baluja",
        "ABSTRACT": "This  paper  describes  a  simple  and  efficient  method  to  make  template-based  object classification invariant to in-plane rotations. The task is divided into two  parts:  orientation discrimination and classification. The key idea is  to perform  the  orientation  discrimination  before  the  classification.  This  can  be  accom(cid:173) plished by hypothesizing, in  turn, that the input image belongs to each class of  interest. The image can then be rotated to  maximize its  similarity to the train(cid:173) ing images in each class (these contain the prototype object in an upright orien(cid:173) tation). This process yields a set of images, at least one of which will have the  object  in  an  upright position.  The resulting  images  can  then be classified by  models  which  have been  trained  with  only  upright examples.  This  approach  has  been  successfully  applied  to  two  real-world  vision-based  tasks:  rotated  handwritten digit recognition and rotated face detection in cluttered scenes.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/ced556cd9f9c0c8315cfbe0744a3baf0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/ced556cd9f9c0c8315cfbe0744a3baf0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "122": {
        "TITLE": "Outcomes of the Equivalence of Adaptive Ridge with Least Absolute Shrinkage",
        "AUTHORS": "Yves Grandvalet, St√©phane Canu",
        "ABSTRACT": "Adaptive Ridge is a special form of Ridge regression, balancing the  quadratic penalization on each parameter of the model. It was shown to  be equivalent to Lasso (least absolute shrinkage and selection operator),  in the sense that both procedures produce the same estimate. Lasso can  thus be viewed as a particular quadratic penalizer.  From this observation, we derive a fixed point algorithm to compute the  Lasso solution. The analogy provides also a new hyper-parameter for tun(cid:173) ing effectively the model complexity. We finally present a series ofpossi(cid:173) ble extensions oflasso performing sparse regression in kernel smoothing,  additive modeling and neural net training.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/cfa5301358b9fcbe7aa45b1ceea088c6-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/cfa5301358b9fcbe7aa45b1ceea088c6-Bibtex.bib",
            "SUPP": ""
        }
    },
    "123": {
        "TITLE": "Bayesian Modeling of Human Concept Learning",
        "AUTHORS": "Joshua B. Tenenbaum",
        "ABSTRACT": "I consider the problem of learning concepts from small numbers of pos(cid:173) itive examples,  a feat  which humans perform routinely but which com(cid:173) puters  are  rarely  capable  of.  Bridging machine  learning  and  cognitive  science perspectives, I present both theoretical analysis and an empirical  study with human subjects for the simple task oflearning concepts corre(cid:173) sponding to axis-aligned rectangles in a multidimensional feature space.  Existing learning models, when applied to this task, cannot explain how  subjects generalize from only a few  examples of the concept.  I propose  a principled Bayesian model based on the assumption that the examples  are  a random sample from  the concept to be  learned.  The  model  gives  precise fits to human behavior on this simple task and provides qualitati ve  insights into more complex, realistic cases of concept learning.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/d010396ca8abf6ead8cacc2c2f2f26c7-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/d010396ca8abf6ead8cacc2c2f2f26c7-Bibtex.bib",
            "SUPP": ""
        }
    },
    "124": {
        "TITLE": "Maximum Conditional Likelihood via Bound Maximization and the CEM Algorithm",
        "AUTHORS": "Tony Jebara, Alex Pentland",
        "ABSTRACT": "We  present  the  CEM  (Conditional  Expectation  Maximi::ation)  al(cid:173) gorithm  as  an  extension  of  the  EM  (Expectation  M aximi::ation)  algorithm to conditional density estimation under  missing data.  A  bounding and maximization process is given to specifically optimize  conditional likelihood instead  of the  usual  joint likelihood.  We  ap(cid:173) ply  the  method  to  conditioned  mixture models  and  use  bounding  techniques  to  derive  the  model's  update  rules .  Monotonic  conver(cid:173) gence,  computational efficiency  and  regression  results  superior  to  EM  are  demonstrated.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/d1a69640d53a32a9fb13e93d1c8f3104-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/d1a69640d53a32a9fb13e93d1c8f3104-Bibtex.bib",
            "SUPP": ""
        }
    },
    "125": {
        "TITLE": "Source Separation as a By-Product of Regularization",
        "AUTHORS": "Sepp Hochreiter, J√ºrgen Schmidhuber",
        "ABSTRACT": "This  paper  reveals  a  previously  ignored  connection  between  two  important fields:  regularization and independent component anal(cid:173) ysis  (ICA).  We  show  that  at  least  one  representative  of a  broad  class  of algorithms  (regularizers  that  reduce  network complexity)  extracts independent  features  as  a  by-product.  This  algorithm  is  Flat  Minimum  Search  (FMS),  a  recent general method for  finding  low-complexity networks with high generalization capability.  FMS  works  by minimizing  both training error and required weight  pre(cid:173) cision.  According  to  our  theoretical  analysis  the  hidden  layer  of  an  FMS-trained  autoassociator attempts  at  coding  each  input  by  a  sparse  code  with  as  few  simple  features  as  possible.  In experi(cid:173) ments  the  method  extracts optimal  codes  for  difficult  versions  of  the  \"noisy bars\"  benchmark problem by separating the underlying  sources,  whereas ICA and PCA fail.  Real world images are coded  with fewer  bits per pixel than by ICA or PCA.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/d1dc3a8270a6f9394f88847d7f0050cf-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/d1dc3a8270a6f9394f88847d7f0050cf-Bibtex.bib",
            "SUPP": ""
        }
    },
    "126": {
        "TITLE": "Analog VLSI Cellular Implementation of the Boundary Contour System",
        "AUTHORS": "Gert Cauwenberghs, James Waskiewicz",
        "ABSTRACT": "We  present an analog VLSI cellular architecture implementing a simpli(cid:173) . fied  version of the Boundary Contour System (BCS) for real-time image  processing.  Inspired by  neuromorphic models  across  several  layers  of  visual  cortex,  the  design  integrates  in  each pixel  the  functions  of sim(cid:173) ple cells,  complex cells,  hyper-complex cells,  and  bipole cells,  in  three  orientations interconnected on a  hexagonal grid.  Analog current-mode  CMOS circuits are used throughout to perform edge detection, local inhi(cid:173) bition, directionally selective long-range diffusive kernels, and renormal(cid:173) izing global gain control. Experimental results from a fabricated 12 x  10  pixel prototype in  1.2 J-tm  CMOS technology demonstrate the robustness  of the  architecture in  selecting image contours in  a cluttered and noisy  background.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/d63fbf8c3173730f82b150c5ef38b8ff-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/d63fbf8c3173730f82b150c5ef38b8ff-Bibtex.bib",
            "SUPP": ""
        }
    },
    "127": {
        "TITLE": "Unsupervised Classification with Non-Gaussian Mixture Models Using ICA",
        "AUTHORS": "Te-Won Lee, Michael S. Lewicki, Terrence J. Sejnowski",
        "ABSTRACT": "We  present  an  unsupervised  classification  algorithm  based  on  an  ICA  mixture  model.  The  ICA  mixture  model  assumes  that  the  observed  data can  be  categorized  into  several  mutually  exclusive  data classes  in  which  the components  in each  class  are generated  by  a  linear  mixture  of independent  sources.  The  algorithm  finds  the independent sources, the mixing matrix for  each class and also  computes  the  class  membership  probability  for  each  data  point.  This  approach  extends  the  Gaussian  mixture  model  so  that  the  classes  can  have  non-Gaussian  structure.  We  demonstrate  that  this method can learn efficient codes to represent images of natural  scenes and text.  The learned classes of basis functions yield a better  approximation of the underlying distributions of the data, and thus  can provide greater coding efficiency.  We  believe that this method  is  well  suited to  modeling structure in  high-dimensional  data and  has many potential applications. \n1",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/d72fbbccd9fe64c3a14f85d225a046f4-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/d72fbbccd9fe64c3a14f85d225a046f4-Bibtex.bib",
            "SUPP": ""
        }
    },
    "128": {
        "TITLE": "Barycentric Interpolators for Continuous Space and Time Reinforcement Learning",
        "AUTHORS": "R√©mi Munos, Andrew W. Moore",
        "ABSTRACT": "In  order  to find  the  optimal control of continuous state-space  and  time  reinforcement  learning  (RL)  problems,  we  approximate  the  value function  (VF)  with a  particular class  of functions  called  the  barycentric  interpolators.  We  establish sufficient  conditions  under  which  a  RL algorithm converges  to the optimal VF,  even  when  we  use  approximate models of the  state  dynamics  and  the  reinforce(cid:173) ment functions .",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/d961e9f236177d65d21100592edb0769-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/d961e9f236177d65d21100592edb0769-Bibtex.bib",
            "SUPP": ""
        }
    },
    "129": {
        "TITLE": "Exploiting Generative Models in Discriminative Classifiers",
        "AUTHORS": "Tommi Jaakkola, David Haussler",
        "ABSTRACT": "Generative probability models such as hidden  ~larkov models pro(cid:173) vide  a  principled  way of treating  missing  information  and  dealing  with variable  length sequences.  On the other hand , discriminative  methods  such  as  support  vector  machines  enable  us  to  construct  flexible  decision  boundaries  and  often  result  in  classification  per(cid:173) formance superior to that of the model based approaches.  An ideal  classifier should  combine these two  complementary approaches.  In  this  paper,  we  develop  a  natural  way  of  achieving  this  combina(cid:173) tion  by deriving kernel functions for  use in discriminative methods  such  as support vector  machines from  generative probability mod(cid:173) els.  We  provide a  theoretical justification for  this  combination  as  well as demonstrate a substantial improvement in  the classification  performance in  the context of D~A and  protein sequence analysis.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/db1915052d15f7815c8b88e879465a1e-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/db1915052d15f7815c8b88e879465a1e-Bibtex.bib",
            "SUPP": ""
        }
    },
    "130": {
        "TITLE": "Learning a Continuous Hidden Variable Model for Binary Data",
        "AUTHORS": "Daniel D. Lee, Haim Sompolinsky",
        "ABSTRACT": "A directed generative model for binary data using a  small number  of hidden  continuous  units  is  investigated.  A  clipping  nonlinear(cid:173) ity distinguishes the model from conventional principal components  analysis.  The relationships between the correlations of the underly(cid:173) ing continuous Gaussian variables  and the binary output variables  are utilized  to learn  the  appropriate weights  of the network.  The  advantages of this  approach are illustrated on a  translationally in(cid:173) variant binary distribution and on handwritten digit  images.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/dc5c768b5dc76a084531934b34601977-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/dc5c768b5dc76a084531934b34601977-Bibtex.bib",
            "SUPP": ""
        }
    },
    "131": {
        "TITLE": "Perceiving without Learning: From Spirals to Inside/Outside Relations",
        "AUTHORS": "Ke Chen, DeLiang L. Wang",
        "ABSTRACT": "As  a benchmark task,  the  spiral  problem  is  well  known  in  neural  net(cid:173) works.  Unlike  previous  work  that  emphasizes  learning,  we  approach  the  problem from  a generic perspective that does  not involve learning.  We  point out that the spiral problem is intrinsically connected to the in(cid:173) side/outside problem.  A  generic  solution  to  both problems is proposed  based on  oscillatory correlation using a time delay network.  Our simu(cid:173) lation results are qualitatively  consistent with  human performance,  and  we  interpret human limitations in  terms  of synchrony and  time  delays,  both biologically plausible.  As a special case, our network without time  delays can always distinguish these figures regardless of shape, position,  size, and orientation.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/dca5672ff3444c7e997aa9a2c4eb2094-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/dca5672ff3444c7e997aa9a2c4eb2094-Bibtex.bib",
            "SUPP": ""
        }
    },
    "132": {
        "TITLE": "Optimizing Classifers for Imbalanced Training Sets",
        "AUTHORS": "Grigoris I. Karakoulas, John Shawe-Taylor",
        "ABSTRACT": "Following  recent  results  [9,  8]  showing  the  importance of the  fat(cid:173) shattering  dimension  in  explaining  the  beneficial  effect  of a  large  margin  on  generalization  performance,  the  current  paper  investi(cid:173) gates  the  implications of these  results  for  the  case  of imbalanced  datasets  and  develops  two  approaches  to  setting  the  threshold.  The  approaches  are  incorporated  into ThetaBoost,  a  boosting  al(cid:173) gorithm for  dealing with  unequal  loss  functions.  The  performance  of ThetaBoost  and the  two approaches  are  tested  experimentally. \nKeywords:  Computational Learning Theory,  Generalization, fat-shattering,  large  margin, pac estimates,  unequal  loss,  imbalanced datasets",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/df12ecd077efc8c23881028604dbb8cc-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/df12ecd077efc8c23881028604dbb8cc-Bibtex.bib",
            "SUPP": ""
        }
    },
    "133": {
        "TITLE": "Blind Separation of Filtered Sources Using State-Space Approach",
        "AUTHORS": "Liqing Zhang, Andrzej Cichocki",
        "ABSTRACT": "In  this  paper  we  present  a  novel  approach  to  multichannel  blind  separation/generalized deconvolution,  assuming that  both mixing  and demixing models are described by stable linear state-space sys(cid:173) tems.  We  decompose  the  blind  separation  problem  into  two  pro(cid:173) cess:  separation and state estimation.  Based on  the minimization  of Kullback-Leibler  Divergence,  we  develop  a  novel  learning algo(cid:173) rithm to train the matrices in the output equation.  To estimate the  state  of the  demixing  model,  we  introduce  a  new  concept,  called  hidden  innovation,  to  numerically  implement  the  Kalman  filter.  Computer simulations are given  to show  the validity  and  high  ef(cid:173) fectiveness of the state-space approach. \n1",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/dfa92d8f817e5b08fcaafb50d03763cf-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/dfa92d8f817e5b08fcaafb50d03763cf-Bibtex.bib",
            "SUPP": ""
        }
    },
    "134": {
        "TITLE": "Recurrent Cortical Amplification Produces Complex Cell Responses",
        "AUTHORS": "Frances S. Chance, Sacha B. Nelson, L. F. Abbott",
        "ABSTRACT": "Cortical amplification has been proposed as a mechanism for enhancing  the selectivity of neurons in the primary visual  cortex.  Less appreciated  is the fact that the same form of amplification can also be used to de-tune  or  broaden selectivity.  Using  a  network  model  with  recurrent  cortical  circuitry,  we  propose  that the  spatial  phase  invariance  of complex  cell  responses  arises  through  recurrent  amplification  of feedforward  input.  Neurons in  the network respond  like simple cells at low  gain and com(cid:173) plex ceUs at high  gain.  Similar recurrent  mechanisms  may  playa role  in generating invariant representations of feedforward input elsewhere in  the visual processing pathway.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/e60e81c4cbe5171cd654662d9887aec2-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/e60e81c4cbe5171cd654662d9887aec2-Bibtex.bib",
            "SUPP": ""
        }
    },
    "135": {
        "TITLE": "Viewing Classifier Systems as Model Free Learning in POMDPs",
        "AUTHORS": "Akira Hayashi, Nobuo Suematsu",
        "ABSTRACT": "Classifier systems are now  viewed disappointing because of their prob(cid:173) lems such as the rule  strength vs rule  set performance problem and the  credit assignment problem.  In order to solve the problems, we have de(cid:173) veloped a hybrid classifier  system:  GLS  (Generalization Learning Sys(cid:173) tem).  In designing GLS, we view CSs as model free learning in POMDPs  and  take a  hybrid approach to finding  the  best generalization,  given the  total  number of rules.  GLS  uses  the policy improvement procedure by  Jaakkola  et al.  for  an locally  optimal  stochastic policy  when  a  set of  rule conditions is  given.  GLS  uses GA to search for  the best set of rule  conditions.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/e655c7716a4b3ea67f48c6322fc42ed6-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/e655c7716a4b3ea67f48c6322fc42ed6-Bibtex.bib",
            "SUPP": ""
        }
    },
    "136": {
        "TITLE": "Reinforcement Learning Based on On-Line EM Algorithm",
        "AUTHORS": "Masa-aki Sato, Shin Ishii",
        "ABSTRACT": "In  this  article,  we  propose  a  new  reinforcement  learning  (RL)  method  based  on  an  actor-critic  architecture.  The  actor  and  the  critic  are  approximated  by  Normalized  Gaussian  Networks  (NGnet),  which  are  networks  of local  linear regression  units.  The  NGnet is trained by the on-line EM algorithm proposed in our pre(cid:173) vious  paper.  We  apply our  RL  method  to the task of swinging-up  and stabilizing a  single pendulum and the task of balancing a  dou(cid:173) ble  pendulum near the upright  position.  The experimental results  show  that our RL  method can  be applied  to optimal control prob(cid:173) lems  having  continuous  state/action  spaces  and  that  the  method  achieves  good  control  with  a  small  number of trial-and-errors.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/e9fd7c2c6623306db59b6aef5c0d5cac-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/e9fd7c2c6623306db59b6aef5c0d5cac-Bibtex.bib",
            "SUPP": ""
        }
    },
    "137": {
        "TITLE": "Learning Multi-Class Dynamics",
        "AUTHORS": "Andrew Blake, Ben North, Michael Isard",
        "ABSTRACT": "Standard  techniques  (eg.  Yule-Walker)  are  available  for  learning  Auto-Regressive process models of simple,  directly observable,  dy(cid:173) namical  processes.  When  sensor  noise  means  that  dynamics  are  observed  only  approximately,  learning can  still  been  achieved  via  Expectation-Maximisation  (EM)  together  with  Kalman  Filtering.  However,  this  does  not  handle  more  complex  dynamics,  involving  multiple  classes  of motion.  For  that  problem,  we  show  here  how  EM  can  be combined  with  the  CONDENSATION  algorithm,  which  is  based on propagation of random sample-sets.  Experiments have  been  performed  with  visually  observed juggling,  and plausible dy(cid:173) namical models are found  to emerge from  the learning process.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/ebb71045453f38676c40deb9864f811d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/ebb71045453f38676c40deb9864f811d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "138": {
        "TITLE": "Markov Processes on Curves for Automatic Speech Recognition",
        "AUTHORS": "Lawrence K. Saul, Mazin G. Rahim",
        "ABSTRACT": "We  investigate  a  probabilistic  framework  for  automatic  speech  recognition  based  on  the  intrinsic  geometric  properties  of curves.  In  particular,  we  analyze  the  setting  in  which  two  variables-one  continuous  (~), one  discrete  (s )-evolve jointly in  time.  We  sup(cid:173) pose  that the vector ~ traces out a smooth multidimensional curve  and  that  the  variable  s  evolves  stochastically  as  a  function  of the  arc  length  traversed  along  this  curve.  Since  arc  length  does  not  depend  on  the  rate  at  which  a  curve  is  traversed,  this  gives  rise  to  a  family  of  Markov  processes  whose  predictions,  Pr[sl~]'  are  invariant  to  nonlinear  warpings  of time.  We  describe  the  use  of  such  models,  known  as  Markov  processes  on  curves  (MPCs),  for  automatic speech  recognition,  where  ~ are  acoustic feature  trajec(cid:173) tories and s are phonetic transcriptions.  On two tasks-recognizing  New  Jersey  town names and connected  alpha-digits- we  find  that  MPCs yield lower word error rates than comparably trained hidden  Markov  models.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "139": {
        "TITLE": "Discovering Hidden Features with Gaussian Processes Regression",
        "AUTHORS": "Francesco Vivarelli, Christopher K. I. Williams",
        "ABSTRACT": "We  study  the  dynamics  of supervised  learning  in  layered  neural  net(cid:173) works,  in  the regime where the size p of the training set is  proportional  to the number N  of inputs.  Here the local fields  are no longer described  by  Gaussian  distributions.  We  use  dynamical  replica theory  to  predict  the  evolution  of macroscopic  observables,  including  the  relevant  error  measures, incorporating the old formalism  in the limit piN --t  00.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/ed4227734ed75d343320b6a5fd16ce57-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/ed4227734ed75d343320b6a5fd16ce57-Bibtex.bib",
            "SUPP": ""
        }
    },
    "140": {
        "TITLE": "Utilizing lime: Asynchronous Binding",
        "AUTHORS": "Bradley C. Love",
        "ABSTRACT": "Historically,  connectionist  systems  have  not excelled  at  represent(cid:173) ing and manipulating complex structures.  How  can a  system com(cid:173) posed  of simple  neuron-like  computing  elements  encode  complex  relations?  Recently,  researchers have begun to appreciate that rep(cid:173) resentations can extend in  both time and space.  Many researchers  have proposed that the synchronous firing of units can encode com(cid:173) plex  representations.  I  identify  the  limitations  of  this  approach  and present an  asynchronous model of binding that effectively rep(cid:173) resents  complex  structures.  The  asynchronous model  extends  the  synchronous approach.  I argue that our cognitive architecture uti(cid:173) lizes  a similar mechanism.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/efb76cff97aaf057654ef2f38cd77d73-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/efb76cff97aaf057654ef2f38cd77d73-Bibtex.bib",
            "SUPP": ""
        }
    },
    "141": {
        "TITLE": "Very Fast EM-Based Mixture Model Clustering Using Multiresolution Kd-Trees",
        "AUTHORS": "Andrew W. Moore",
        "ABSTRACT": "Clust ering  is  impor ta nt  in  m any  fields  including  m anufac tlll'ing ,  biol og~',  fin ance , a nd astronomy.  l\\Iixturp  models arp a  popula r  ap(cid:173) proach  due  to  their  st.atist.ical  found a t.ions,  and  EM  is  a  very  pop(cid:173) ular  l1wthocl  for  fillding  mixture  models.  EM,  however,  requires  lllany accesses  of the dat a , a nd  thus h as  been  dismissed  as  imprac(cid:173) t ical  (e.g.  [9])  for  d ata mining of enormous dataset.s.  We  present  a  nt' \\¬∑  algorit.hm,  baspd  on  thp  l1lultiresolution  ~.'Cl- trees of [5] ,  which  dramatically reelucps  the cost  of EtlI-baspd  clusteriug , wit.h  savings  rising  linearl:;  wit.h  the number  of datapoints.  Although  prespnt.pd  lwre for  maximum likplihoocl estimation of Gaussian mixt.ure mod(cid:173) f'ls ,  it.  is  also  applicable  to  non-(~aussian models  (provided  class  densit.ies  are  monotonic in  Mahalanobis dist.ance), mixed  categori(cid:173) cal/ nUllwric  clusters.  anel  Bayesian  nwthocls  such  as  Antoclass  [1]. \n1  Learning Mixture Models  In  a  Gaussian mixture lllod f'l  (e.g.  [3]) , we  aSSUI1W  t.hat  d ata points {Xl  .. . XR}  ha\\'p  bef'n  gelw r\n. \nwhere  8  den otps  all  the  parameters of the  mixture:  the class  probabilities Vi  (wlwre  Vi  =  P(Cj  18)) , the class  centers  fl j  and  the  class covariances  ~j' \nTlw  job  of a  mixture  m odel  learn er  is  to  find  a  good  estimat e  of  t.he  modeL  and  Expectation  MaximizRtion  (EM) ,  also  known  a::l  \"Fuzzy  ~'-me a n::l\",  i::l  a  popular \n544",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/f187a23c3ee681ef6913f31fd6d6446b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/f187a23c3ee681ef6913f31fd6d6446b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "142": {
        "TITLE": "Tight Bounds for the VC-Dimension of Piecewise Polynomial Networks",
        "AUTHORS": "Akito Sakurai",
        "ABSTRACT": "O(ws(s log d+log(dqh/ s))) and O(ws((h/ s) log q) +log(dqh/ s)) are  upper bounds for  the VC-dimension of a  set of neural networks of  units  with  piecewise  polynomial  activation  functions,  where  s  is  the  depth  of  the  network,  h  is  the  number  of  hidden  units,  w  is  the  number  of  adjustable  parameters,  q  is  the  maximum  of  the  number of polynomial segments of the activation function, and d is  the  maximum degree  of  the polynomials;  also  n(wslog(dqh/s))  is  a  lower  bound  for  the VC-dimension  of such  a  network set,  which  are tight for  the cases  s =  8(h)  and  s is  constant.  For the special  case q =  1,  the VC-dimension is  8(ws log d).",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/f18a6d1cde4b205199de8729a6637b42-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/f18a6d1cde4b205199de8729a6637b42-Bibtex.bib",
            "SUPP": ""
        }
    },
    "143": {
        "TITLE": "The Effect of Eligibility Traces on Finding Optimal Memoryless Policies in Partially Observable Markov Decision Processes",
        "AUTHORS": "John Loch",
        "ABSTRACT": "Agents  acting  in  the  real  world  are  confronted  with  the  problem  of  making  good  decisions  with  limited  knowledge  of the  environment.  Partially  observable  Markov  decision  processes  (POMDPs)  model  decision problems in which an  agent tries to maximize its reward in  the  face  of limited sensor feedback.  Recent work has shown empirically that  a  reinforcement  learning  (RL)  algorithm  called  Sarsa(A)  can  efficiently  find  optimal  memoryless  policies,  which  map  current  observations  to  actions,  for  POMDP  problems  (Loch  and  Singh  1998).  The  Sarsa(A)  algorithm uses a form  of short-term memory  called an eligibility trace,  which  distributes  temporally  delayed  rewards  to  observation-action  pairs  which  lead  up  to  the  reward.  This  paper  explores  the  effect  of  eligibility traces on the ability of the Sarsa(A) algorithm to find  optimal  memoryless  policies.  A  variant  of  Sarsa(A)  called  k-step  truncated  Sarsa(A)  is  applied to four test  problems taken  from  the  recent  work of  Littman,  Littman,  Cassandra  and  Kaelbling,  Parr  and  Russell,  and  Chrisman.  The  empirical  results  show  that  eligibility  traces  can  be  significantly  truncated  without  affecting  the  ability  of Sarsa(A)  to  find  optimal memoryless policies for POMDPs.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/f3173935ed8ac4bf073c1bcd63171f8a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/f3173935ed8ac4bf073c1bcd63171f8a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "144": {
        "TITLE": "Exploratory Data Analysis Using Radial Basis Function Latent Variable Models",
        "AUTHORS": "Alan D. Marrs, Andrew R. Webb",
        "ABSTRACT": "Two  developments of nonlinear latent variable models  based  on  radial  basis functions are discussed:  in the first, the use of priors or constraints  on allowable models is considered as a means of preserving data structure  in  low-dimensional representations for  visualisation  purposes.  Also,  a  resampling approach  is  introduced which  makes  more  effective use  of  the latent samples in evaluating the likelihood.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/f337d999d9ad116a7b4f3d409fcc6480-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/f337d999d9ad116a7b4f3d409fcc6480-Bibtex.bib",
            "SUPP": ""
        }
    },
    "145": {
        "TITLE": "A V1 Model of Pop Out and Asymmetty in Visual Search",
        "AUTHORS": "Zhaoping Li",
        "ABSTRACT": "Visual  search is  the task of finding  a  target in  an image against  a  background of distractors.  Unique features  of targets enable them  to pop out against the background, while targets defined by lacks of  features or conjunctions of features are more difficult to spot.  It is  known that the ease of target detection can change when the roles  of figure  and  ground  are  switched.  The  mechanisms  underlying  the  ease  of  pop  out  and  asymmetry  in  visual  search  have  been  elusive.  This paper shows that a model of segmentation in VI based  on  intracortical  interactions  can  explain  many  of the  qualitative  aspects of visual search.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/f60bb6bb4c96d4df93c51bd69dcc15a0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/f60bb6bb4c96d4df93c51bd69dcc15a0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "146": {
        "TITLE": "A High Performance k-NN Classifier Using a Binary Correlation Matrix Memory",
        "AUTHORS": "Ping Zhou, Jim Austin, John Kennedy",
        "ABSTRACT": "This  paper  presents  a  novel  and  fast  k-NN  classifier  that  is  based  on  a  binary  CMM  (Correlation  Matrix  Memory)  neural  network.  A  robust  encoding  method  is  developed  to  meet  CMM  input  requirements .  A  hardware implementation of the CMM is  described, which gives  over 200  times  the  speed  of a  current  mid-range  workstation,  and  is  scaleable  to  very  large  problems.  When  tested  on  several  benchmarks  and  compared  with a simple k-NN method, the CMM classifier gave  less than  I %  lower  accuracy  and  over  4  and  12  times  speed-up  in  software  and  hardware  respectively.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/fa1e9c965314ccd7810fb5ea838303e5-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/fa1e9c965314ccd7810fb5ea838303e5-Bibtex.bib",
            "SUPP": ""
        }
    },
    "147": {
        "TITLE": "Exploring Unknown Environments with Real-Time Search or Reinforcement Learning",
        "AUTHORS": "Sven Koenig",
        "ABSTRACT": "Learning Real-Time A*  (LRTA*) is a popular control method that interleaves plan(cid:173) ning  and  plan  execution  and  has  been  shown  to  solve  search  problems  in  known  environments efficiently. In this paper, we apply LRTA * to the problem of getting to  a given goal location in an initially unknown environment.  Uninformed LRTA * with  maximal  lookahead always moves  on a  shortest path to the closest unvisited  state,  that is, to the closest potential goal state.  This was believed to be a good exploration  heuristic, but we show that it does not minimize the worst-case plan-execution time  compared to other uninformed exploration methods.  This result is also of interest to  reinforcement-learning researchers  since many  reinforcement learning methods use  asynchronous dynamic  programming,  interleave  planning and  plan  execution,  and  exhibit optimism in the face of uncertainty, just like LRTA *.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/faafda66202d234463057972460c04f5-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/faafda66202d234463057972460c04f5-Bibtex.bib",
            "SUPP": ""
        }
    },
    "148": {
        "TITLE": "Inference in Multilayer Networks via Large Deviation Bounds",
        "AUTHORS": "Michael J. Kearns, Lawrence K. Saul",
        "ABSTRACT": "We  study  probabilistic  inference  in  large,  layered  Bayesian  net(cid:173) works  represented  as  directed  acyclic  graphs.  We  show  that  the  intractability of exact inference in such  networks does  not preclude  their effective  use.  We  give algorithms for  approximate probabilis(cid:173) tic inference  that exploit averaging phenomena occurring at nodes  with  large  numbers  of parents.  We  show  that  these  algorithms  compute  rigorous  lower  and  upper  bounds  on  marginal probabili(cid:173) ties  of interest,  prove  that these  bounds  become exact in  the limit  of large networks,  and provide rates  of convergence.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/facf9f743b083008a894eee7baa16469-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/facf9f743b083008a894eee7baa16469-Bibtex.bib",
            "SUPP": ""
        }
    },
    "149": {
        "TITLE": "Basis Selection for Wavelet Regression",
        "AUTHORS": "Kevin R. Wheeler, Atam P. Dhawan",
        "ABSTRACT": "A  wavelet  basis  selection  procedure  is  presented  for  wavelet  re(cid:173) gression.  Both  the  basis  and  threshold  are  selected  using  cross(cid:173) validation.  The  method  includes  the  capability  of  incorporating  prior knowledge on the smoothness (or shape of the basis functions)  into  the  basis  selection  procedure.  The  results  of the  method  are  demonstrated  using  widely  published  sampled  functions.  The  re(cid:173) sults of the method are contrasted with other basis function  based  methods.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/fc528592c3858f90196fbfacc814f235-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/fc528592c3858f90196fbfacc814f235-Bibtex.bib",
            "SUPP": ""
        }
    },
    "150": {
        "TITLE": "Where Does the Population Vector of Motor Cortical Cells Point during Reaching Movements?",
        "AUTHORS": "Pierre Baraduc, Emmanuel Guigon, Yves Burnod",
        "ABSTRACT": "Visually-guided  arm  reaching  movements  are  produced  by  distributed  neural networks within parietal and frontal regions of the cerebral cortex.  Experimental  data  indicate  that (I) single  neurons  in  these  regions  are  broadly tuned to parameters of movement; (2) appropriate commands are  elaborated by  populations of neurons; (3) the coordinated action of neu(cid:173) rons can be  visualized  using a neuronal population vector (NPV).  How(cid:173) ever,  the  NPV  provides only  a rough  estimate  of movement parameters  (direction, velocity) and may even fail  to reflect the parameters of move(cid:173) ment when arm posture is changed.  We designed a model of the cortical  motor command to  investigate the relation between the desired direction  of the  movement, the  actual direction of movement and  the direction of  the NPV in  motor cortex. The model is a two-layer self-organizing neural  network  which  combines  broadly-tuned  (muscular)  proprioceptive  and  (cartesian) visual information to calculate (angular) motor commands for  the  initial  part  of the  movement of a  two-link  arm.  The  network  was  trained  by  motor babbling  in  5  positions.  Simulations  showed  that  (1)  the  network produced appropriate movement direction over a  large part  of the  workspace;  (2) small  deviations of the  actual  trajectory  from  the  desired  trajectory existed  at the extremities of the  workspace;  (3)  these  deviations were accompanied by  large deviations of the NPV from both  trajectories. These results suggest the NPV does not give a faithful image  of cortical processing during arm reaching movements. \n‚Ä¢ to  whom correspondence should be addressed \n84 \nP.  Baraduc,  E. Guigon and Y.  Burnod",
        "CONFERENCE": "Advances in Neural Information Processing Systems 11  (NIPS 1998)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1998/file/fcdf25d6e191893e705819b177cddea0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1998/file/fcdf25d6e191893e705819b177cddea0-Bibtex.bib",
            "SUPP": ""
        }
    }
}